{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TubesA_NIM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f21PRg_RKh0W"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi-fungsi aktivasi\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "  e = np.exp(x)\n",
        "  return e / np.sum(e)\n",
        "\n",
        "activation_function = {\n",
        "    \"Linear\": linear,\n",
        "    \"Sigmoid\": sigmoid,\n",
        "    \"ReLU\": relu,\n",
        "    \"Softmax\": softmax,\n",
        "}"
      ],
      "metadata": {
        "id": "ZiWfMZQfMkVd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer():\n",
        "  def __init__(self, weight, bias, activation):\n",
        "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
        "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
        "                                      % activation)\n",
        "    self.weight = weight\n",
        "    self.bias = bias\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward_propagation(self, input):\n",
        "    result = np.dot(input , self.weight) + self.bias\n",
        "    return activation_function[self.activation](result)\n",
        "\n",
        "class NeuralNetwork():\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "  \n",
        "  def summary(self):\n",
        "    print(\"Jumlah layer: \", len(self.layers))\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      print(\"============================================================\")\n",
        "      print('Layer {} (Activation: \"{}\", Units: {})'\n",
        "      .format(i+1, layer.activation, len(layer.weight)))\n",
        "      \n",
        "      print(\"Weight:\")\n",
        "      print(np.array(layer.weight))\n",
        "      \n",
        "      print(\"Bias:\")\n",
        "      print(np.array(layer.bias))\n",
        "    print(\"============================================================\")\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "  \n",
        "  def predict(self, input):\n",
        "    pass\n",
        "\n",
        "  def load_file(self, filename):\n",
        "    '''\n",
        "    File format\n",
        "    <depth>\n",
        "    <units> <activation function>\n",
        "    <weight0> \n",
        "    <bias0>\n",
        "    '''\n",
        "    with open(filename, 'r') as file:\n",
        "      depth = int(file.readline().strip())\n",
        "      for i in range (depth):\n",
        "        line = file.readline().strip().split()\n",
        "        unit = int(line[0])\n",
        "        activation = line[1]\n",
        "\n",
        "        # Weight Matrice\n",
        "        weight = []\n",
        "        for j in range(unit):\n",
        "          weight.append(list(map(float, file.readline().strip().split())))\n",
        "\n",
        "        # Bias Matrice\n",
        "        bias = list(map(float, file.readline().strip().split()))\n",
        "        \n",
        "\n",
        "        # Add layer\n",
        "        layer = Layer(weight, bias, activation)\n",
        "        self.add(layer)\n",
        "      \n",
        "      # End of file\n",
        "    # Close file"
      ],
      "metadata": {
        "id": "igZEzwshQzRh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR Sigmoid Model dan XOR Relu-Linear Model\n",
        "model = NeuralNetwork()\n",
        "filename = 'xor_sigmoid.txt'\n",
        "model.load_file(filename)"
      ],
      "metadata": {
        "id": "JFyzxJGRM6Hs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLLk7YS54k6T",
        "outputId": "f14522fe-8472-4256-bb2b-65cc2c048804"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah layer:  2\n",
            "============================================================\n",
            "Layer 1 (Activation: \"Sigmoid\", Units: 2)\n",
            "Weight:\n",
            "[[ 20.  20.]\n",
            " [-20. -20.]]\n",
            "Bias:\n",
            "[-10.  30.]\n",
            "============================================================\n",
            "Layer 2 (Activation: \"Sigmoid\", Units: 1)\n",
            "Weight:\n",
            "[[20. 20.]]\n",
            "Bias:\n",
            "[-30.]\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}