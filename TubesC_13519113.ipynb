{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Cross Entopy (Softmax)\n",
    "def cross_entropy(target, output):\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    np.random.seed(69)\n",
    "    self.weight = np.random.randn(output, input)\n",
    "    self.bias = np.random.randn(output, 1)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.delta = np.zeros(output)\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def set_weight(self, weight):\n",
    "    self.weight = weight\n",
    "\n",
    "  def set_bias(self, bias):\n",
    "    self.bias = bias\n",
    "    \n",
    "  def net(self):\n",
    "    net = np.dot(self.weight, self.data_in) + self.bias\n",
    "    return net\n",
    "\n",
    "  def output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net)\n",
    "  \n",
    "  def derivative_output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net, derivative = True)\n",
    "\n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "        \n",
    "  def load_file(self, filename):\n",
    "    '''\n",
    "    ### DEPRECATED ###\n",
    "    File format\n",
    "    <depth>\n",
    "    <units> <activation function>\n",
    "    <weight0> \n",
    "    <bias0>\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "      depth = int(file.readline().strip())\n",
    "      for i in range (depth):\n",
    "        line = file.readline().strip().split()\n",
    "        unit = int(line[0])\n",
    "        activation = line[1]\n",
    "\n",
    "        # Weight Matrix\n",
    "        weight = []\n",
    "        for j in range(unit):\n",
    "          weight.append(list(map(float, file.readline().strip().split())))\n",
    "\n",
    "        # Bias Matrix\n",
    "        bias = list(map(float, file.readline().strip().split()))\n",
    "        \n",
    "        # Add layer\n",
    "        layer = Layer(weight, bias, activation)\n",
    "        self.add(layer)\n",
    "      \n",
    "      # End of file\n",
    "    # Close file\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr_in = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr_in\n",
    "      arr_in = layer.output()\n",
    "    return arr_in\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_result = [] \n",
    "    y_result = []\n",
    "\n",
    "    for i in arr_id:\n",
    "      X_result.append(list(X[i]))\n",
    "      y_result.append(list(y[i]))\n",
    "\n",
    "    return X_result, y_result\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(np.array(X[head : tail]))\n",
    "      batch_y.append(np.array(y[head : tail]))\n",
    "        \n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        if (layer.activation == \"Softmax\"):\n",
    "          # Derivative of Cross Entropy times derivative output\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "        else:\n",
    "          # Derivative of MSE times derivative output\n",
    "          dE = (output - y)\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, nextl.delta)\n",
    "        layer.delta = error * layer.derivative_output()\n",
    "    \n",
    "      layer.delta_weight = np.dot(layer.delta, layer.data_in.T) * self.learning_rate\n",
    "      layer.delta_bias = layer.delta * self.learning_rate\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Shuffle data\n",
    "      data_X, data_y = self.shuffle(X, y)\n",
    "\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(data_X, data_y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        y_predict = self.forward_propagation(X_train)\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_predict, y_train.T)\n",
    "      \n",
    "        # Backward propagation to count delta\n",
    "        self.backward_propagation(X_train.T, y_train.T, y_predict)\n",
    "\n",
    "        # Update each layer\n",
    "        for layer in self.layers:\n",
    "          # Update weight\n",
    "          layer.weight += layer.delta_weight\n",
    "\n",
    "          # Update bias\n",
    "          delta_bias = layer.delta_bias\n",
    "          layer.bias += np.sum(delta_bias, axis=1).reshape(len(delta_bias), 1)\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1/len(X)\n",
    "      if iteration % 50 == 0:\n",
    "        print(f\"Iteration {iteration}: \", error)\n",
    "\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reached maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(model, X, y, n_splits=10):\n",
    "  data_X, data_y = X, y\n",
    "  total_score = 0\n",
    "    \n",
    "  if len(X) != len(y):\n",
    "    raise Exception(\"Length X and y is not the same\")\n",
    "\n",
    "  data_size = len(X)\n",
    "  fold_size = data_size // n_splits\n",
    "  remainder = data_size % n_splits\n",
    "  last_idx = fold_size * data_size + remainder\n",
    "  for test_idx in range(n_splits):\n",
    "    # Split the datasets\n",
    "    head_test = fold_size * test_idx\n",
    "    # If last fold, same as last_idx\n",
    "    tail_test = last_idx if (test_idx == n_splits - 1) else fold_size * (test_idx + 1)\n",
    "      \n",
    "    X_left_train = data_X[0 : head_test]\n",
    "    X_right_train = data_X[tail_test : last_idx]\n",
    "    y_left_train = data_y[0 : head_test]\n",
    "    y_right_train = data_y[tail_test : last_idx]\n",
    "\n",
    "    X_train = np.concatenate((X_left_train, X_right_train))\n",
    "    y_train = np.concatenate((y_left_train, y_right_train))\n",
    "    X_test = data_X[head_test : tail_test]\n",
    "    y_test = data_y[head_test : tail_test]\n",
    "\n",
    "    # Train the dataset\n",
    "    model.mgd(X_train, y_train)\n",
    "\n",
    "    # Get the prediction\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "      label_pred.append(np.argmax(prediction[:, i]))\n",
    "\n",
    "    y_test_label = []\n",
    "    for i in range(y_test.shape[0]):\n",
    "      y_test_label.append(np.argmax(y_test[i, :]))\n",
    "\n",
    "    # Count the score\n",
    "    accuracy = accuracy_score(label_pred, y_test_label)\n",
    "    print(\"Fold {} score: {}\".format(test_idx+1, accuracy))\n",
    "    total_score += accuracy\n",
    "\n",
    "  average_score = total_score / n_splits\n",
    "  return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=2000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "prediction_sklearn = model_sklearn.predict(X_test)\n",
    "print(prediction_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(prediction_sklearn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "model_scratch = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model_scratch.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model_scratch.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model_scratch.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model_scratch.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "# Layer Output\n",
    "model_scratch.mgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah layer:  4\n",
      "============================================================\n",
      "Layer 1 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.67720323 -1.23746885  1.25133917 -0.29505852]\n",
      " [-1.59748566  0.39772612  1.20586542  1.05629911]\n",
      " [ 0.6706775   1.32459608 -0.67377452 -1.03859201]\n",
      " [-0.84007163  1.18058312 -1.32884084 -0.12439675]\n",
      " [-0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.75949648 -0.78476026  0.3340992  -1.48850774]\n",
      " [-1.98192696 -1.19387174  0.16862644 -2.09861293]\n",
      " [ 0.01915532  0.30218571 -0.96038242  1.59176309]\n",
      " [ 0.35233315 -0.6519459  -1.16200164 -2.28303757]\n",
      " [-0.63227771 -0.50860144  1.45315962 -0.05379178]]\n",
      "Bias:\n",
      "[[-0.39635569]\n",
      " [-1.38093545]\n",
      " [ 0.05231323]\n",
      " [-0.10417045]\n",
      " [ 0.08445267]\n",
      " [ 0.93908086]\n",
      " [-1.57490495]\n",
      " [-1.41536889]\n",
      " [ 0.93064584]\n",
      " [ 0.3119037 ]]\n",
      "============================================================\n",
      "Layer 2 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 1.03354081 -0.60354197  0.95869381 -0.60140014 -1.59748566  0.41333897\n",
      "   1.20586542  1.05629911  0.85254391  0.80179974]\n",
      " [-0.22473706 -0.52218217 -0.84007163  1.18058312 -1.32884084 -0.12439675\n",
      "  -0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.88017804 -1.16996432  0.75327063 -1.11215464 -1.98192696 -1.21379757\n",
      "   0.16862644 -2.09861293  0.01915532  0.2807504 ]\n",
      " [-0.96038242  1.59176309  0.35233315 -0.6519459  -1.16200164 -2.28303757\n",
      "  -0.66273522 -0.27412048  1.37576624 -0.19023313]\n",
      " [-0.30758362 -1.38093545 -0.01824588 -0.10417045  0.08445267  0.73836805\n",
      "  -1.57490495 -1.41536889  0.93064584  0.28890111]\n",
      " [ 1.56620209 -2.46875639 -0.04028904 -1.21841107 -0.74092644  1.08029953\n",
      "  -0.71417021  1.85473142  1.52459913 -1.65361595]\n",
      " [-0.62744719  0.1300763   1.72485148 -0.7012101   0.13190869 -1.70227519\n",
      "  -2.10575458  0.73512193 -0.67059795 -0.15096595]\n",
      " [-0.72590177 -0.15787655  2.07961661 -0.42211077  0.79197497  1.34447036\n",
      "   0.8749181  -1.17416614  0.60359526 -1.19075375]\n",
      " [ 1.28976485  1.59487438  0.47447315  1.89835026  1.78385212  1.39816958\n",
      "   0.79380501 -0.42655674 -1.48182836  0.27530757]\n",
      " [ 0.29793932  0.93476078 -0.33487531  0.89987826  2.12052387  0.93078574\n",
      "   0.05341478  0.15107901 -1.49249972 -0.19980485]]\n",
      "Bias:\n",
      "[[-0.39009601]\n",
      " [ 0.53368414]\n",
      " [ 0.27063917]\n",
      " [-0.62918173]\n",
      " [-0.05810989]\n",
      " [-0.3651913 ]\n",
      " [ 0.96183405]\n",
      " [ 0.00774374]\n",
      " [ 1.68513638]\n",
      " [-0.55810837]]\n",
      "============================================================\n",
      "Layer 3 (Activation: \"Linear\", Units: 5)\n",
      "Weight:\n",
      "[[ 0.89825967 -0.60354197  1.15332919 -0.60140014 -1.59432759  0.37432946\n",
      "   1.20585624  1.02047463  0.82887306  0.68827494]\n",
      " [-0.17677886 -0.52218217 -0.83600562  1.18058312 -1.30485379 -0.07658156\n",
      "  -0.49749947 -2.09205723 -1.76784179  0.31124286]\n",
      " [ 0.97244968 -1.16996432  0.72960617 -1.11215464 -1.94567844 -1.15728794\n",
      "   0.1685947  -2.34585456  0.09245292  0.36565871]\n",
      " [-0.98998378  1.59176309  0.36311593 -0.6519459  -1.19905893 -2.29768961\n",
      "  -0.66269196 -0.01927041  1.31620624 -0.2471049 ]\n",
      " [-0.11718657 -1.38093545 -0.20319787 -0.10417045  0.13430597  0.81006994\n",
      "  -1.57495471 -1.76845827  1.01581777  0.48031205]]\n",
      "Bias:\n",
      "[[ 1.41714611]\n",
      " [-2.48706015]\n",
      " [ 0.16709862]\n",
      " [-1.18594848]\n",
      " [-0.78637373]]\n",
      "============================================================\n",
      "Layer 4 (Activation: \"Sigmoid\", Units: 3)\n",
      "Weight:\n",
      "[[ 0.9155071  -0.60354197  1.16229517 -0.60140014 -1.59748566]\n",
      " [ 0.39772613  1.20586542  1.05629911  0.85254391  0.68939142]\n",
      " [-0.00726308 -0.89292187 -1.2716291   1.131487   -1.77736753]]\n",
      "Bias:\n",
      "[[-0.12439675]\n",
      " [-0.4974814 ]\n",
      " [-1.91740618]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model_scratch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      " [1.86554341e-23 1.70877395e-59 1.55648070e-11 7.15333866e-21\n",
      "  1.19003128e-26 1.61127026e-54 6.17703606e-29 1.50521988e-14\n",
      "  9.98607217e-19 2.49060883e-27 3.18115572e-15 1.84346137e-52\n",
      "  2.11096474e-59 2.53881188e-53 3.67046827e-56 5.02863257e-23\n",
      "  1.35918197e-07 1.46023639e-25 4.61993544e-20 8.88776829e-08\n",
      "  1.00506168e-50 1.06075267e-14 2.73904694e-51 3.84992387e-08\n",
      "  2.31173039e-20 4.86749268e-11 9.03736353e-11 1.19677793e-07\n",
      "  9.75241674e-50 7.89762430e-51 3.70304074e-54 3.19345868e-62\n",
      "  4.04081914e-31 1.29749843e-52 6.35757184e-50 1.86403652e-11\n",
      "  1.41229373e-26 1.94034614e-56 4.56676127e-56 3.26256993e-61\n",
      "  2.17388816e-07 2.55168099e-23 5.24360377e-27 2.85890419e-58\n",
      "  5.00380698e-59 3.39466156e-27 1.13076527e-17 1.13682298e-12\n",
      "  1.45811783e-29 1.41607008e-08 2.05878854e-23 4.25127776e-11\n",
      "  1.45157409e-17 3.58849268e-56 3.38552970e-12 3.52914180e-28\n",
      "  2.81978386e-56 1.52645507e-54 4.53210867e-53 7.73746881e-22\n",
      "  6.80659834e-15 2.57327761e-51 1.56695265e-51 1.33999220e-49\n",
      "  2.85316855e-21 3.82092526e-55 2.20325139e-25 5.88149670e-13\n",
      "  6.86697766e-51 2.35054556e-27 3.38141438e-07 7.34313309e-63\n",
      "  1.19215932e-14 2.17388816e-07 3.99539295e-28]\n",
      " [9.99995271e-01 1.00000000e+00 6.28660237e-18 9.99996801e-01\n",
      "  9.99998315e-01 1.00000000e+00 1.00000000e+00 6.08165520e-04\n",
      "  8.60107317e-04 1.00000000e+00 1.27345693e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999999e-01\n",
      "  5.44374281e-12 1.00000000e+00 9.99975089e-01 4.79246748e-12\n",
      "  1.00000000e+00 8.05760715e-02 1.00000000e+00 2.66932242e-12\n",
      "  2.10676602e-02 8.84223274e-08 4.11732330e-15 2.41412561e-12\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 5.66029691e-10\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.42972361e-10 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 8.86401817e-03 1.46582971e-06\n",
      "  1.00000000e+00 1.48823286e-10 1.00000000e+00 8.95501594e-17\n",
      "  9.99976583e-01 1.00000000e+00 5.21985725e-18 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99998867e-01\n",
      "  8.24776473e-05 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999738e-01 1.00000000e+00 1.00000000e+00 1.28197751e-09\n",
      "  1.00000000e+00 1.00000000e+00 1.94313327e-10 1.00000000e+00\n",
      "  3.27931869e-01 1.42972361e-10 9.99999984e-01]]\n"
     ]
    }
   ],
   "source": [
    "prediction_scratch = model_scratch.predict(X_test)\n",
    "print(prediction_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = []\n",
    "for i in range(prediction_scratch.shape[1]):\n",
    "    label_pred.append(np.argmax(prediction_scratch[:, i]))\n",
    "\n",
    "y_test_label = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_label.append(np.argmax(y_test[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(label_pred)\n",
    "print(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(label_pred, y_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 1 score: 1.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 2 score: 1.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 3 score: 1.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 4 score: 0.3333333333333333\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 5 score: 0.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 6 score: 0.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 7 score: 0.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 8 score: 0.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 9 score: 0.0\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 10 score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "model = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "kFold(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
