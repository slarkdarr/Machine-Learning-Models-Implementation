{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Cross Entopy (Softmax)\n",
    "def cross_entropy(target, output):\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    np.random.seed(69)\n",
    "    self.weight = np.random.randn(output, input)\n",
    "    self.bias = np.random.randn(output, 1)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.delta = np.zeros(output)\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def set_weight(self, weight):\n",
    "    self.weight = weight\n",
    "\n",
    "  def set_bias(self, bias):\n",
    "    self.bias = bias\n",
    "    \n",
    "  def net(self):\n",
    "    net = np.dot(self.weight, self.data_in) + self.bias\n",
    "    return net\n",
    "\n",
    "  def output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net)\n",
    "  \n",
    "  def derivative_output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net, derivative = True)\n",
    "\n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "\n",
    "  def save_file(self) :\n",
    "    filename = input(\"Nama file yang ingin disimpan : \")\n",
    "    f = open(filename,\"w\")\n",
    "\n",
    "    n_layer = len(self.layers)\n",
    "    layer_arr = []\n",
    "\n",
    "    for i, layer in enumerate(self.layers) :\n",
    "      weight_arr = np.array(layer.weight)\n",
    "      bias_arr = np.array(layer.bias)\n",
    "      activation = layer.activation\n",
    "      \n",
    "      layer_content = {\n",
    "        \"weight\" : weight_arr,\n",
    "        \"bias\" : bias_arr,\n",
    "        \"activation\" : activation\n",
    "      }\n",
    "\n",
    "      layer_arr.append(layer_content)\n",
    "\n",
    "    content = {\n",
    "      \"n_layer\" : n_layer,\n",
    "      \"layers\" : layer_arr,\n",
    "      \"learning_rate\" : self.learning_rate,\n",
    "      \"error_threshold\" : self.error_threshold,\n",
    "      \"max_iter\" : self.max_iter,\n",
    "      \"batch_size\" : self.batch_size\n",
    "    }\n",
    "\n",
    "    json.dump(content, f, indent = 6)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "  def load_file(self) :\n",
    "    filename = input(\"Nama file yang ingin dibuka : \")\n",
    "    with open(filename) as json_model :\n",
    "      dataModel = json.load(json_model)\n",
    "      \n",
    "      for i in range (dataModel[\"n_layer\"]) :\n",
    "        bias = dataModel[\"layers\"][\"bias\"]\n",
    "        weight = dataModel[\"layers\"][\"weight\"]\n",
    "        activation = dataModel[\"layers\"][\"activation\"]\n",
    "\n",
    "        input = len(dataModel[\"layers\"][\"weight\"][0])\n",
    "        output = len(dataModel[\"layers\"][\"weight\"])\n",
    "\n",
    "        tempLayer = Layer(activation,input,output)\n",
    "        tempLayer.set_weight(weight)\n",
    "        tempLayer.set_bias(bias)\n",
    "\n",
    "        add(tempLayer)\n",
    "\n",
    "      self.learning_rate = dataModel[\"learning_rate\"]\n",
    "      self.error_threshold = dataModel[\"error_threshold\"] \n",
    "      self.max_iter = dataModel[\"max_iter\"]\n",
    "      self.batch_size = dataModel[\"batch_size\"]\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr_in = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr_in\n",
    "      arr_in = layer.output()\n",
    "    return arr_in\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_result = [] \n",
    "    y_result = []\n",
    "\n",
    "    for i in arr_id:\n",
    "      X_result.append(list(X[i]))\n",
    "      y_result.append(list(y[i]))\n",
    "\n",
    "    return X_result, y_result\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(np.array(X[head : tail]))\n",
    "      batch_y.append(np.array(y[head : tail]))\n",
    "        \n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        if (layer.activation == \"Softmax\"):\n",
    "          # Derivative of Cross Entropy times derivative output\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "        else:\n",
    "          # Derivative of MSE times derivative output\n",
    "          dE = -(output - y)\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, nextl.delta)\n",
    "        layer.delta = error * layer.derivative_output()\n",
    "    \n",
    "      layer.delta_weight = np.dot(layer.delta, layer.data_in.T) * self.learning_rate\n",
    "      layer.delta_bias = layer.delta * self.learning_rate\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Shuffle data\n",
    "      data_X, data_y = self.shuffle(X, y)\n",
    "\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(data_X, data_y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        y_predict = self.forward_propagation(X_train)\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_predict, y_train.T)\n",
    "      \n",
    "        # Backward propagation to count delta\n",
    "        self.backward_propagation(X_train.T, y_train.T, y_predict)\n",
    "\n",
    "        # Update each layer\n",
    "        for layer in self.layers:\n",
    "          # Update weight\n",
    "          layer.weight += layer.delta_weight\n",
    "\n",
    "          # Update bias\n",
    "          delta_bias = layer.delta_bias\n",
    "          layer.bias += np.sum(delta_bias, axis=1).reshape(len(delta_bias), 1)\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1/len(X)\n",
    "      if iteration % 50 == 0:\n",
    "        print(f\"Iteration {iteration}: \", error)\n",
    "\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reached maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5\n",
      "Precision score: [0.0, 0.4, 1.0]\n",
      "Recall score: [0.0, 1.0, 0.6666666666666666]\n",
      "F1 score: [0.0, 0.5714285714285715, 0.8]\n"
     ]
    }
   ],
   "source": [
    "class Metrics():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_confusion_matrix(self, y_true, y_pred):\n",
    "        K = len(np.unique(y_true)) # Number of classes \n",
    "        result = np.zeros((K, K))\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            result[y_true[i]][y_pred[i]] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def accuracy_score(self, y_true, y_pred, normalize=True):\n",
    "        if normalize:\n",
    "            acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "        else:\n",
    "            acc = np.sum(np.equal(y_true, y_pred))\n",
    "\n",
    "        return acc\n",
    "  \n",
    "    def precision_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        pred_unique, pred_counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "        if class_label is not None:\n",
    "            class_index = np.where(pred_unique == class_label)[0][0]\n",
    "\n",
    "            tp = np.sum(np.equal(y_true, class_label) & np.equal(y_pred, class_label))\n",
    "            total = pred_counts[class_index]\n",
    "\n",
    "            prec = tp/total\n",
    "        \n",
    "        else:\n",
    "            if binary:\n",
    "                tp = np.sum(np.equal(y_true, 1) & np.equal(y_pred, 1))\n",
    "                total = pred_counts[1]\n",
    "\n",
    "                prec = tp/total\n",
    "            else:\n",
    "                prec = []\n",
    "                for (i, val) in enumerate(pred_unique):\n",
    "                    class_index = np.where(pred_unique == val)[0][0]\n",
    "\n",
    "                    tp = np.sum(np.equal(y_true, val) & np.equal(y_pred, val))\n",
    "                    total = pred_counts[i]\n",
    "\n",
    "                    prec.append(tp/total)\n",
    "        \n",
    "        return prec\n",
    "\n",
    "    def recall_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        true_unique, true_counts = np.unique(y_true, return_counts=True)\n",
    "\n",
    "        if class_label is not None:\n",
    "            class_index = np.where(true_unique == class_label)[0][0]\n",
    "\n",
    "            tp = np.sum(np.equal(y_true, class_label) & np.equal(y_pred, class_label))\n",
    "            total = true_counts[class_index]\n",
    "\n",
    "            prec = tp/total\n",
    "        \n",
    "        else:\n",
    "            if binary:\n",
    "                tp = np.sum(np.equal(y_true, 1) & np.equal(y_pred, 1))\n",
    "                total = true_counts[1]\n",
    "\n",
    "                prec = tp/total\n",
    "            else:\n",
    "                prec = []\n",
    "                for (i, val) in enumerate(true_unique):\n",
    "                    class_index = np.where(true_unique == val)[0][0]\n",
    "\n",
    "                    tp = np.sum(np.equal(y_true, val) & np.equal(y_pred, val))\n",
    "                    total = true_counts[i]\n",
    "\n",
    "                    prec.append(tp/total)\n",
    "\n",
    "        return prec\n",
    "\n",
    "    def f1_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        prec = self.precision_score(y_true, y_pred, binary, class_label)\n",
    "        rec = self.recall_score(y_true, y_pred, binary, class_label)\n",
    "\n",
    "        if class_label is not None:\n",
    "            f1 = 2 * ((prec * rec)/(prec + rec))\n",
    "        else:\n",
    "            f1 = []\n",
    "            class_length = len(prec) | len(rec)\n",
    "            for i in range(class_length):\n",
    "                f1_scr = (2 * ((prec[i] * rec[i])/(prec[i] + rec[i]))) if (prec[i]+rec[i] != 0) else 0.0\n",
    "\n",
    "                f1.append(f1_scr)\n",
    "\n",
    "        return f1\n",
    "\n",
    "metrics = Metrics()\n",
    "a = [0,2,1,0,2,0,1,2]\n",
    "b = [1,2,1,1,0,1,1,2]\n",
    "\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(a, b, normalize=True)))\n",
    "print('Precision score: ' + str(metrics.precision_score(a, b, binary=False)))\n",
    "print('Recall score: ' + str(metrics.recall_score(a, b, binary=False)))\n",
    "print('F1 score: ' + str(metrics.f1_score(a, b, binary=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(model, X, y, n_splits=10, scratch=True):\n",
    "  data_X, data_y = X, y\n",
    "  total_score = 0\n",
    "    \n",
    "  if len(X) != len(y):\n",
    "    raise Exception(\"Length X and y is not the same\")\n",
    "\n",
    "  data_size = len(X)\n",
    "  fold_size = data_size // n_splits\n",
    "  remainder = data_size % n_splits\n",
    "  last_idx = fold_size * data_size + remainder\n",
    "  for test_idx in range(n_splits):\n",
    "    # Split the datasets\n",
    "    head_test = fold_size * test_idx\n",
    "    # If last fold, same as last_idx\n",
    "    tail_test = last_idx if (test_idx == n_splits - 1) else fold_size * (test_idx + 1)\n",
    "      \n",
    "    X_left_train = data_X[0 : head_test]\n",
    "    X_right_train = data_X[tail_test : last_idx]\n",
    "    y_left_train = data_y[0 : head_test]\n",
    "    y_right_train = data_y[tail_test : last_idx]\n",
    "\n",
    "    X_train = np.concatenate((X_left_train, X_right_train))\n",
    "    y_train = np.concatenate((y_left_train, y_right_train))\n",
    "    X_test = data_X[head_test : tail_test]\n",
    "    y_test = data_y[head_test : tail_test]\n",
    "\n",
    "    # Train the dataset\n",
    "    if not scratch:\n",
    "      model.fit(X_train, y_train)\n",
    "    else:\n",
    "      model.mgd(X_train, y_train)\n",
    "\n",
    "    # Get the prediction\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    # Count the score\n",
    "    accuracy = score_prediction(prediction, y_test, scratch)\n",
    "    print(\"Fold {} score: {}\".format(test_idx+1, accuracy))\n",
    "    total_score += accuracy\n",
    "\n",
    "  average_score = total_score / n_splits\n",
    "  return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset iris\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.8344637342734189\n",
      "Iteration 50:  0.8332005585350447\n",
      "Iteration 100:  0.8329270120479051\n",
      "Iteration 150:  0.8423381842974287\n",
      "Iteration 200:  0.6666719484974449\n",
      "Iteration 250:  0.6666687632169056\n",
      "Iteration 300:  0.6666679679791139\n",
      "Iteration 350:  0.666667608443739\n",
      "Iteration 400:  0.6666674039247352\n",
      "Iteration 450:  0.6666672720810244\n",
      "Iteration 500:  0.6666671800667273\n",
      "Iteration 550:  0.6666671122341928\n",
      "Iteration 600:  0.666667060170655\n",
      "Iteration 650:  0.666667018958473\n",
      "Iteration 700:  0.6666669855304095\n",
      "Iteration 750:  0.6666669578746494\n",
      "Iteration 800:  0.6666669346171826\n",
      "Iteration 850:  0.6666669147876133\n",
      "Iteration 900:  0.6666668976812197\n",
      "Iteration 950:  0.6666668827738993\n",
      "Iteration 1000:  0.6666668696678286\n",
      "Iteration 1050:  0.666666858055601\n",
      "Iteration 1100:  0.6666668476959509\n",
      "Iteration 1150:  0.6666668383968354\n",
      "Iteration 1200:  0.6666668300035973\n",
      "Iteration 1250:  0.6666668223901282\n",
      "Iteration 1300:  0.6666668154527843\n",
      "Iteration 1350:  0.666666809105582\n",
      "Iteration 1400:  0.6666668032759687\n",
      "Iteration 1450:  0.6666667979035877\n",
      "Iteration 1500:  0.6666667929366111\n",
      "Iteration 1550:  0.6666667883308893\n",
      "Iteration 1600:  0.666666784048438\n",
      "Iteration 1650:  0.6666667800564182\n",
      "Iteration 1700:  0.6666667763262895\n",
      "Iteration 1750:  0.6666667728331301\n",
      "Iteration 1800:  0.6666667695550804\n",
      "Iteration 1850:  0.6666667664728871\n",
      "Iteration 1900:  0.6666667635695274\n",
      "Iteration 1950:  0.6666667608298934\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "# Melakukan pembelajaran dalam batch\n",
    "model_sklearn_1 = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn_1.fit(X, y)\n",
    "\n",
    "model_scratch_1 = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "model_scratch_1.add(Layer(\"ReLU\", 4, 10)) # Layer 1\n",
    "model_scratch_1.add(Layer(\"ReLU\", 10, 10)) # Layer 2\n",
    "model_scratch_1.add(Layer(\"Linear\", 10, 5)) # Layer 3\n",
    "model_scratch_1.add(Layer(\"Sigmoid\", 5, 3)) # Layer Output\n",
    "model_scratch_1.mgd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(prediction, y, scratch=True):\n",
    "  if scratch:\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "      label_pred.append(np.argmax(prediction[:, i]))\n",
    "\n",
    "    label_y = []\n",
    "    for i in range(y.shape[0]):\n",
    "      label_y.append(np.argmax(y[i, :]))\n",
    "    return accuracy_score(label_pred, label_y)\n",
    "\n",
    "  else:\n",
    "    return (accuracy_score(prediction, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Membandingkan kedua hasil prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil model sklearn: 0.9733333333333334\n",
      "Hasil model scratch: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "prediction_sklearn_1 = model_sklearn_1.predict(X)\n",
    "print(\"Hasil model sklearn: {}\".format(score_prediction(prediction_sklearn_1, y, scratch=False)))\n",
    "\n",
    "prediction_scratch_1 = model_scratch_1.predict(X)\n",
    "print(\"Hasil model scratch: {}\".format(score_prediction(prediction_scratch_1, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Melakukan pembelajaran dengan skema split train 90% dan test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan split data train 90% dan data test 10%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.8301444158043019\n",
      "Iteration 50:  0.82591081197782\n",
      "Iteration 100:  0.8258389976544714\n",
      "Iteration 150:  0.8257702190919224\n",
      "Iteration 200:  0.8251163118877487\n",
      "Iteration 250:  0.8259257226866845\n",
      "Iteration 300:  0.8259257167984402\n",
      "Iteration 350:  0.8259257105574979\n",
      "Iteration 400:  0.8259257039297215\n",
      "Iteration 450:  0.8259256968785782\n",
      "Iteration 500:  0.825925689364013\n",
      "Iteration 550:  0.8259256813334239\n",
      "Iteration 600:  0.8259256727366858\n",
      "Iteration 650:  0.8259256635099537\n",
      "Iteration 700:  0.8259256535830756\n",
      "Iteration 750:  0.8259256428669772\n",
      "Iteration 800:  0.8259256312681275\n",
      "Iteration 850:  0.8259256186752185\n",
      "Iteration 900:  0.8259256049506337\n",
      "Iteration 950:  0.825925589930705\n",
      "Iteration 1000:  0.8259255734313599\n",
      "Iteration 1050:  0.8259255552161039\n",
      "Iteration 1100:  0.8259255350057972\n",
      "Iteration 1150:  0.8259255124402604\n",
      "Iteration 1200:  0.8259254871061538\n",
      "Iteration 1250:  0.8259254584336465\n",
      "Iteration 1300:  0.8259254257294232\n",
      "Iteration 1350:  0.8259253880785669\n",
      "Iteration 1400:  0.825925344254569\n",
      "Iteration 1450:  0.8259252926004716\n",
      "Iteration 1500:  0.8259252308053665\n",
      "Iteration 1550:  0.8259251555683002\n",
      "Iteration 1600:  0.825925061929076\n",
      "Iteration 1650:  0.8259249421980717\n",
      "Iteration 1700:  0.8259247836325577\n",
      "Iteration 1750:  0.825924563680757\n",
      "Iteration 1800:  0.8259242379067966\n",
      "Iteration 1850:  0.8259237054360504\n",
      "Iteration 1900:  0.8259226769841588\n",
      "Iteration 1950:  0.8259198451405847\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "# Melakukan pembelajaran menggunakan data split\n",
    "model_sklearn_2 = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn_2.fit(X_train, y_train)\n",
    "\n",
    "model_scratch_2 = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "model_scratch_2.add(Layer(\"ReLU\", 4, 10)) # Layer 1\n",
    "model_scratch_2.add(Layer(\"ReLU\", 10, 10)) # Layer 2\n",
    "model_scratch_2.add(Layer(\"Linear\", 10, 5)) # Layer 3\n",
    "model_scratch_2.add(Layer(\"Sigmoid\", 5, 3)) # Layer Output\n",
    "model_scratch_2.mgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil model sklearn: 1.0\n",
      "Hasil model scratch: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Membandingkan kedua hasil prediksi\n",
    "prediction_sklearn_2 = model_sklearn_2.predict(X_test)\n",
    "print(\"Hasil model sklearn: {}\".format(score_prediction(prediction_sklearn_2, y_test, scratch=False)))\n",
    "\n",
    "prediction_scratch_2 = model_scratch_2.predict(X_test)\n",
    "print(\"Hasil model scratch: {}\".format(score_prediction(prediction_scratch_2, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Melakukan pembelajaran dengan skema 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 score: 1.0\n",
      "Fold 2 score: 1.0\n",
      "Fold 3 score: 1.0\n",
      "Fold 4 score: 1.0\n",
      "Fold 5 score: 0.8\n",
      "Fold 6 score: 0.9333333333333333\n",
      "Fold 7 score: 1.0\n",
      "Fold 8 score: 1.0\n",
      "Fold 9 score: 0.8666666666666667\n",
      "Fold 10 score: 1.0\n",
      "Iteration 0:  0.8703118509550729\n",
      "Iteration 50:  0.7407521709125973\n",
      "Iteration 100:  0.7407436071280161\n",
      "Iteration 150:  0.7407423659778378\n",
      "Iteration 200:  0.740741872205911\n",
      "Iteration 250:  0.7407416076175651\n",
      "Iteration 300:  0.740741442902036\n",
      "Iteration 350:  0.7407413305675564\n",
      "Iteration 400:  0.7407412490917029\n",
      "Iteration 450:  0.740741187313579\n",
      "Iteration 500:  0.7407411388710583\n",
      "Iteration 550:  0.7407410998737471\n",
      "Iteration 600:  0.740741067807712\n",
      "Iteration 650:  0.7407410409795184\n",
      "Iteration 700:  0.7407410182043521\n",
      "Iteration 750:  0.7407409986296041\n",
      "Iteration 800:  0.7407409816259489\n",
      "Iteration 850:  0.7407409667187725\n",
      "Iteration 900:  0.7407409535434057\n",
      "Iteration 950:  0.7407409418150405\n",
      "Iteration 1000:  0.7407409313080563\n",
      "Iteration 1050:  0.7407409218415654\n",
      "Iteration 1100:  0.7407409132679417\n",
      "Iteration 1150:  0.7407409054672434\n",
      "Iteration 1200:  0.7407408983394284\n",
      "Iteration 1250:  0.7407408918011532\n",
      "Iteration 1300:  0.7407408857822739\n",
      "Iteration 1350:  0.7407408802233415\n",
      "Iteration 1400:  0.7407408750736413\n",
      "Iteration 1450:  0.7407408702896429\n",
      "Iteration 1500:  0.7407408658337753\n",
      "Iteration 1550:  0.7407408616734752\n",
      "Iteration 1600:  0.7407408577802136\n",
      "Iteration 1650:  0.7407408541292035\n",
      "Iteration 1700:  0.740740850698513\n",
      "Iteration 1750:  0.7407408474688074\n",
      "Iteration 1800:  0.7407408444229482\n",
      "Iteration 1850:  0.7407408415456891\n",
      "Iteration 1900:  0.7407408388234243\n",
      "Iteration 1950:  0.7407408362439724\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 1 score: 1.0\n",
      "Iteration 0:  0.74074074083199\n",
      "Iteration 50:  0.7407407408319872\n",
      "Iteration 100:  0.7407407408319845\n",
      "Iteration 150:  0.7407407408319817\n",
      "Iteration 200:  0.7407407408319788\n",
      "Iteration 250:  0.740740740831976\n",
      "Iteration 300:  0.7407407408319734\n",
      "Iteration 350:  0.7407407408319706\n",
      "Iteration 400:  0.740740740831968\n",
      "Iteration 450:  0.740740740831965\n",
      "Iteration 500:  0.7407407408319621\n",
      "Iteration 550:  0.7407407408319594\n",
      "Iteration 600:  0.7407407408319565\n",
      "Iteration 650:  0.7407407408319537\n",
      "Iteration 700:  0.7407407408319511\n",
      "Iteration 750:  0.7407407408319483\n",
      "Iteration 800:  0.7407407408319456\n",
      "Iteration 850:  0.7407407408319429\n",
      "Iteration 900:  0.7407407408319399\n",
      "Iteration 950:  0.7407407408319371\n",
      "Iteration 1000:  0.7407407408319344\n",
      "Iteration 1050:  0.7407407408319316\n",
      "Iteration 1100:  0.740740740831929\n",
      "Iteration 1150:  0.7407407408319261\n",
      "Iteration 1200:  0.7407407408319234\n",
      "Iteration 1250:  0.7407407408319205\n",
      "Iteration 1300:  0.7407407408319178\n",
      "Iteration 1350:  0.7407407408319149\n",
      "Iteration 1400:  0.7407407408319121\n",
      "Iteration 1450:  0.7407407408319092\n",
      "Iteration 1500:  0.7407407408319064\n",
      "Iteration 1550:  0.7407407408319038\n",
      "Iteration 1600:  0.7407407408319011\n",
      "Iteration 1650:  0.7407407408318983\n",
      "Iteration 1700:  0.7407407408318953\n",
      "Iteration 1750:  0.7407407408318927\n",
      "Iteration 1800:  0.7407407408318898\n",
      "Iteration 1850:  0.7407407408318872\n",
      "Iteration 1900:  0.7407407408318842\n",
      "Iteration 1950:  0.7407407408318816\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 2 score: 1.0\n",
      "Iteration 0:  0.7407408336681202\n",
      "Iteration 50:  0.740740831349311\n",
      "Iteration 100:  0.7407408291431353\n",
      "Iteration 150:  0.7407408270415898\n",
      "Iteration 200:  0.7407408250374131\n",
      "Iteration 250:  0.7407408231239953\n",
      "Iteration 300:  0.7407408212953186\n",
      "Iteration 350:  0.7407408195458772\n",
      "Iteration 400:  0.7407408178706392\n",
      "Iteration 450:  0.7407408162649852\n",
      "Iteration 500:  0.7407408147246723\n",
      "Iteration 550:  0.740740813245796\n",
      "Iteration 600:  0.7407408118247559\n",
      "Iteration 650:  0.7407408104582278\n",
      "Iteration 700:  0.7407408091431372\n",
      "Iteration 750:  0.7407408078766481\n",
      "Iteration 800:  0.7407408066560838\n",
      "Iteration 850:  0.7407408054790263\n",
      "Iteration 900:  0.7407408043431815\n",
      "Iteration 950:  0.7407408032464289\n",
      "Iteration 1000:  0.7407408021867735\n",
      "Iteration 1050:  0.7407408011623784\n",
      "Iteration 1100:  0.7407408001715106\n",
      "Iteration 1150:  0.740740799212552\n",
      "Iteration 1200:  0.7407407982839871\n",
      "Iteration 1250:  0.7407407973843942\n",
      "Iteration 1300:  0.7407407965124403\n",
      "Iteration 1350:  0.740740795666872\n",
      "Iteration 1400:  0.74074079484651\n",
      "Iteration 1450:  0.7407407940502455\n",
      "Iteration 1500:  0.7407407932770326\n",
      "Iteration 1550:  0.7407407925258888\n",
      "Iteration 1600:  0.7407407917958739\n",
      "Iteration 1650:  0.7407407910861189\n",
      "Iteration 1700:  0.7407407903957925\n",
      "Iteration 1750:  0.7407407897241002\n",
      "Iteration 1800:  0.7407407890703166\n",
      "Iteration 1850:  0.7407407884337023\n",
      "Iteration 1900:  0.7407407878136227\n",
      "Iteration 1950:  0.7407407872094253\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 3 score: 1.0\n",
      "Iteration 0:  0.6666667125720885\n",
      "Iteration 50:  0.6666667119971553\n",
      "Iteration 100:  0.6666667114364231\n",
      "Iteration 150:  0.6666667108893691\n",
      "Iteration 200:  0.6666667103554995\n",
      "Iteration 250:  0.6666667098343431\n",
      "Iteration 300:  0.6666667093254516\n",
      "Iteration 350:  0.6666667088283973\n",
      "Iteration 400:  0.6666667083427729\n",
      "Iteration 450:  0.6666667078681903\n",
      "Iteration 500:  0.6666667074042719\n",
      "Iteration 550:  0.666666706950668\n",
      "Iteration 600:  0.6666667065070373\n",
      "Iteration 650:  0.6666667060730541\n",
      "Iteration 700:  0.6666667056484097\n",
      "Iteration 750:  0.6666667052328005\n",
      "Iteration 800:  0.6666667048259464\n",
      "Iteration 850:  0.6666667044275753\n",
      "Iteration 900:  0.6666667040374167\n",
      "Iteration 950:  0.6666667036552272\n",
      "Iteration 1000:  0.6666667032807626\n",
      "Iteration 1050:  0.6666667029137926\n",
      "Iteration 1100:  0.6666667025540917\n",
      "Iteration 1150:  0.6666667022014484\n",
      "Iteration 1200:  0.6666667018556567\n",
      "Iteration 1250:  0.6666667015165191\n",
      "Iteration 1300:  0.6666667011838467\n",
      "Iteration 1350:  0.6666667008574529\n",
      "Iteration 1400:  0.6666667005371653\n",
      "Iteration 1450:  0.6666667002228123\n",
      "Iteration 1500:  0.6666666999142362\n",
      "Iteration 1550:  0.666666699611265\n",
      "Iteration 1600:  0.6666666993137619\n",
      "Iteration 1650:  0.6666666990215752\n",
      "Iteration 1700:  0.6666666987345635\n",
      "Iteration 1750:  0.6666666984525913\n",
      "Iteration 1800:  0.6666666981755267\n",
      "Iteration 1850:  0.6666666979032428\n",
      "Iteration 1900:  0.6666666976356187\n",
      "Iteration 1950:  0.6666666973725313\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 4 score: 0.3333333333333333\n",
      "Iteration 0:  0.6296296600769461\n",
      "Iteration 50:  0.6296296598225979\n",
      "Iteration 100:  0.6296296595724595\n",
      "Iteration 150:  0.6296296593264219\n",
      "Iteration 200:  0.6296296590843898\n",
      "Iteration 250:  0.6296296588462651\n",
      "Iteration 300:  0.6296296586119539\n",
      "Iteration 350:  0.629629658381365\n",
      "Iteration 400:  0.6296296581544111\n",
      "Iteration 450:  0.6296296579310081\n",
      "Iteration 500:  0.6296296577110703\n",
      "Iteration 550:  0.6296296574945177\n",
      "Iteration 600:  0.6296296572812763\n",
      "Iteration 650:  0.6296296570712689\n",
      "Iteration 700:  0.6296296568644226\n",
      "Iteration 750:  0.6296296566606665\n",
      "Iteration 800:  0.629629656459932\n",
      "Iteration 850:  0.6296296562621527\n",
      "Iteration 900:  0.6296296560672633\n",
      "Iteration 950:  0.6296296558752016\n",
      "Iteration 1000:  0.629629655685906\n",
      "Iteration 1050:  0.629629655499317\n",
      "Iteration 1100:  0.6296296553153783\n",
      "Iteration 1150:  0.6296296551340324\n",
      "Iteration 1200:  0.6296296549552253\n",
      "Iteration 1250:  0.6296296547789046\n",
      "Iteration 1300:  0.6296296546050185\n",
      "Iteration 1350:  0.6296296544335168\n",
      "Iteration 1400:  0.6296296542643514\n",
      "Iteration 1450:  0.6296296540974743\n",
      "Iteration 1500:  0.6296296539328394\n",
      "Iteration 1550:  0.6296296537704021\n",
      "Iteration 1600:  0.6296296536101189\n",
      "Iteration 1650:  0.6296296534519467\n",
      "Iteration 1700:  0.6296296532958451\n",
      "Iteration 1750:  0.6296296531417724\n",
      "Iteration 1800:  0.6296296529896913\n",
      "Iteration 1850:  0.6296296528395601\n",
      "Iteration 1900:  0.6296296526913447\n",
      "Iteration 1950:  0.6296296525450075\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 5 score: 0.0\n",
      "Iteration 0:  0.629629652400513\n",
      "Iteration 50:  0.629629652257827\n",
      "Iteration 100:  0.6296296521169158\n",
      "Iteration 150:  0.6296296519777456\n",
      "Iteration 200:  0.6296296518402852\n",
      "Iteration 250:  0.6296296517045034\n",
      "Iteration 300:  0.6296296515703692\n",
      "Iteration 350:  0.629629651437855\n",
      "Iteration 400:  0.6296296513069258\n",
      "Iteration 450:  0.6296296511775593\n",
      "Iteration 500:  0.6296296510497266\n",
      "Iteration 550:  0.6296296509233972\n",
      "Iteration 600:  0.6296296507985483\n",
      "Iteration 650:  0.629629650675153\n",
      "Iteration 700:  0.6296296505531883\n",
      "Iteration 750:  0.6296296504326226\n",
      "Iteration 800:  0.6296296503134389\n",
      "Iteration 850:  0.6296296501956108\n",
      "Iteration 900:  0.629629650079116\n",
      "Iteration 950:  0.6296296499639318\n",
      "Iteration 1000:  0.6296296498500361\n",
      "Iteration 1050:  0.6296296497374074\n",
      "Iteration 1100:  0.6296296496260249\n",
      "Iteration 1150:  0.6296296495158678\n",
      "Iteration 1200:  0.629629649406916\n",
      "Iteration 1250:  0.6296296492991503\n",
      "Iteration 1300:  0.6296296491925515\n",
      "Iteration 1350:  0.6296296490870992\n",
      "Iteration 1400:  0.6296296489827766\n",
      "Iteration 1450:  0.6296296488795656\n",
      "Iteration 1500:  0.629629648777448\n",
      "Iteration 1550:  0.6296296486764066\n",
      "Iteration 1600:  0.6296296485764246\n",
      "Iteration 1650:  0.6296296484774855\n",
      "Iteration 1700:  0.6296296483795729\n",
      "Iteration 1750:  0.629629648282671\n",
      "Iteration 1800:  0.6296296481867646\n",
      "Iteration 1850:  0.6296296480918375\n",
      "Iteration 1900:  0.6296296479978757\n",
      "Iteration 1950:  0.6296296479048651\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 6 score: 0.0\n",
      "Iteration 0:  0.6296296478127889\n",
      "Iteration 50:  0.6296296477216353\n",
      "Iteration 100:  0.6296296476313901\n",
      "Iteration 150:  0.6296296475420394\n",
      "Iteration 200:  0.6296296474535702\n",
      "Iteration 250:  0.62962964736597\n",
      "Iteration 300:  0.6296296472792247\n",
      "Iteration 350:  0.6296296471933229\n",
      "Iteration 400:  0.6296296471082522\n",
      "Iteration 450:  0.6296296470240008\n",
      "Iteration 500:  0.6296296469405566\n",
      "Iteration 550:  0.6296296468579086\n",
      "Iteration 600:  0.6296296467760442\n",
      "Iteration 650:  0.6296296466949534\n",
      "Iteration 700:  0.6296296466146251\n",
      "Iteration 750:  0.6296296465350486\n",
      "Iteration 800:  0.6296296464562137\n",
      "Iteration 850:  0.6296296463781091\n",
      "Iteration 900:  0.6296296463007255\n",
      "Iteration 950:  0.6296296462240535\n",
      "Iteration 1000:  0.6296296461480814\n",
      "Iteration 1050:  0.6296296460728014\n",
      "Iteration 1100:  0.6296296459982038\n",
      "Iteration 1150:  0.6296296459242792\n",
      "Iteration 1200:  0.6296296458510181\n",
      "Iteration 1250:  0.6296296457784123\n",
      "Iteration 1300:  0.6296296457064531\n",
      "Iteration 1350:  0.6296296456351307\n",
      "Iteration 1400:  0.6296296455644381\n",
      "Iteration 1450:  0.6296296454943667\n",
      "Iteration 1500:  0.6296296454249075\n",
      "Iteration 1550:  0.6296296453560531\n",
      "Iteration 1600:  0.6296296452877961\n",
      "Iteration 1650:  0.629629645220128\n",
      "Iteration 1700:  0.6296296451530419\n",
      "Iteration 1750:  0.6296296450865297\n",
      "Iteration 1800:  0.6296296450205845\n",
      "Iteration 1850:  0.6296296449551988\n",
      "Iteration 1900:  0.6296296448903655\n",
      "Iteration 1950:  0.629629644826078\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 7 score: 0.0\n",
      "Iteration 0:  0.6296296447623368\n",
      "Iteration 50:  0.6296296446991198\n",
      "Iteration 100:  0.6296296446364282\n",
      "Iteration 150:  0.6296296445742555\n",
      "Iteration 200:  0.6296296445125952\n",
      "Iteration 250:  0.6296296444514411\n",
      "Iteration 300:  0.6296296443907867\n",
      "Iteration 350:  0.6296296443306263\n",
      "Iteration 400:  0.6296296442709535\n",
      "Iteration 450:  0.6296296442117626\n",
      "Iteration 500:  0.6296296441530479\n",
      "Iteration 550:  0.6296296440948043\n",
      "Iteration 600:  0.6296296440370239\n",
      "Iteration 650:  0.6296296439797034\n",
      "Iteration 700:  0.6296296439228369\n",
      "Iteration 750:  0.6296296438664185\n",
      "Iteration 800:  0.6296296438104435\n",
      "Iteration 850:  0.6296296437549066\n",
      "Iteration 900:  0.6296296436998018\n",
      "Iteration 950:  0.6296296436451251\n",
      "Iteration 1000:  0.6296296435908713\n",
      "Iteration 1050:  0.6296296435370351\n",
      "Iteration 1100:  0.6296296434836123\n",
      "Iteration 1150:  0.6296296434305979\n",
      "Iteration 1200:  0.6296296433779871\n",
      "Iteration 1250:  0.6296296433257756\n",
      "Iteration 1300:  0.6296296432739585\n",
      "Iteration 1350:  0.6296296432225318\n",
      "Iteration 1400:  0.6296296431714907\n",
      "Iteration 1450:  0.6296296431208308\n",
      "Iteration 1500:  0.6296296430705485\n",
      "Iteration 1550:  0.629629643020639\n",
      "Iteration 1600:  0.6296296429710988\n",
      "Iteration 1650:  0.6296296429219223\n",
      "Iteration 1700:  0.6296296428731073\n",
      "Iteration 1750:  0.6296296428246491\n",
      "Iteration 1800:  0.6296296427765435\n",
      "Iteration 1850:  0.6296296427287873\n",
      "Iteration 1900:  0.6296296426813763\n",
      "Iteration 1950:  0.629629642634307\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 8 score: 0.0\n",
      "Iteration 0:  0.6296296425875686\n",
      "Iteration 50:  0.6296296425411715\n",
      "Iteration 100:  0.6296296424951052\n",
      "Iteration 150:  0.6296296424493654\n",
      "Iteration 200:  0.6296296424039499\n",
      "Iteration 250:  0.6296296423588545\n",
      "Iteration 300:  0.6296296423140759\n",
      "Iteration 350:  0.6296296422696107\n",
      "Iteration 400:  0.6296296422254564\n",
      "Iteration 450:  0.6296296421816086\n",
      "Iteration 500:  0.629629642138065\n",
      "Iteration 550:  0.629629642094822\n",
      "Iteration 600:  0.6296296420518768\n",
      "Iteration 650:  0.629629642009226\n",
      "Iteration 700:  0.629629641966867\n",
      "Iteration 750:  0.6296296419247963\n",
      "Iteration 800:  0.6296296418830113\n",
      "Iteration 850:  0.6296296418415089\n",
      "Iteration 900:  0.6296296418002865\n",
      "Iteration 950:  0.629629641759341\n",
      "Iteration 1000:  0.6296296417186704\n",
      "Iteration 1050:  0.6296296416782706\n",
      "Iteration 1100:  0.62962964163814\n",
      "Iteration 1150:  0.6296296415982751\n",
      "Iteration 1200:  0.6296296415586741\n",
      "Iteration 1250:  0.6296296415193338\n",
      "Iteration 1300:  0.6296296414802518\n",
      "Iteration 1350:  0.6296296414414261\n",
      "Iteration 1400:  0.6296296414028529\n",
      "Iteration 1450:  0.6296296413645311\n",
      "Iteration 1500:  0.6296296413264577\n",
      "Iteration 1550:  0.6296296412886302\n",
      "Iteration 1600:  0.6296296412510463\n",
      "Iteration 1650:  0.6296296412137034\n",
      "Iteration 1700:  0.6296296411765998\n",
      "Iteration 1750:  0.6296296411397327\n",
      "Iteration 1800:  0.6296296411030999\n",
      "Iteration 1850:  0.6296296410666995\n",
      "Iteration 1900:  0.629629641030529\n",
      "Iteration 1950:  0.6296296409945863\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 9 score: 0.0\n",
      "Iteration 0:  0.6296296409588694\n",
      "Iteration 50:  0.6296296409233756\n",
      "Iteration 100:  0.6296296408881037\n",
      "Iteration 150:  0.6296296408530512\n",
      "Iteration 200:  0.6296296408182158\n",
      "Iteration 250:  0.6296296407835965\n",
      "Iteration 300:  0.6296296407491894\n",
      "Iteration 350:  0.6296296407149942\n",
      "Iteration 400:  0.6296296406810086\n",
      "Iteration 450:  0.6296296406472303\n",
      "Iteration 500:  0.6296296406136578\n",
      "Iteration 550:  0.6296296405802886\n",
      "Iteration 600:  0.6296296405471219\n",
      "Iteration 650:  0.6296296405141549\n",
      "Iteration 700:  0.6296296404813866\n",
      "Iteration 750:  0.6296296404488145\n",
      "Iteration 800:  0.6296296404164371\n",
      "Iteration 850:  0.6296296403842531\n",
      "Iteration 900:  0.6296296403522598\n",
      "Iteration 950:  0.6296296403204563\n",
      "Iteration 1000:  0.6296296402888409\n",
      "Iteration 1050:  0.6296296402574113\n",
      "Iteration 1100:  0.6296296402261669\n",
      "Iteration 1150:  0.6296296401951049\n",
      "Iteration 1200:  0.6296296401642245\n",
      "Iteration 1250:  0.6296296401335241\n",
      "Iteration 1300:  0.6296296401030018\n",
      "Iteration 1350:  0.629629640072656\n",
      "Iteration 1400:  0.6296296400424861\n",
      "Iteration 1450:  0.6296296400124888\n",
      "Iteration 1500:  0.629629639982664\n",
      "Iteration 1550:  0.6296296399530099\n",
      "Iteration 1600:  0.6296296399235252\n",
      "Iteration 1650:  0.6296296398942081\n",
      "Iteration 1700:  0.6296296398650574\n",
      "Iteration 1750:  0.6296296398360715\n",
      "Iteration 1800:  0.6296296398072494\n",
      "Iteration 1850:  0.6296296397785893\n",
      "Iteration 1900:  0.6296296397500898\n",
      "Iteration 1950:  0.6296296397217497\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 10 score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "avg_sklearn = kFold(model_sklearn_2, X, y, scratch=False)\n",
    "avg_scratch = kFold(model_scratch_2, X, y, scratch=True)\n",
    "\n",
    "print(\"Rata-rata score dari model sklearn: {}\".format(avg_sklearn))\n",
    "print(\"Rata-rata score dari model scratch: {}\".format(avg_scratch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Menyimpan model hipotesis hasil pembelajaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Membaca model hipotesis dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Membuat instance baru lalu memprediksi hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create random n instances\n",
    "def create_random_instance(n):\n",
    "    '''\n",
    "    Creating n random instances\n",
    "    '''\n",
    "    rand_array = []\n",
    "    n_attr = 4\n",
    "    for i in range (n):\n",
    "        rand_row = []\n",
    "        for j in range (n_attr):\n",
    "            rand_row.append(round(np.random.uniform(0, 7), 2))\n",
    "        rand_array.append(rand_row)\n",
    "    return(rand_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: \n",
      "[[3.27 5.21 3.38 0.77]\n",
      " [3.18 4.98 5.42 6.44]\n",
      " [0.89 1.05 5.72 0.14]\n",
      " ...\n",
      " [1.1  5.34 0.55 5.21]\n",
      " [0.77 6.47 3.28 0.85]\n",
      " [5.35 4.39 2.79 4.55]]\n",
      "result: \n",
      "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 2.40835363e-06\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99982000e-01\n",
      "  1.00000000e+00 1.00000000e+00 2.48519586e-03 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999996e-01 1.00000000e+00\n",
      "  1.00000000e+00 9.99999961e-01 1.00000000e+00 9.79043408e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99997551e-01\n",
      "  1.00000000e+00 9.99999993e-01 9.99993878e-01 1.00000000e+00\n",
      "  2.33715598e-05 1.00000000e+00 1.00000000e+00 9.99999998e-01\n",
      "  1.00000000e+00 9.99999998e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999789e-01\n",
      "  9.99992083e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.98856730e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99825019e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999959e-01 1.00000000e+00\n",
      "  1.00000000e+00 4.01321189e-04 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.73731219e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99988126e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  3.43498895e-12 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999020e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999938e-01 9.99999718e-01\n",
      "  3.12244039e-04 1.00000000e+00 2.24821994e-07 9.99999920e-01\n",
      "  1.00000000e+00 1.00000000e+00 9.99999968e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99863556e-01\n",
      "  1.00000000e+00 3.05466633e-02 9.03495402e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 8.84081121e-02\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999997e-01 9.99995988e-01 2.37181703e-04 9.99999985e-01\n",
      "  1.00000000e+00 1.00000000e+00 9.99960819e-01 1.00000000e+00\n",
      "  9.99999994e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.95236076e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99943087e-01 1.00000000e+00 9.99999949e-01\n",
      "  9.99999949e-01 1.00000000e+00 1.02063350e-05 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999290e-01\n",
      "  5.65821402e-04 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  2.19879435e-01 1.00000000e+00 9.99997478e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.59251380e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.41264890e-01 1.00000000e+00 3.72978137e-02 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 4.11506062e-13\n",
      "  1.00000000e+00 1.00000000e+00 6.71295281e-02 9.99999998e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.18310590e-02\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99989142e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999838e-01\n",
      "  3.49338684e-02 8.34232410e-04 5.62937470e-01 2.86692311e-05\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999998e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.52891752e-11 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 8.44877001e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 6.30562841e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 6.12243850e-05 9.99999983e-01\n",
      "  1.00000000e+00 1.00000000e+00 9.99993054e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999994e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99999993e-01 9.98687730e-01 9.99999632e-01\n",
      "  6.31103551e-01 5.86182075e-04 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999998e-01 1.00000000e+00]\n",
      " [9.23188469e-20 1.32028838e-06 9.66159542e-01 2.86535781e-11\n",
      "  8.02156478e-30 1.38705665e-11 3.70771497e-35 5.25207382e-06\n",
      "  3.24755163e-10 4.57360154e-62 7.35235170e-01 6.38137434e-02\n",
      "  2.93096321e-06 3.68121155e-22 1.90199127e-03 4.07486706e-57\n",
      "  1.86897684e-21 4.65404999e-06 5.77110707e-03 4.29836142e-04\n",
      "  1.20436413e-07 4.35335386e-04 1.70907300e-20 5.13372054e-05\n",
      "  6.35733527e-12 2.97081541e-17 9.12195062e-01 4.78858117e-01\n",
      "  6.13881530e-06 2.43031778e-04 3.39346291e-05 3.36558449e-29\n",
      "  1.87274015e-05 1.14795075e-16 1.03570297e-56 9.89826023e-13\n",
      "  1.41826022e-07 9.02289000e-01 1.64226321e-46 9.56805541e-26\n",
      "  9.87327215e-01 9.04606350e-17 1.28595711e-04 1.26038321e-11\n",
      "  3.66125212e-61 9.52859513e-01 3.33574918e-29 4.18558555e-01\n",
      "  5.83151362e-01 6.09056479e-08 2.90058983e-06 1.06092541e-06\n",
      "  5.15629603e-20 1.46718148e-03 4.69723699e-35 3.15726813e-06\n",
      "  9.49643426e-17 3.98541276e-43 3.93386789e-10 2.10407168e-14\n",
      "  9.26739649e-02 1.23265585e-01 1.45621687e-18 3.19626760e-55\n",
      "  6.39824620e-24 1.42858640e-47 1.30560406e-21 1.15583219e-30\n",
      "  2.30630685e-07 5.36211629e-03 4.97091992e-11 2.31776592e-33\n",
      "  2.30727334e-36 5.67694912e-04 6.23410161e-07 1.07138220e-26\n",
      "  4.95871515e-24 1.11690478e-02 4.53891830e-29 5.81989791e-25\n",
      "  7.79619727e-33 6.73066864e-31 9.83226703e-02 2.60052456e-01\n",
      "  3.59090602e-42 8.50999308e-02 1.76533179e-05 4.94627950e-08\n",
      "  9.36675747e-29 6.43995739e-24 2.40951275e-06 3.22447120e-37\n",
      "  2.72943603e-40 2.07516351e-34 1.05845675e-31 1.10578532e-19\n",
      "  1.56053114e-02 1.35217250e-47 1.56431701e-14 2.47402435e-24\n",
      "  3.25968160e-15 1.36498429e-09 3.97702818e-26 5.19390652e-31\n",
      "  1.08984150e-12 1.22179860e-04 2.47537943e-36 2.84945682e-34\n",
      "  1.80336140e-07 3.98562820e-09 1.05238353e-15 1.18765439e-11\n",
      "  2.70737603e-02 6.58255891e-39 7.35799925e-06 9.39082528e-06\n",
      "  1.02795685e-45 6.70799319e-08 3.70540289e-01 2.54115441e-07\n",
      "  3.98400132e-34 5.59556110e-36 1.42817228e-16 2.17732199e-03\n",
      "  1.65672176e-05 1.64973348e-05 1.01066849e-02 6.44969063e-12\n",
      "  5.10348065e-23 2.45438735e-06 2.21265534e-07 1.66980320e-01\n",
      "  4.75134525e-33 8.52589331e-42 1.03204391e-11 2.23892887e-18\n",
      "  1.44965824e-06 3.10323744e-14 5.06158349e-09 2.93812700e-08\n",
      "  2.21591540e-26 2.74265926e-06 2.25220121e-21 3.49228444e-15\n",
      "  5.18629498e-07 6.39190578e-07 1.91716590e-65 2.18800687e-24\n",
      "  1.35273416e-02 5.35874767e-04 1.15733602e-06 8.40253475e-01\n",
      "  8.81974778e-23 9.40216146e-41 3.20087987e-07 1.22281744e-07\n",
      "  1.54409651e-07 1.21323331e-13 2.64533924e-17 1.97970744e-02\n",
      "  2.46925681e-21 4.47334114e-34 1.62656882e-16 5.07471942e-19\n",
      "  1.63198539e-04 7.26474867e-06 2.18925331e-13 7.16196972e-25\n",
      "  6.94758925e-05 7.80792872e-05 3.01123401e-24 8.75425112e-06\n",
      "  1.48888025e-04 1.35262387e-33 1.68674879e-04 6.30753486e-66\n",
      "  1.90454349e-28 9.36278892e-20 2.43917438e-04 9.93108467e-01\n",
      "  1.91663124e-02 1.29589292e-06 3.03051808e-04 4.20350486e-21\n",
      "  1.18968219e-05 8.28202520e-48 7.39363334e-34 4.65271780e-06\n",
      "  3.69087630e-24 8.14488329e-04 3.67793932e-15 5.33514478e-09\n",
      "  1.96881700e-04 6.14884830e-25 5.32208370e-08 1.19303993e-40\n",
      "  1.08856736e-13 2.07558806e-40 1.39105880e-66 1.24038676e-39\n",
      "  2.34288766e-03 3.63963053e-18 1.86294662e-19 4.59923137e-05\n",
      "  3.45443101e-05 2.87679052e-18 2.25224229e-04 6.26860104e-12\n",
      "  1.38764749e-04 1.09864360e-13 1.43184486e-05 6.09099402e-32\n",
      "  3.55941419e-39 4.50118029e-13 3.52878159e-06 8.57066513e-05\n",
      "  1.33438087e-16 3.11536799e-08 8.99999308e-03 7.16781936e-04\n",
      "  6.70652992e-37 2.05826400e-07 9.23306440e-09 5.65573540e-22\n",
      "  7.33240178e-23 6.54195718e-35 1.46483424e-17 5.70654428e-04\n",
      "  5.21240563e-06 2.87817516e-40 4.37792727e-32 7.13569492e-06\n",
      "  1.81611320e-04 3.47363245e-06 4.57276212e-17 1.63905526e-05\n",
      "  1.33884659e-05 8.94278778e-05 6.89533336e-01 2.27580373e-06\n",
      "  1.08544338e-33 2.72050665e-35 3.85425850e-14 4.53185182e-09\n",
      "  2.16185571e-08 1.80911643e-55 4.17770467e-18 1.59247620e-05\n",
      "  1.10415549e-05 3.89302672e-08 1.23619054e-11 4.94155697e-32\n",
      "  6.51123014e-37 2.94819578e-05 1.36079923e-26 4.07326806e-27\n",
      "  8.71488183e-15 1.81012243e-06 4.55017386e-02 1.26312791e-01\n",
      "  4.00804151e-05 5.30262105e-15 3.02318530e-06 9.21788736e-06\n",
      "  1.24009336e-04 4.68876287e-42 1.57992896e-04 1.79329073e-08\n",
      "  2.31056614e-16 6.86764259e-14 1.06653844e-04 6.88147622e-17\n",
      "  1.03251545e-11 1.62796080e-10 2.72727999e-06 9.28568335e-01\n",
      "  5.34142808e-07 2.18794266e-08 4.03835180e-05 2.07185580e-04\n",
      "  2.43007791e-29 1.05947237e-06 4.58244681e-57 2.14183613e-04\n",
      "  5.22660938e-01 4.07014696e-29 1.15301959e-08 2.50228969e-21\n",
      "  1.24195880e-43 1.16098440e-01 3.15244546e-04 1.18061826e-06\n",
      "  1.50912580e-07 1.06101431e-02 7.24431796e-48 4.30853503e-12\n",
      "  1.35609764e-05 1.23238352e-04 2.76851056e-09 8.37192312e-24]\n",
      " [8.14067355e-01 2.89205229e-02 5.94129754e-03 5.13812616e-09\n",
      "  2.93137074e-28 7.26558362e-09 2.25170704e-31 9.99089772e-01\n",
      "  6.09948006e-06 5.93721778e-19 3.36971912e-03 2.56368902e-01\n",
      "  2.32115492e-01 9.51989287e-24 9.80708154e-01 5.52651539e-16\n",
      "  1.01556167e-24 5.00101988e-06 2.26839793e-01 5.00115893e-03\n",
      "  1.21309873e-04 9.98996827e-01 9.98450099e-01 7.15431411e-01\n",
      "  9.12333162e-06 7.73262487e-17 3.15644622e-02 3.00566281e-01\n",
      "  1.31842988e-01 4.80445018e-01 5.14484095e-01 1.49353424e-06\n",
      "  9.99235988e-01 4.12597820e-16 2.11703509e-18 8.32339446e-03\n",
      "  1.33814611e-02 5.01682416e-02 5.94840375e-16 1.13823429e-18\n",
      "  1.07848403e-02 2.99114894e-01 2.54017550e-02 1.68406471e-02\n",
      "  3.59254537e-18 4.50639384e-03 3.44810268e-04 5.44223013e-03\n",
      "  3.75479881e-01 8.35885067e-07 1.75055826e-16 7.90076820e-01\n",
      "  3.98637779e-27 3.88628482e-03 2.11042594e-11 3.36839755e-01\n",
      "  1.43976675e-11 6.00862028e-12 8.40592772e-01 1.86465898e-16\n",
      "  3.59228983e-01 9.78471289e-03 3.99333101e-05 2.04382931e-18\n",
      "  3.69323016e-19 7.41504248e-15 1.24705791e-07 2.82944012e-05\n",
      "  2.67784322e-15 1.44690103e-03 1.13331857e-13 7.24947786e-15\n",
      "  2.74592167e-07 7.88747813e-02 9.99887203e-01 7.23025983e-15\n",
      "  2.33843032e-22 9.05572781e-01 8.47568250e-16 9.47709522e-01\n",
      "  2.78528347e-25 2.21268277e-09 8.31961819e-03 8.63342860e-01\n",
      "  6.01760085e-16 6.53639334e-03 3.97505967e-04 9.99968935e-01\n",
      "  2.20866863e-10 9.99716446e-01 7.67412009e-03 4.12877840e-16\n",
      "  5.34741455e-04 7.20586482e-10 4.39114355e-02 6.08536661e-23\n",
      "  9.96523965e-01 2.81741791e-15 5.74895712e-16 8.99092430e-23\n",
      "  1.14471433e-17 6.99015081e-02 1.49376349e-09 1.49072805e-35\n",
      "  1.38293023e-18 1.90276208e-01 1.97367352e-02 1.27571967e-18\n",
      "  7.59885084e-01 1.18078569e-02 1.01054128e-12 1.07263175e-08\n",
      "  9.80547578e-01 1.68462629e-06 9.94054990e-01 5.50184851e-01\n",
      "  8.11820022e-13 8.02079747e-01 6.19626020e-02 1.75489341e-05\n",
      "  4.11745032e-19 1.39703882e-07 9.38841020e-01 6.60447468e-01\n",
      "  5.02209211e-01 9.47370448e-01 4.64145384e-01 2.62754148e-22\n",
      "  5.19265453e-02 2.71450142e-04 1.29917699e-12 9.66168962e-01\n",
      "  5.64888146e-39 4.94591417e-06 6.45828505e-14 1.02432465e-10\n",
      "  1.25485059e-01 9.39588912e-15 5.13829480e-08 3.94147276e-07\n",
      "  1.09410945e-06 8.79358889e-15 7.47160742e-26 6.64663513e-20\n",
      "  3.08598051e-04 5.27385433e-03 9.66298616e-19 4.54983929e-21\n",
      "  1.21380627e-02 9.98410829e-01 9.71172105e-01 6.81243457e-02\n",
      "  2.06507561e-26 1.14331504e-11 9.94786894e-01 6.06748047e-02\n",
      "  1.90755148e-01 3.74086144e-16 9.33286687e-07 1.17120087e-02\n",
      "  1.21289649e-03 2.30511496e-20 8.89291484e-20 9.99972270e-01\n",
      "  1.87599842e-01 9.87401838e-01 7.19820712e-01 2.57714152e-19\n",
      "  1.09221587e-01 4.84289553e-01 2.38000837e-14 9.80160988e-02\n",
      "  8.70409772e-01 7.63477565e-18 9.93391618e-01 1.87449496e-18\n",
      "  3.86275684e-20 9.99999989e-01 3.45040494e-01 8.56669309e-01\n",
      "  9.74477289e-01 9.15182369e-02 3.84281379e-03 1.09174724e-28\n",
      "  1.85146833e-03 2.35634987e-13 3.30235734e-40 1.89174937e-03\n",
      "  1.04023235e-28 2.86305763e-02 9.99992075e-01 2.43899106e-01\n",
      "  9.36933197e-01 9.69019272e-34 4.57198768e-01 1.98749915e-13\n",
      "  9.96094568e-01 9.41136806e-11 2.26500065e-19 6.91200828e-17\n",
      "  9.19929908e-01 4.15501875e-22 3.84238007e-25 3.01704136e-02\n",
      "  5.04064384e-04 8.88390354e-01 1.20140284e-04 9.99901230e-01\n",
      "  8.75973003e-01 1.49368959e-12 9.97353248e-01 4.26960889e-09\n",
      "  3.18026002e-06 9.99998036e-01 2.18036197e-03 9.99427778e-01\n",
      "  2.09187317e-02 3.49203693e-01 7.20335642e-01 5.51772353e-01\n",
      "  1.59226045e-07 5.37351991e-04 1.03058514e-02 3.57772438e-24\n",
      "  2.72405401e-08 3.74536838e-13 2.10023489e-14 5.83457328e-01\n",
      "  4.48769459e-01 3.28551997e-16 1.35177324e-08 9.93623346e-01\n",
      "  1.12528214e-09 1.15368710e-01 2.87521171e-17 2.43618461e-01\n",
      "  9.36018831e-01 9.86640021e-01 9.76984496e-01 9.86473083e-01\n",
      "  2.00447990e-06 6.06025886e-08 9.99468838e-01 9.86873670e-01\n",
      "  3.76682891e-01 1.14949383e-16 1.88997943e-17 8.05636564e-01\n",
      "  3.62002851e-01 7.15756763e-03 9.71361684e-01 4.39822051e-40\n",
      "  1.85159604e-06 9.99939441e-01 6.83559262e-32 1.44088184e-26\n",
      "  1.99548530e-14 8.04333189e-01 8.75715947e-03 4.78148636e-07\n",
      "  7.76120547e-02 4.79829837e-16 7.95230606e-13 2.27996294e-02\n",
      "  3.88701034e-01 5.39524150e-12 3.49674159e-03 9.41367313e-08\n",
      "  1.12646321e-19 6.00666677e-10 8.23743750e-01 8.57257889e-06\n",
      "  3.29747454e-01 9.82455322e-01 9.79500402e-01 6.75157421e-02\n",
      "  1.68004925e-01 1.57482840e-02 3.10540490e-01 2.09815987e-01\n",
      "  2.29151991e-09 1.72591647e-04 8.53377997e-17 2.22994940e-01\n",
      "  1.93653146e-07 5.11983784e-06 4.55851736e-01 3.75882071e-02\n",
      "  1.81198170e-18 4.29774527e-02 9.04513113e-01 9.55280125e-02\n",
      "  9.44065067e-01 5.83089802e-01 3.91643062e-13 8.52655999e-03\n",
      "  8.05806460e-02 8.10137202e-03 9.99975704e-01 6.90882567e-26]]\n"
     ]
    }
   ],
   "source": [
    "# Create new instances & predict them using scratch model\n",
    "new_instances = create_random_instance(300)\n",
    "result = model_scratch_1.predict(new_instances)\n",
    "\n",
    "# Print instance & result of predict\n",
    "print(\"instance: \")\n",
    "print(np.array(new_instances))\n",
    "print(\"result: \")\n",
    "print(result)\n",
    "print(list(map(np.argmax, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Analisis Hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis hasil nomor 2.\n",
    "\n",
    "Berdasarkan hasil perbandingan confusion matrix dan perhitungan kinerja dari sklearn yang telah dieksekusi, didapatkan bahwa <!-- kinerja dari model yang dibuat oleh sklearn lebih baik dibandingkan dengan model yang dibuat secara sendiri. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis hasil nomor 3.\n",
    "\n",
    "Berdasarkan hasil pembelajaran FFNN untuk dataset iris dengan skema split train 90% dan test 10% yang telah dieksekusi, didapatkan bahwa <!-- kinerja dari model yang dibuat oleh sklearn lebih baik dibandingkan dengan model yang dibuat secara sendiri. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan hasil nomor 2 dan 3\n",
    "Berdasarkan hasil pembelajaran nomor 2 dan nomor 3 dapat dilihat bahwa ketika menggunakan skema 90% data training dan 10% data testing maka didapatkan hasil yang lebih baik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
