{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Cross Entopy (Softmax)\n",
    "def cross_entropy(target, output):\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    np.random.seed(69)\n",
    "    self.weight = np.random.randn(output, input)\n",
    "    self.bias = np.random.randn(output, 1)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.delta = np.zeros(output)\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def set_weight(self, weight):\n",
    "    self.weight = weight\n",
    "\n",
    "  def set_bias(self, bias):\n",
    "    self.bias = bias\n",
    "    \n",
    "  def net(self):\n",
    "    net = np.dot(self.weight, self.data_in) + self.bias\n",
    "    return net\n",
    "\n",
    "  def output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net)\n",
    "  \n",
    "  def derivative_output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net, derivative = True)\n",
    "\n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "        \n",
    "  def load_file(self, filename):\n",
    "    '''\n",
    "    ### DEPRECATED ###\n",
    "    File format\n",
    "    <depth>\n",
    "    <units> <activation function>\n",
    "    <weight0> \n",
    "    <bias0>\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "      depth = int(file.readline().strip())\n",
    "      for i in range (depth):\n",
    "        line = file.readline().strip().split()\n",
    "        unit = int(line[0])\n",
    "        activation = line[1]\n",
    "\n",
    "        # Weight Matrix\n",
    "        weight = []\n",
    "        for j in range(unit):\n",
    "          weight.append(list(map(float, file.readline().strip().split())))\n",
    "\n",
    "        # Bias Matrix\n",
    "        bias = list(map(float, file.readline().strip().split()))\n",
    "        \n",
    "        # Add layer\n",
    "        layer = Layer(weight, bias, activation)\n",
    "        self.add(layer)\n",
    "      \n",
    "      # End of file\n",
    "    # Close file\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr_in = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr_in\n",
    "      arr_in = layer.output()\n",
    "    return arr_in\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_result = [] \n",
    "    y_result = []\n",
    "\n",
    "    for i in arr_id:\n",
    "      X_result.append(list(X[i]))\n",
    "      y_result.append(list(y[i]))\n",
    "\n",
    "    return X_result, y_result\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(np.array(X[head : tail]))\n",
    "      batch_y.append(np.array(y[head : tail]))\n",
    "        \n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        if (layer.activation == \"Softmax\"):\n",
    "          # Derivative of Cross Entropy times derivative output\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "        else:\n",
    "          # Derivative of MSE times derivative output\n",
    "          dE = (output - y)\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, nextl.delta)\n",
    "        layer.delta = error * layer.derivative_output()\n",
    "    \n",
    "      layer.delta_weight = np.dot(layer.delta, layer.data_in.T) * self.learning_rate\n",
    "      layer.delta_bias = layer.delta * self.learning_rate\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Shuffle data\n",
    "      data_X, data_y = self.shuffle(X, y)\n",
    "\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(data_X, data_y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        y_predict = self.forward_propagation(X_train)\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_predict, y_train.T)\n",
    "      \n",
    "        # Backward propagation to count delta\n",
    "        self.backward_propagation(X_train.T, y_train.T, y_predict)\n",
    "\n",
    "        # Update each layer\n",
    "        for layer in self.layers:\n",
    "          # Update weight\n",
    "          layer.weight += layer.delta_weight\n",
    "\n",
    "          # Update bias\n",
    "          delta_bias = layer.delta_bias\n",
    "          layer.bias += np.sum(delta_bias, axis=1).reshape(len(delta_bias), 1)\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1/len(X)\n",
    "      if iteration % 50 == 0:\n",
    "        print(f\"Iteration {iteration}: \", error)\n",
    "\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reached maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_confusion_matrix(y_true, y_pred):\n",
    "        K = len(np.unique(y_true)) # Number of classes \n",
    "        result = np.zeros((K, K))\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            result[y_true[i]][y_pred[i]] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "        return acc\n",
    "  \n",
    "    def precision(self, y_true, y_pred):\n",
    "        fp = 0\n",
    "        tp = 0\n",
    "        for (i, val) in enumerate(y_true):\n",
    "            for j in range(val):\n",
    "                if (y_true[i][j] == 1 and y_pred[i][j] == 1):\n",
    "                    tp += 1\n",
    "                elif (y_true[i][j] == 0 and y_pred[i][j] == 1):\n",
    "                    fp += 1\n",
    "\n",
    "        prec = tp/(tp + fp)\n",
    "        return prec\n",
    "\n",
    "    def recall(self, y_true, y_pred):\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "        for (i, val) in enumerate(y_true):\n",
    "            for j in range(val):\n",
    "                if (y_true[i][j] == 1 and y_pred[i][j] == 1):\n",
    "                    tp += 1\n",
    "                elif (y_true[i][j] == 1 and y_pred[i][j] == 0):\n",
    "                    fn += 1\n",
    "        \n",
    "        rec = tp/(tp + fn)\n",
    "        return rec\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        prec = self.precision(y_true, y_pred)\n",
    "        rec = self.recall(y_true, y_pred)\n",
    "        f1 = 2 * ((prec * rec)/(prec + rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(model, X, y, n_splits=10):\n",
    "  data_X, data_y = X, y\n",
    "  total_score = 0\n",
    "    \n",
    "  if len(X) != len(y):\n",
    "    raise Exception(\"Length X and y is not the same\")\n",
    "\n",
    "  data_size = len(X)\n",
    "  fold_size = data_size // n_splits\n",
    "  remainder = data_size % n_splits\n",
    "  last_idx = fold_size * data_size + remainder\n",
    "  for test_idx in range(n_splits):\n",
    "    # Split the datasets\n",
    "    head_test = fold_size * test_idx\n",
    "    # If last fold, same as last_idx\n",
    "    tail_test = last_idx if (test_idx == n_splits - 1) else fold_size * (test_idx + 1)\n",
    "      \n",
    "    X_left_train = data_X[0 : head_test]\n",
    "    X_right_train = data_X[tail_test : last_idx]\n",
    "    y_left_train = data_y[0 : head_test]\n",
    "    y_right_train = data_y[tail_test : last_idx]\n",
    "\n",
    "    X_train = np.concatenate((X_left_train, X_right_train))\n",
    "    y_train = np.concatenate((y_left_train, y_right_train))\n",
    "    X_test = data_X[head_test : tail_test]\n",
    "    y_test = data_y[head_test : tail_test]\n",
    "\n",
    "    # Train the dataset\n",
    "    model.mgd(X_train, y_train)\n",
    "\n",
    "    # Get the prediction\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "      label_pred.append(np.argmax(prediction[:, i]))\n",
    "\n",
    "    y_test_label = []\n",
    "    for i in range(y_test.shape[0]):\n",
    "      y_test_label.append(np.argmax(y_test[i, :]))\n",
    "\n",
    "    # Count the score\n",
    "    accuracy = accuracy_score(label_pred, y_test_label)\n",
    "    print(\"Fold {} score: {}\".format(test_idx+1, accuracy))\n",
    "    total_score += accuracy\n",
    "\n",
    "  average_score = total_score / n_splits\n",
    "  return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "prediction_sklearn = model_sklearn.predict(X_test)\n",
    "print(prediction_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(prediction_sklearn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.85958824871429\n",
      "Iteration 50:  0.8664271383877938\n",
      "Iteration 100:  0.9863424895175223\n",
      "Iteration 150:  0.9898387940446075\n",
      "Iteration 200:  1.006676669744341\n",
      "Iteration 250:  0.9989250159367618\n",
      "Iteration 300:  1.0054180166741193\n",
      "Iteration 350:  1.0065572591600704\n",
      "Iteration 400:  1.006383200769154\n",
      "Iteration 450:  1.0246537052375744\n",
      "Iteration 500:  1.0146623503648629\n",
      "Iteration 550:  1.0175015069949296\n",
      "Iteration 600:  1.0125485572027337\n",
      "Iteration 650:  1.0206306324738483\n",
      "Iteration 700:  1.0226735081494525\n",
      "Iteration 750:  1.0170284357168744\n",
      "Iteration 800:  1.0186770128821194\n",
      "Iteration 850:  1.020205990067933\n",
      "Iteration 900:  1.0209520642778276\n",
      "Iteration 950:  1.016458375720153\n",
      "Iteration 1000:  1.0231709648937464\n",
      "Iteration 1050:  1.0244521583995667\n",
      "Iteration 1100:  1.0210158418005626\n",
      "Iteration 1150:  1.0202548964961522\n",
      "Iteration 1200:  1.024155680606967\n",
      "Iteration 1250:  1.024962084734202\n",
      "Iteration 1300:  1.0245036294388938\n",
      "Iteration 1350:  1.024842586630811\n",
      "Iteration 1400:  1.0252660687718207\n",
      "Iteration 1450:  1.0258589217177787\n",
      "Iteration 1500:  1.025665496283734\n",
      "Iteration 1550:  1.0258868196287472\n",
      "Iteration 1600:  1.0255753537447927\n",
      "Iteration 1650:  1.0257028065169786\n",
      "Iteration 1700:  1.025873613337619\n",
      "Iteration 1750:  1.0255605812116677\n",
      "Iteration 1800:  1.0259083364440158\n",
      "Iteration 1850:  1.025922016348679\n",
      "Iteration 1900:  1.026043439208745\n",
      "Iteration 1950:  1.0259301784761221\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "model_scratch = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model_scratch.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model_scratch.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model_scratch.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model_scratch.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "# Layer Output\n",
    "model_scratch.mgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah layer:  4\n",
      "============================================================\n",
      "Layer 1 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.67879435 -1.24118777  1.25135229 -0.28999701]\n",
      " [-1.59748566  0.39772612  1.20586542  1.05629911]\n",
      " [ 0.67011832  1.32639244 -0.67249428 -1.04395704]\n",
      " [-0.84007163  1.18058312 -1.32884084 -0.12439675]\n",
      " [-0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.75969772 -0.78863112  0.33764768 -1.48690934]\n",
      " [-1.98192696 -1.19387174  0.16862644 -2.09861293]\n",
      " [ 0.01915532  0.30218571 -0.96038242  1.59176309]\n",
      " [ 0.35233315 -0.6519459  -1.16200164 -2.28303757]\n",
      " [-0.63182666 -0.50416998  1.4516454  -0.05451633]]\n",
      "Bias:\n",
      "[[-0.39424211]\n",
      " [-1.38093545]\n",
      " [ 0.047078  ]\n",
      " [-0.10417045]\n",
      " [ 0.08445267]\n",
      " [ 0.93357375]\n",
      " [-1.57490495]\n",
      " [-1.41536889]\n",
      " [ 0.93064584]\n",
      " [ 0.31623553]]\n",
      "============================================================\n",
      "Layer 2 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 1.03366945 -0.60354197  0.95732616 -0.60140014 -1.59748566  0.41359044\n",
      "   1.20586542  1.05629911  0.85254391  0.8012377 ]\n",
      " [-0.22473706 -0.52218217 -0.84007163  1.18058312 -1.32884084 -0.12439675\n",
      "  -0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.87990989 -1.16996432  0.75414365 -1.11215464 -1.98192696 -1.21379836\n",
      "   0.16862644 -2.09861293  0.01915532  0.28052597]\n",
      " [-0.96038242  1.59176309  0.35233315 -0.6519459  -1.16200164 -2.28303757\n",
      "  -0.66273522 -0.27412048  1.37576624 -0.19023313]\n",
      " [-0.30667269 -1.38093545 -0.01816822 -0.10417045  0.08445267  0.73834981\n",
      "  -1.57490495 -1.41536889  0.93064584  0.28976367]\n",
      " [ 1.5677616  -2.46875639 -0.0438194  -1.21841107 -0.74092644  1.08078823\n",
      "  -0.71417021  1.85473142  1.52459913 -1.6535793 ]\n",
      " [-0.62752701  0.1300763   1.72475697 -0.7012101   0.13190869 -1.70232878\n",
      "  -2.10575458  0.73512193 -0.67059795 -0.15096596]\n",
      " [-0.7248768  -0.15787655  2.08333993 -0.42211077  0.79197497  1.34449802\n",
      "   0.8749181  -1.17416614  0.60359526 -1.18913554]\n",
      " [ 1.29015195  1.59487438  0.47500409  1.89835026  1.78385212  1.3980272\n",
      "   0.79380501 -0.42655674 -1.48182836  0.27551098]\n",
      " [ 0.2982124   0.93476078 -0.33638679  0.89987826  2.12052387  0.93089143\n",
      "   0.05341478  0.15107901 -1.49249972 -0.20004943]]\n",
      "Bias:\n",
      "[[-0.38958533]\n",
      " [ 0.53368414]\n",
      " [ 0.27067868]\n",
      " [-0.62918173]\n",
      " [-0.05917308]\n",
      " [-0.36537288]\n",
      " [ 0.96181417]\n",
      " [ 0.00495878]\n",
      " [ 1.68477729]\n",
      " [-0.55772814]]\n",
      "============================================================\n",
      "Layer 3 (Activation: \"Linear\", Units: 5)\n",
      "Weight:\n",
      "[[ 0.89825443 -0.60354197  1.1532312  -0.60140014 -1.59429691  0.37466812\n",
      "   1.20586512  1.02043519  0.82897619  0.68842913]\n",
      " [-0.17753071 -0.52218217 -0.83645157  1.18058312 -1.3048412  -0.07650559\n",
      "  -0.49748202 -2.09269622 -1.76777645  0.3112552 ]\n",
      " [ 0.9713357  -1.16996432  0.72889898 -1.11215464 -1.94570016 -1.15683434\n",
      "   0.16862537 -2.34633721  0.09266496  0.3657254 ]\n",
      " [-0.98902887  1.59176309  0.36400623 -0.6519459  -1.19922049 -2.29873872\n",
      "  -0.66273376 -0.01763955  1.31584148 -0.24768782]\n",
      " [-0.11867111 -1.38093545 -0.20429806 -0.10417045  0.13439372  0.81114652\n",
      "  -1.57490664 -1.77044878  1.01625069  0.48080921]]\n",
      "Bias:\n",
      "[[ 1.41709827]\n",
      " [-2.48675557]\n",
      " [ 0.16759255]\n",
      " [-1.18609856]\n",
      " [-0.78593422]]\n",
      "============================================================\n",
      "Layer 4 (Activation: \"Sigmoid\", Units: 3)\n",
      "Weight:\n",
      "[[ 0.9155071  -0.60354197  1.16229517 -0.60140014 -1.59748566]\n",
      " [ 0.39772613  1.20586542  1.05629911  0.85254391  0.68939142]\n",
      " [-0.00577546 -0.89424438 -1.27119981  1.13301773 -1.7801596 ]]\n",
      "Bias:\n",
      "[[-0.12439675]\n",
      " [-0.4974814 ]\n",
      " [-1.91765398]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model_scratch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      " [1.80164009e-23 1.76132198e-59 1.27018115e-11 7.27353662e-21\n",
      "  1.15285016e-26 1.70460101e-54 6.98893139e-29 1.62443718e-14\n",
      "  9.69431667e-19 2.60613671e-27 3.21096486e-15 1.91726337e-52\n",
      "  2.21695843e-59 2.61511993e-53 3.86559127e-56 5.35028345e-23\n",
      "  1.20815607e-07 1.49714278e-25 4.59837242e-20 7.88411773e-08\n",
      "  1.04371402e-50 1.06413013e-14 2.89304825e-51 3.42256788e-08\n",
      "  2.10341112e-20 4.90772390e-11 7.85181512e-11 1.06892607e-07\n",
      "  1.03485764e-49 8.18734912e-51 4.02500870e-54 3.40446085e-62\n",
      "  4.16874660e-31 1.34671990e-52 6.76528312e-50 1.80343576e-11\n",
      "  1.51117667e-26 2.01800543e-56 4.79654684e-56 3.41399288e-61\n",
      "  1.98080505e-07 2.78259133e-23 5.27852675e-27 3.08867741e-58\n",
      "  5.19466891e-59 3.50649477e-27 1.07012365e-17 1.07544423e-12\n",
      "  1.49480265e-29 1.46432043e-08 2.13029634e-23 3.62884499e-11\n",
      "  1.48703481e-17 3.76262107e-56 2.84504083e-12 3.55871727e-28\n",
      "  2.95156449e-56 1.62988910e-54 4.69119631e-53 7.96415544e-22\n",
      "  6.69230857e-15 2.61669720e-51 1.58786331e-51 1.41745393e-49\n",
      "  2.91451283e-21 3.97744845e-55 2.33291894e-25 5.26482138e-13\n",
      "  7.23865045e-51 2.40339414e-27 3.05233467e-07 7.76112295e-63\n",
      "  1.20916806e-14 1.98080505e-07 3.93940868e-28]\n",
      " [9.99995810e-01 1.00000000e+00 6.12399029e-18 9.99996998e-01\n",
      "  9.99998461e-01 1.00000000e+00 1.00000000e+00 5.63001907e-04\n",
      "  8.38244015e-04 1.00000000e+00 1.29055530e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999999e-01\n",
      "  5.71653359e-12 1.00000000e+00 9.99977214e-01 4.95705834e-12\n",
      "  1.00000000e+00 8.22156514e-02 1.00000000e+00 2.76944457e-12\n",
      "  2.42436110e-02 8.23724625e-08 4.15754658e-15 2.55690244e-12\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 5.32823939e-10\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.50348387e-10 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.48098300e-03 1.54514936e-06\n",
      "  1.00000000e+00 1.43658469e-10 1.00000000e+00 9.15493097e-17\n",
      "  9.99978181e-01 1.00000000e+00 5.23744765e-18 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99998886e-01\n",
      "  8.12803135e-05 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999755e-01 1.00000000e+00 1.00000000e+00 1.42236953e-09\n",
      "  1.00000000e+00 1.00000000e+00 2.00499015e-10 1.00000000e+00\n",
      "  3.31966928e-01 1.50348387e-10 9.99999985e-01]]\n"
     ]
    }
   ],
   "source": [
    "prediction_scratch = model_scratch.predict(X_test)\n",
    "print(prediction_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = []\n",
    "for i in range(prediction_scratch.shape[1]):\n",
    "    label_pred.append(np.argmax(prediction_scratch[:, i]))\n",
    "\n",
    "y_test_label = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_label.append(np.argmax(y_test[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(label_pred)\n",
    "print(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(label_pred, y_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.8734740179395201\n",
      "Iteration 50:  0.8703634865615099\n",
      "Iteration 100:  0.8703668670285022\n",
      "Iteration 150:  0.8703680398695443\n",
      "Iteration 200:  0.8703686280052914\n",
      "Iteration 250:  0.8703689802741165\n",
      "Iteration 300:  0.8703692144355661\n",
      "Iteration 350:  0.8703693812398866\n",
      "Iteration 400:  0.8703695060612097\n",
      "Iteration 450:  0.8703696029440171\n",
      "Iteration 500:  0.8703696803056253\n",
      "Iteration 550:  0.870369743502959\n",
      "Iteration 600:  0.8703697961049758\n",
      "Iteration 650:  0.870369840547665\n",
      "Iteration 700:  0.8703698786166616\n",
      "Iteration 750:  0.8703699115711023\n",
      "Iteration 800:  0.8703699403878011\n",
      "Iteration 850:  0.8703699657890991\n",
      "Iteration 900:  0.8703699883577967\n",
      "Iteration 950:  0.8703700085364844\n",
      "Iteration 1000:  0.8703700266865743\n",
      "Iteration 1050:  0.8703700431001771\n",
      "Iteration 1100:  0.8703700580179724\n",
      "Iteration 1150:  0.8703700716317303\n",
      "Iteration 1200:  0.8703700841052497\n",
      "Iteration 1250:  0.8703700955789866\n",
      "Iteration 1300:  0.870370106163538\n",
      "Iteration 1350:  0.870370115965009\n",
      "Iteration 1400:  0.8703701250621942\n",
      "Iteration 1450:  0.8703701335292598\n",
      "Iteration 1500:  0.8703701414313147\n",
      "Iteration 1550:  0.8703701488210239\n",
      "Iteration 1600:  0.870370155748037\n",
      "Iteration 1650:  0.870370162255827\n",
      "Iteration 1700:  0.8703701683762379\n",
      "Iteration 1750:  0.8703701741493575\n",
      "Iteration 1800:  0.870370179599776\n",
      "Iteration 1850:  0.8703701847536633\n",
      "Iteration 1900:  0.8703701896383743\n",
      "Iteration 1950:  0.8703701942705266\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 1 score: 1.0\n",
      "Iteration 0:  0.8703701986705056\n",
      "Iteration 50:  0.870370202853222\n",
      "Iteration 100:  0.8703702068409512\n",
      "Iteration 150:  0.8703702106397888\n",
      "Iteration 200:  0.8703702142666065\n",
      "Iteration 250:  0.8703702177320949\n",
      "Iteration 300:  0.8703702210466551\n",
      "Iteration 350:  0.8703702242196796\n",
      "Iteration 400:  0.8703702272594288\n",
      "Iteration 450:  0.8703702301762127\n",
      "Iteration 500:  0.8703702329766142\n",
      "Iteration 550:  0.8703702356665969\n",
      "Iteration 600:  0.8703702382527272\n",
      "Iteration 650:  0.8703702407413944\n",
      "Iteration 700:  0.8703702431374123\n",
      "Iteration 750:  0.8703702454466884\n",
      "Iteration 800:  0.8703702476729445\n",
      "Iteration 850:  0.870370249821448\n",
      "Iteration 900:  0.8703702518953474\n",
      "Iteration 950:  0.8703702538992394\n",
      "Iteration 1000:  0.8703702558359103\n",
      "Iteration 1050:  0.8703702577095903\n",
      "Iteration 1100:  0.8703702595226251\n",
      "Iteration 1150:  0.870370261277632\n",
      "Iteration 1200:  0.8703702629778456\n",
      "Iteration 1250:  0.8703702646264307\n",
      "Iteration 1300:  0.8703702662246537\n",
      "Iteration 1350:  0.8703702677752415\n",
      "Iteration 1400:  0.8703702692801151\n",
      "Iteration 1450:  0.8703702707410547\n",
      "Iteration 1500:  0.8703702721607203\n",
      "Iteration 1550:  0.870370273540186\n",
      "Iteration 1600:  0.870370274881225\n",
      "Iteration 1650:  0.8703702761856847\n",
      "Iteration 1700:  0.8703702774544199\n",
      "Iteration 1750:  0.870370278690077\n",
      "Iteration 1800:  0.8703702798928175\n",
      "Iteration 1850:  0.8703702810644344\n",
      "Iteration 1900:  0.8703702822060139\n",
      "Iteration 1950:  0.8703702833185282\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 2 score: 1.0\n",
      "Iteration 0:  0.8703702844036566\n",
      "Iteration 50:  0.8703702854614749\n",
      "Iteration 100:  0.8703702864936048\n",
      "Iteration 150:  0.8703702875011627\n",
      "Iteration 200:  0.8703702884845885\n",
      "Iteration 250:  0.87037028944483\n",
      "Iteration 300:  0.8703702903827824\n",
      "Iteration 350:  0.8703702912990593\n",
      "Iteration 400:  0.8703702921946358\n",
      "Iteration 450:  0.870370293069919\n",
      "Iteration 500:  0.8703702939259331\n",
      "Iteration 550:  0.8703702947631224\n",
      "Iteration 600:  0.8703702955821387\n",
      "Iteration 650:  0.8703702963834292\n",
      "Iteration 700:  0.870370297167678\n",
      "Iteration 750:  0.8703702979356122\n",
      "Iteration 800:  0.8703702986874639\n",
      "Iteration 850:  0.8703702994235054\n",
      "Iteration 900:  0.8703703001451382\n",
      "Iteration 950:  0.8703703008518863\n",
      "Iteration 1000:  0.8703703015442881\n",
      "Iteration 1050:  0.8703703022234289\n",
      "Iteration 1100:  0.8703703028890495\n",
      "Iteration 1150:  0.8703703035416228\n",
      "Iteration 1200:  0.8703703041818721\n",
      "Iteration 1250:  0.8703703048099329\n",
      "Iteration 1300:  0.8703703054260407\n",
      "Iteration 1350:  0.8703703060307464\n",
      "Iteration 1400:  0.8703703066240882\n",
      "Iteration 1450:  0.8703703072066754\n",
      "Iteration 1500:  0.8703703077786072\n",
      "Iteration 1550:  0.8703703083402247\n",
      "Iteration 1600:  0.8703703088920554\n",
      "Iteration 1650:  0.8703703094338769\n",
      "Iteration 1700:  0.8703703099663374\n",
      "Iteration 1750:  0.8703703104894847\n",
      "Iteration 1800:  0.8703703110036428\n",
      "Iteration 1850:  0.8703703115090685\n",
      "Iteration 1900:  0.8703703120056941\n",
      "Iteration 1950:  0.870370312494232\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 3 score: 1.0\n",
      "Iteration 0:  0.7962962507778195\n",
      "Iteration 50:  0.7962962510832111\n",
      "Iteration 100:  0.7962962513843611\n",
      "Iteration 150:  0.7962962516813578\n",
      "Iteration 200:  0.796296251974735\n",
      "Iteration 250:  0.7962962522641018\n",
      "Iteration 300:  0.7962962525497495\n",
      "Iteration 350:  0.7962962528317075\n",
      "Iteration 400:  0.7962962531099939\n",
      "Iteration 450:  0.7962962533848175\n",
      "Iteration 500:  0.7962962536561239\n",
      "Iteration 550:  0.796296253923975\n",
      "Iteration 600:  0.7962962541884917\n",
      "Iteration 650:  0.7962962544497598\n",
      "Iteration 700:  0.7962962547077689\n",
      "Iteration 750:  0.7962962549626341\n",
      "Iteration 800:  0.7962962552143565\n",
      "Iteration 850:  0.7962962554630718\n",
      "Iteration 900:  0.7962962557087306\n",
      "Iteration 950:  0.7962962559514949\n",
      "Iteration 1000:  0.7962962561913246\n",
      "Iteration 1050:  0.7962962564283156\n",
      "Iteration 1100:  0.7962962566625438\n",
      "Iteration 1150:  0.7962962568940305\n",
      "Iteration 1200:  0.7962962571228448\n",
      "Iteration 1250:  0.7962962573489659\n",
      "Iteration 1300:  0.7962962575725119\n",
      "Iteration 1350:  0.7962962577934787\n",
      "Iteration 1400:  0.7962962580119558\n",
      "Iteration 1450:  0.7962962582279263\n",
      "Iteration 1500:  0.796296258441393\n",
      "Iteration 1550:  0.7962962586526972\n",
      "Iteration 1600:  0.796296258861528\n",
      "Iteration 1650:  0.7962962590680298\n",
      "Iteration 1700:  0.7962962592722973\n",
      "Iteration 1750:  0.7962962594742709\n",
      "Iteration 1800:  0.7962962596741738\n",
      "Iteration 1850:  0.7962962598717429\n",
      "Iteration 1900:  0.7962962600673432\n",
      "Iteration 1950:  0.7962962602608062\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 4 score: 0.3333333333333333\n",
      "Iteration 0:  0.7592592284901283\n",
      "Iteration 50:  0.7592592286236259\n",
      "Iteration 100:  0.7592592287559311\n",
      "Iteration 150:  0.75925922888705\n",
      "Iteration 200:  0.7592592290171287\n",
      "Iteration 250:  0.7592592291460408\n",
      "Iteration 300:  0.7592592292738052\n",
      "Iteration 350:  0.7592592294006052\n",
      "Iteration 400:  0.759259229526274\n",
      "Iteration 450:  0.7592592296508868\n",
      "Iteration 500:  0.7592592297744526\n",
      "Iteration 550:  0.7592592298969737\n",
      "Iteration 600:  0.759259230018463\n",
      "Iteration 650:  0.7592592301390073\n",
      "Iteration 700:  0.7592592302585176\n",
      "Iteration 750:  0.7592592303770738\n",
      "Iteration 800:  0.7592592304946245\n",
      "Iteration 850:  0.7592592306112285\n",
      "Iteration 900:  0.7592592307269105\n",
      "Iteration 950:  0.7592592308416587\n",
      "Iteration 1000:  0.7592592309554662\n",
      "Iteration 1050:  0.7592592310682629\n",
      "Iteration 1100:  0.7592592311803601\n",
      "Iteration 1150:  0.759259231291467\n",
      "Iteration 1200:  0.7592592314016878\n",
      "Iteration 1250:  0.7592592315110439\n",
      "Iteration 1300:  0.7592592316195339\n",
      "Iteration 1350:  0.7592592317272151\n",
      "Iteration 1400:  0.759259231834016\n",
      "Iteration 1450:  0.7592592319400144\n",
      "Iteration 1500:  0.759259232045185\n",
      "Iteration 1550:  0.7592592321495388\n",
      "Iteration 1600:  0.7592592322530819\n",
      "Iteration 1650:  0.7592592323558599\n",
      "Iteration 1700:  0.7592592324578634\n",
      "Iteration 1750:  0.7592592325590534\n",
      "Iteration 1800:  0.7592592326594979\n",
      "Iteration 1850:  0.7592592327592004\n",
      "Iteration 1900:  0.7592592328581514\n",
      "Iteration 1950:  0.7592592329563632\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 5 score: 0.0\n",
      "Iteration 0:  0.7592592334957713\n",
      "Iteration 50:  0.7592592335894893\n",
      "Iteration 100:  0.7592592336824698\n",
      "Iteration 150:  0.75925923377489\n",
      "Iteration 200:  0.7592592338665987\n",
      "Iteration 250:  0.7592592339576254\n",
      "Iteration 300:  0.7592592340479675\n",
      "Iteration 350:  0.7592592341377137\n",
      "Iteration 400:  0.759259234226832\n",
      "Iteration 450:  0.7592592343152637\n",
      "Iteration 500:  0.7592592344031385\n",
      "Iteration 550:  0.7592592344903697\n",
      "Iteration 600:  0.7592592345769623\n",
      "Iteration 650:  0.759259234662944\n",
      "Iteration 700:  0.7592592347484058\n",
      "Iteration 750:  0.7592592348332132\n",
      "Iteration 800:  0.7592592349174275\n",
      "Iteration 850:  0.7592592350010794\n",
      "Iteration 900:  0.7592592350841593\n",
      "Iteration 950:  0.759259235166609\n",
      "Iteration 1000:  0.7592592352486006\n",
      "Iteration 1050:  0.7592592353299852\n",
      "Iteration 1100:  0.7592592354108093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1150:  0.7592592354911001\n",
      "Iteration 1200:  0.7592592355708414\n",
      "Iteration 1250:  0.7592592356500376\n",
      "Iteration 1300:  0.7592592357287186\n",
      "Iteration 1350:  0.75925923580686\n",
      "Iteration 1400:  0.7592592358845092\n",
      "Iteration 1450:  0.7592592359616345\n",
      "Iteration 1500:  0.7592592360382575\n",
      "Iteration 1550:  0.7592592361143367\n",
      "Iteration 1600:  0.7592592361899722\n",
      "Iteration 1650:  0.7592592362650659\n",
      "Iteration 1700:  0.7592592363397175\n",
      "Iteration 1750:  0.7592592364138514\n",
      "Iteration 1800:  0.7592592364875266\n",
      "Iteration 1850:  0.7592592365607109\n",
      "Iteration 1900:  0.7592592366334191\n",
      "Iteration 1950:  0.7592592367056593\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 6 score: 0.0\n",
      "Iteration 0:  0.7962962673150666\n",
      "Iteration 50:  0.7962962674362042\n",
      "Iteration 100:  0.7962962675562891\n",
      "Iteration 150:  0.7962962676753681\n",
      "Iteration 200:  0.7962962677934865\n",
      "Iteration 250:  0.7962962679106098\n",
      "Iteration 300:  0.796296268026787\n",
      "Iteration 350:  0.7962962681419768\n",
      "Iteration 400:  0.7962962682562746\n",
      "Iteration 450:  0.7962962683696\n",
      "Iteration 500:  0.7962962684820543\n",
      "Iteration 550:  0.796296268593587\n",
      "Iteration 600:  0.7962962687042334\n",
      "Iteration 650:  0.7962962688139852\n",
      "Iteration 700:  0.7962962689228645\n",
      "Iteration 750:  0.7962962690308928\n",
      "Iteration 800:  0.7962962691380531\n",
      "Iteration 850:  0.7962962692443603\n",
      "Iteration 900:  0.7962962693498804\n",
      "Iteration 950:  0.7962962694544999\n",
      "Iteration 1000:  0.7962962695584088\n",
      "Iteration 1050:  0.7962962696614655\n",
      "Iteration 1100:  0.7962962697637377\n",
      "Iteration 1150:  0.7962962698652195\n",
      "Iteration 1200:  0.7962962699659181\n",
      "Iteration 1250:  0.7962962700658547\n",
      "Iteration 1300:  0.7962962701650004\n",
      "Iteration 1350:  0.7962962702634375\n",
      "Iteration 1400:  0.7962962703611381\n",
      "Iteration 1450:  0.796296270458089\n",
      "Iteration 1500:  0.796296270554326\n",
      "Iteration 1550:  0.7962962706498096\n",
      "Iteration 1600:  0.7962962707446379\n",
      "Iteration 1650:  0.7962962708387485\n",
      "Iteration 1700:  0.796296270932169\n",
      "Iteration 1750:  0.7962962710248321\n",
      "Iteration 1800:  0.7962962711169317\n",
      "Iteration 1850:  0.796296271208291\n",
      "Iteration 1900:  0.7962962712990255\n",
      "Iteration 1950:  0.7962962713890925\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 7 score: 0.0\n",
      "Iteration 0:  0.8703703415842743\n",
      "Iteration 50:  0.8703703417030911\n",
      "Iteration 100:  0.8703703418209704\n",
      "Iteration 150:  0.8703703419378546\n",
      "Iteration 200:  0.8703703420537574\n",
      "Iteration 250:  0.8703703421687621\n",
      "Iteration 300:  0.8703703422828132\n",
      "Iteration 350:  0.8703703423959283\n",
      "Iteration 400:  0.8703703425081537\n",
      "Iteration 450:  0.8703703426194734\n",
      "Iteration 500:  0.8703703427298816\n",
      "Iteration 550:  0.8703703428394416\n",
      "Iteration 600:  0.8703703429481306\n",
      "Iteration 650:  0.8703703430559595\n",
      "Iteration 700:  0.8703703431629142\n",
      "Iteration 750:  0.8703703432690881\n",
      "Iteration 800:  0.8703703433744034\n",
      "Iteration 850:  0.8703703434788854\n",
      "Iteration 900:  0.8703703435825736\n",
      "Iteration 950:  0.8703703436854677\n",
      "Iteration 1000:  0.8703703437875612\n",
      "Iteration 1050:  0.8703703438888886\n",
      "Iteration 1100:  0.8703703439894354\n",
      "Iteration 1150:  0.8703703440892183\n",
      "Iteration 1200:  0.8703703441882333\n",
      "Iteration 1250:  0.870370344286513\n",
      "Iteration 1300:  0.8703703443840486\n",
      "Iteration 1350:  0.8703703444808709\n",
      "Iteration 1400:  0.8703703445769682\n",
      "Iteration 1450:  0.8703703446723453\n",
      "Iteration 1500:  0.8703703447670289\n",
      "Iteration 1550:  0.8703703448609991\n",
      "Iteration 1600:  0.8703703449542813\n",
      "Iteration 1650:  0.8703703450468753\n",
      "Iteration 1700:  0.8703703451388288\n",
      "Iteration 1750:  0.8703703452301069\n",
      "Iteration 1800:  0.8703703453206985\n",
      "Iteration 1850:  0.8703703454106371\n",
      "Iteration 1900:  0.8703703454999379\n",
      "Iteration 1950:  0.8703703455886338\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 8 score: 0.0\n",
      "Iteration 0:  0.8703703456789433\n",
      "Iteration 50:  0.8703703457662897\n",
      "Iteration 100:  0.8703703458530725\n",
      "Iteration 150:  0.8703703459392358\n",
      "Iteration 200:  0.8703703460247257\n",
      "Iteration 250:  0.870370346109719\n",
      "Iteration 300:  0.870370346194061\n",
      "Iteration 350:  0.8703703462778409\n",
      "Iteration 400:  0.8703703463610224\n",
      "Iteration 450:  0.8703703464436414\n",
      "Iteration 500:  0.8703703465256707\n",
      "Iteration 550:  0.8703703466071437\n",
      "Iteration 600:  0.8703703466880872\n",
      "Iteration 650:  0.8703703467684585\n",
      "Iteration 700:  0.8703703468482923\n",
      "Iteration 750:  0.8703703469275742\n",
      "Iteration 800:  0.8703703470063291\n",
      "Iteration 850:  0.870370347084546\n",
      "Iteration 900:  0.870370347162256\n",
      "Iteration 950:  0.8703703472394351\n",
      "Iteration 1000:  0.8703703473160954\n",
      "Iteration 1050:  0.8703703473922657\n",
      "Iteration 1100:  0.8703703474679187\n",
      "Iteration 1150:  0.8703703475430877\n",
      "Iteration 1200:  0.8703703476177465\n",
      "Iteration 1250:  0.8703703476919211\n",
      "Iteration 1300:  0.8703703477656298\n",
      "Iteration 1350:  0.8703703478388386\n",
      "Iteration 1400:  0.8703703479115862\n",
      "Iteration 1450:  0.8703703479838341\n",
      "Iteration 1500:  0.8703703480556603\n",
      "Iteration 1550:  0.8703703481270124\n",
      "Iteration 1600:  0.870370348197885\n",
      "Iteration 1650:  0.8703703482683378\n",
      "Iteration 1700:  0.8703703483383156\n",
      "Iteration 1750:  0.8703703484078793\n",
      "Iteration 1800:  0.8703703484769802\n",
      "Iteration 1850:  0.8703703485456507\n",
      "Iteration 1900:  0.870370348613903\n",
      "Iteration 1950:  0.870370348681712\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 9 score: 0.0\n",
      "Iteration 0:  0.870370348749076\n",
      "Iteration 50:  0.8703703488160559\n",
      "Iteration 100:  0.8703703488826148\n",
      "Iteration 150:  0.8703703489487497\n",
      "Iteration 200:  0.8703703490145037\n",
      "Iteration 250:  0.8703703490798357\n",
      "Iteration 300:  0.8703703491447708\n",
      "Iteration 350:  0.8703703492093218\n",
      "Iteration 400:  0.870370349273461\n",
      "Iteration 450:  0.8703703493372132\n",
      "Iteration 500:  0.8703703494005876\n",
      "Iteration 550:  0.8703703494635948\n",
      "Iteration 600:  0.8703703495262056\n",
      "Iteration 650:  0.87037034958845\n",
      "Iteration 700:  0.8703703496503042\n",
      "Iteration 750:  0.870370349711814\n",
      "Iteration 800:  0.8703703497729454\n",
      "Iteration 850:  0.8703703498337187\n",
      "Iteration 900:  0.8703703498941144\n",
      "Iteration 950:  0.8703703499541772\n",
      "Iteration 1000:  0.8703703500138718\n",
      "Iteration 1050:  0.8703703500732579\n",
      "Iteration 1100:  0.8703703501322654\n",
      "Iteration 1150:  0.8703703501909248\n",
      "Iteration 1200:  0.8703703502492488\n",
      "Iteration 1250:  0.8703703503072463\n",
      "Iteration 1300:  0.8703703503648996\n",
      "Iteration 1350:  0.8703703504222206\n",
      "Iteration 1400:  0.8703703504792151\n",
      "Iteration 1450:  0.87037035053589\n",
      "Iteration 1500:  0.8703703505922289\n",
      "Iteration 1550:  0.8703703506482493\n",
      "Iteration 1600:  0.8703703507039606\n",
      "Iteration 1650:  0.8703703507593584\n",
      "Iteration 1700:  0.8703703508144315\n",
      "Iteration 1750:  0.870370350869211\n",
      "Iteration 1800:  0.8703703509236739\n",
      "Iteration 1850:  0.8703703509778389\n",
      "Iteration 1900:  0.8703703510316951\n",
      "Iteration 1950:  0.8703703510852561\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 10 score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "model = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "kFold(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create random n instances\n",
    "def create_random_instance(n):\n",
    "    rand_array = []\n",
    "    n_attr = 4\n",
    "    for i in range (n):\n",
    "        rand_row = []\n",
    "        for j in range (n_attr):\n",
    "            rand_row.append(round(np.random.uniform(0, 7), 2))\n",
    "        rand_array.append(rand_row)\n",
    "    return(rand_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: \n",
      "[[6.46 6.11 3.18 5.55]\n",
      " [2.93 2.26 5.28 4.73]\n",
      " [5.43 3.5  6.95 6.01]\n",
      " ...\n",
      " [0.29 0.9  0.67 6.9 ]\n",
      " [2.63 1.06 0.02 4.05]\n",
      " [6.85 1.04 1.14 6.62]]\n",
      "result: \n",
      "[[1.00000000e+00 9.99999941e-01 1.00000000e+00 1.00000000e+00\n",
      "  9.99999943e-01 1.00000000e+00 1.00000000e+00 9.99999994e-01\n",
      "  2.53855075e-03 5.74396136e-07 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99754162e-01 1.00000000e+00 1.00000000e+00\n",
      "  9.99757160e-01 9.61341280e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999998e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.70809665e-05 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.93424853e-01 1.00000000e+00 8.05122501e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999998e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99998026e-01 1.00000000e+00\n",
      "  9.99999983e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  6.65176520e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 4.25353987e-08 1.00000000e+00\n",
      "  9.33610308e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  4.85434858e-07 1.00000000e+00 1.00000000e+00 9.99999999e-01\n",
      "  9.99999932e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.41217882e-06 1.00000000e+00 4.36573994e-04\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99958002e-01 9.99999998e-01 2.33217489e-05\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999983e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99998832e-01\n",
      "  9.99956822e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.95160522e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999998e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99999931e-01 9.99999964e-01 3.37092675e-04 9.99999995e-01\n",
      "  9.99999454e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.06939795e-11 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99566139e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.95418928e-05 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999998e-01\n",
      "  3.45413114e-01 1.00000000e+00 9.99962513e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999996e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99889679e-01 6.81817160e-01\n",
      "  1.00000000e+00 9.99486001e-10 1.00000000e+00 1.00000000e+00\n",
      "  9.99955938e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 9.99999997e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  6.03457234e-02 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.48419918e-01 1.00000000e+00 1.00000000e+00 9.54007167e-15\n",
      "  1.00000000e+00 4.70203921e-10 9.99999957e-01 5.57493418e-07\n",
      "  1.00000000e+00 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
      "  4.65271869e-02 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99597254e-01 9.99999999e-01 1.15177078e-03 1.00000000e+00\n",
      "  5.15085543e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99000618e-01 1.00000000e+00 9.99324642e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999904e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99995106e-01 5.35400555e-09 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99999981e-01 9.98835502e-01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  3.27137321e-03 9.99969048e-01 1.00000000e+00 7.67233709e-01\n",
      "  1.00000000e+00 2.76400539e-02 9.99998227e-01 9.99999973e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99691735e-01\n",
      "  9.03793049e-01 1.42648428e-10 1.00000000e+00 1.00000000e+00\n",
      "  9.99999995e-01 9.99828350e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99957929e-01 9.99999999e-01 1.00000000e+00\n",
      "  1.00000000e+00 9.76605331e-01 1.00000000e+00 9.99999993e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99297941e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99436522e-01 4.94931168e-16 2.05391929e-05 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 2.41559196e-07 9.19214208e-01\n",
      "  9.99981311e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  6.12928170e-02 8.42295700e-01 1.00000000e+00 1.00000000e+00\n",
      "  9.99900099e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99973380e-01 9.99999930e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  6.47225780e-07 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      " [1.50118716e-09 6.46601729e-05 2.26251100e-18 3.13095701e-12\n",
      "  2.08810734e-03 3.50659656e-18 3.65893859e-12 1.58736303e-12\n",
      "  9.95748339e-01 5.63193766e-04 6.88305264e-13 6.31116605e-20\n",
      "  2.83373197e-62 7.67029439e-07 5.10116913e-16 1.26287515e-21\n",
      "  1.05855192e-01 9.67486964e-01 7.84439504e-58 1.04773796e-12\n",
      "  5.66950692e-25 3.48839366e-13 4.20165009e-05 1.28134658e-21\n",
      "  8.94935364e-15 2.77403421e-03 3.01307818e-23 8.84193233e-43\n",
      "  6.69856701e-23 2.51692898e-04 1.42641218e-16 6.05466630e-01\n",
      "  2.89215842e-03 1.44374000e-06 1.59022744e-04 1.72021314e-47\n",
      "  3.05077302e-15 2.25460402e-17 4.41968593e-52 1.60080565e-55\n",
      "  9.28212747e-01 5.38302940e-18 1.54188611e-02 2.51860063e-26\n",
      "  3.93383408e-05 1.87577150e-16 1.01365940e-46 5.11649705e-29\n",
      "  6.87919278e-12 5.39365543e-23 2.90954238e-08 9.39630529e-49\n",
      "  9.27215561e-01 1.91757459e-42 2.93150658e-26 2.37183999e-04\n",
      "  3.74977209e-31 9.99735073e-01 5.71307030e-04 9.88559026e-01\n",
      "  9.93606754e-01 2.35866003e-22 2.01724717e-09 8.09095519e-01\n",
      "  8.13741353e-01 8.31646464e-15 5.78479295e-06 4.60767091e-12\n",
      "  9.90053454e-01 5.16422453e-29 3.81128697e-15 2.94908165e-23\n",
      "  3.15283155e-16 6.13534896e-01 9.98397569e-01 9.90774886e-01\n",
      "  9.99899619e-01 7.65707048e-20 9.99680673e-01 6.63129267e-15\n",
      "  7.08322104e-11 1.00018267e-08 6.60176622e-02 7.93628626e-03\n",
      "  1.53100491e-15 1.97180745e-58 2.07630991e-10 1.29515589e-07\n",
      "  3.57285933e-24 3.30162007e-05 3.34496444e-15 9.98206714e-01\n",
      "  7.44594908e-04 1.05411754e-22 2.46620358e-19 1.23229942e-33\n",
      "  6.72735446e-07 2.08943032e-26 2.45208968e-31 8.22880064e-03\n",
      "  1.46750352e-40 8.56878947e-46 1.02430815e-02 5.60163496e-06\n",
      "  1.30423358e-30 9.99336282e-01 8.17067057e-12 1.97109681e-05\n",
      "  7.83218354e-04 3.62335926e-10 4.13606606e-04 9.49254585e-01\n",
      "  1.68168166e-04 5.42828979e-50 2.46230340e-06 9.45659257e-05\n",
      "  6.68273939e-02 1.78826350e-48 1.19454742e-04 1.72386538e-39\n",
      "  3.39130442e-04 3.02993148e-22 1.75736241e-11 6.70180168e-31\n",
      "  1.87792497e-68 7.86451461e-05 5.68098862e-04 1.01332316e-59\n",
      "  1.91124726e-09 2.09678842e-27 1.44865208e-21 9.89911940e-06\n",
      "  9.28846600e-01 1.08474275e-19 9.85205275e-01 2.06490761e-08\n",
      "  6.47872585e-15 1.51797099e-08 1.19771164e-29 2.58576110e-20\n",
      "  2.62555850e-10 5.36592751e-14 2.66127211e-22 3.39684728e-13\n",
      "  4.57704122e-01 4.30960358e-51 1.30654070e-01 1.28122208e-04\n",
      "  2.51232940e-21 1.39691103e-01 9.86128455e-05 9.58730639e-10\n",
      "  9.41925567e-01 2.53637044e-35 2.34219247e-28 1.99689721e-23\n",
      "  7.19573005e-28 1.69309106e-12 7.75230011e-42 2.28240072e-08\n",
      "  2.24179967e-64 9.99814204e-01 3.17632128e-19 7.13397962e-37\n",
      "  1.43245863e-01 5.46559662e-18 1.75412553e-18 9.99161738e-01\n",
      "  5.23332425e-14 5.96597244e-13 2.90182101e-07 2.59669158e-03\n",
      "  9.38138241e-01 2.79944794e-18 2.28552420e-15 9.99867823e-01\n",
      "  8.17814606e-02 3.12055144e-29 1.37443901e-07 2.57457672e-01\n",
      "  2.65788847e-11 5.02083407e-01 9.75022008e-01 1.47291668e-02\n",
      "  7.16330531e-12 3.94836732e-10 9.88941458e-01 6.55109736e-17\n",
      "  9.80218161e-01 1.86557554e-42 9.17251872e-11 9.98481320e-01\n",
      "  5.56526092e-02 9.97212083e-01 1.64312836e-02 3.18726168e-57\n",
      "  4.12491640e-01 4.39747665e-16 2.64519717e-01 5.04396557e-07\n",
      "  1.92946286e-01 5.16378087e-40 1.55659224e-01 8.71188167e-01\n",
      "  1.68004385e-08 7.56976495e-04 1.59508401e-25 4.41962287e-03\n",
      "  4.82623952e-39 9.24953279e-38 1.21587726e-43 7.51568335e-02\n",
      "  1.37351265e-05 2.26572378e-02 6.40301464e-09 1.83630343e-07\n",
      "  4.14970417e-16 1.26765361e-10 7.15703652e-01 6.93546269e-09\n",
      "  1.75814844e-06 7.59027891e-58 7.02580687e-12 2.72909332e-18\n",
      "  6.58984542e-01 3.19774351e-11 3.42574853e-21 9.70470808e-01\n",
      "  9.70177331e-01 1.10347188e-04 1.86205773e-03 2.32851439e-15\n",
      "  9.99924816e-01 4.13450541e-21 8.15587059e-24 6.56155487e-03\n",
      "  7.49706502e-04 1.69120106e-03 1.19950493e-11 2.32114652e-44\n",
      "  9.98369265e-01 6.36443638e-01 4.63377575e-13 3.62865287e-10\n",
      "  4.61309473e-07 4.81465572e-09 2.16483518e-06 1.14280187e-14\n",
      "  7.08357221e-06 1.02277313e-03 1.48568147e-02 7.80854999e-06\n",
      "  6.96812140e-42 9.98198726e-01 3.87453561e-39 2.56240276e-06\n",
      "  9.98013470e-01 9.99558759e-01 1.56721100e-12 1.07723370e-10\n",
      "  1.12082669e-03 5.59688889e-45 9.95226113e-01 2.42916135e-07\n",
      "  9.88331297e-01 1.30973594e-01 7.95784952e-02 3.21891446e-48\n",
      "  8.71613624e-01 1.28162269e-04 9.13812451e-01 9.51259568e-01\n",
      "  9.78487014e-01 3.92434856e-63 8.39854364e-12 7.18889754e-09\n",
      "  9.25961548e-13 2.71818029e-34 2.54613099e-13 9.89686161e-01\n",
      "  1.24653457e-03 5.80363102e-01 8.72345925e-39 3.51374348e-26\n",
      "  9.99408718e-01 3.22708534e-21 4.37646945e-22 9.10395415e-38\n",
      "  2.09709844e-05 9.64996456e-01 6.08947824e-04 9.70769312e-08\n",
      "  1.28916120e-07 7.04671146e-15 1.06197476e-09 3.96192029e-19\n",
      "  8.80327460e-01 1.41011267e-25 2.93211028e-23 2.02090662e-55]\n",
      " [9.99999999e-01 9.99134977e-01 1.00000000e+00 1.00000000e+00\n",
      "  9.99882570e-01 3.02487829e-08 1.03763137e-01 4.47894174e-01\n",
      "  2.10702519e-02 9.99984322e-01 1.00000000e+00 3.92079011e-22\n",
      "  1.00000000e+00 9.99999987e-01 3.48089479e-08 1.00000000e+00\n",
      "  7.19206685e-01 6.77574083e-01 1.00000000e+00 9.99999999e-01\n",
      "  9.99999976e-01 1.00000000e+00 9.99993338e-01 1.00000000e+00\n",
      "  2.00686018e-18 9.99883628e-01 6.05925062e-13 1.00000000e+00\n",
      "  1.00000000e+00 9.99560259e-01 2.52804676e-02 5.17371574e-01\n",
      "  1.52874992e-02 1.29243603e-08 8.78682576e-15 1.00000000e+00\n",
      "  1.77070736e-21 1.96965025e-07 1.00000000e+00 1.00000000e+00\n",
      "  3.56926908e-03 1.00000000e+00 9.17309528e-01 1.00000000e+00\n",
      "  9.99999999e-01 6.49580632e-17 1.00000000e+00 2.54027603e-16\n",
      "  1.00000000e+00 1.71159346e-15 5.80954629e-01 1.00000000e+00\n",
      "  9.09224767e-01 1.00000000e+00 1.00000000e+00 9.88148641e-01\n",
      "  1.00000000e+00 8.85591644e-05 9.99995731e-01 2.87624426e-02\n",
      "  5.20302111e-02 1.47628216e-12 9.97821036e-01 3.10576384e-01\n",
      "  8.84604372e-01 3.65860126e-17 9.98708627e-01 3.19923828e-14\n",
      "  3.22615372e-01 1.00000000e+00 1.00000000e+00 2.37396342e-16\n",
      "  8.34430257e-10 9.54610549e-01 6.97558792e-02 8.42272121e-01\n",
      "  9.64213373e-04 2.21838569e-20 1.34564789e-04 1.00000000e+00\n",
      "  4.12216496e-07 1.00000000e+00 3.40497678e-01 9.99628800e-01\n",
      "  2.08819496e-15 1.00000000e+00 1.00000000e+00 9.99999995e-01\n",
      "  7.28873504e-19 1.34315439e-10 1.00000000e+00 7.02220237e-02\n",
      "  9.98236946e-01 1.00000000e+00 2.16056399e-24 1.00000000e+00\n",
      "  2.21923155e-01 1.20866246e-04 1.00000000e+00 9.94645510e-01\n",
      "  1.00000000e+00 1.00000000e+00 1.19642621e-01 9.99991061e-01\n",
      "  1.70298829e-03 8.08078275e-02 3.27866046e-06 7.83167821e-07\n",
      "  2.09952751e-04 3.75147860e-12 9.99948796e-01 5.38089367e-03\n",
      "  9.98907278e-01 1.00000000e+00 9.99971044e-01 9.96240380e-01\n",
      "  9.99685304e-01 1.00000000e+00 9.99971789e-01 9.99999472e-01\n",
      "  9.99988523e-01 9.40493215e-26 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 9.99999626e-01 9.90544658e-01 1.00000000e+00\n",
      "  9.99999882e-01 9.95987034e-01 3.64133094e-01 9.99998095e-01\n",
      "  7.94616690e-01 3.80308685e-04 4.24308692e-01 9.99999582e-01\n",
      "  1.00000000e+00 9.99999961e-01 1.00000000e+00 2.80375538e-06\n",
      "  1.52194528e-04 9.31582451e-19 1.92998757e-14 9.99999957e-01\n",
      "  1.16072881e-06 1.00000000e+00 7.71312890e-01 9.99998992e-01\n",
      "  9.99999949e-01 9.98892179e-01 9.92169635e-01 8.51666937e-01\n",
      "  2.24311683e-02 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 3.50832619e-12 1.00000000e+00 2.62167068e-02\n",
      "  1.00000000e+00 9.17396872e-03 1.13674952e-10 1.00000000e+00\n",
      "  5.74572240e-02 9.99635093e-01 1.00000000e+00 2.71348829e-05\n",
      "  1.00000000e+00 1.00000000e+00 5.76011069e-01 1.04964952e-02\n",
      "  8.21686117e-01 1.00000000e+00 2.73635472e-08 1.10402387e-04\n",
      "  9.56325234e-01 1.00000000e+00 9.99999802e-01 9.99779784e-01\n",
      "  1.58705083e-05 9.96155312e-01 4.74668143e-02 9.99640574e-01\n",
      "  3.49079653e-03 7.19451846e-13 7.94431941e-04 2.68910339e-03\n",
      "  7.85955357e-01 1.00000000e+00 9.84361188e-01 1.65214947e-02\n",
      "  9.96135184e-01 1.47709685e-02 9.97039187e-01 1.00000000e+00\n",
      "  9.64518845e-01 1.00074399e-14 5.73454269e-07 4.11564398e-11\n",
      "  5.84006772e-01 1.00000000e+00 8.57870912e-01 7.56193315e-01\n",
      "  9.99998745e-01 7.57188505e-09 2.75244655e-12 9.99280486e-01\n",
      "  1.00000000e+00 9.99999894e-01 1.00000000e+00 1.03289984e-01\n",
      "  9.99999718e-01 9.99802976e-01 9.99999999e-01 2.60251789e-13\n",
      "  1.26249164e-04 4.92613707e-08 1.60848477e-01 2.81968640e-10\n",
      "  9.99996634e-01 1.00000000e+00 2.41569592e-03 5.33895031e-04\n",
      "  9.55337594e-01 1.05821599e-12 6.44873950e-23 2.50110596e-01\n",
      "  6.55960216e-02 9.99981083e-01 9.87190106e-01 6.13364307e-21\n",
      "  3.82863537e-03 1.00000000e+00 1.65318535e-16 9.81412650e-01\n",
      "  9.99148959e-01 9.99971734e-01 1.81482863e-06 1.00000000e+00\n",
      "  5.09613064e-02 8.45895552e-01 1.00000000e+00 1.89032882e-10\n",
      "  1.24395915e-12 1.00000000e+00 9.99999977e-01 2.05532040e-04\n",
      "  9.99768728e-01 9.99998169e-01 7.43968064e-01 9.99999060e-01\n",
      "  1.00000000e+00 1.01884591e-01 1.00000000e+00 1.86425023e-04\n",
      "  8.99903784e-03 6.71880966e-04 8.96521391e-01 6.19939742e-06\n",
      "  9.99175909e-01 1.00000000e+00 6.40097630e-02 5.74455868e-13\n",
      "  3.20833430e-01 9.99813540e-01 9.97535916e-01 1.00000000e+00\n",
      "  3.00930690e-06 9.89250594e-01 8.21689854e-01 6.98034178e-01\n",
      "  4.66059228e-03 1.00000000e+00 2.37627037e-19 6.20587728e-13\n",
      "  1.00000000e+00 1.00000000e+00 3.10245035e-17 2.99679366e-04\n",
      "  9.99357646e-01 8.80773895e-01 1.00000000e+00 1.00000000e+00\n",
      "  2.16629071e-02 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99957330e-01 8.58326820e-02 9.95137724e-01 1.00000000e+00\n",
      "  1.11070610e-08 4.45217067e-10 2.01137204e-13 5.31889687e-07\n",
      "  9.66589553e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#create new instances & predict them using scratch model\n",
    "new_instances = create_random_instance(300)\n",
    "result = model_scratch.predict(new_instances)\n",
    "\n",
    "#print instance & result of predict\n",
    "print(\"instance: \")\n",
    "print(np.array(new_instances))\n",
    "print(\"result: \")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
