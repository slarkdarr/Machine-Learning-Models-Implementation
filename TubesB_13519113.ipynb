{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Softmax\n",
    "def cross_entropy(target):\n",
    "    # print(target)\n",
    "    # Rusak\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    self.weight = np.random.rand(output, input)\n",
    "    self.bias = np.random.rand(output, 1)\n",
    "    self.activation = activation\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.delta = np.zeros(output)\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def net(self, input):\n",
    "    result = np.dot(self.weight, input) + self.bias\n",
    "    # except:\n",
    "    #   print(\"!\")\n",
    "    #   print(input.shape)\n",
    "    #   print(input_T.shape)\n",
    "    #   print(self.weight.shape)\n",
    "    return result\n",
    "\n",
    "  def forward_propagation(self, input):\n",
    "    net = self.net(input)\n",
    "    return activation_function[self.activation](net)\n",
    "    \n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target) if self.activation == \"Softmax\" else cost_function[self.activation](target, output)\n",
    "        \n",
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "    # arr = np.array(input)\n",
    "    # if arr.ndim == 1:\n",
    "    #   instance = arr\n",
    "    #   instance_res = instance\n",
    "    #   for layer in self.layers:\n",
    "    #     instance_res = layer.forward_propagation(instance_res)\n",
    "    #   return instance_res.tolist()\n",
    "\n",
    "    # batch = arr\n",
    "    # batch_res = []\n",
    "    # for instance in batch:\n",
    "    #   instance_res = instance\n",
    "    #   for layer in self.layers:\n",
    "    #     instance_res = layer.forward_propagation(instance_res)\n",
    "      \n",
    "    #   batch_res.append(instance_res.tolist())\n",
    "    \n",
    "    # return batch_res\n",
    "        \n",
    "  def load_file(self, filename):\n",
    "    '''\n",
    "    File format\n",
    "    <depth>\n",
    "    <units> <activation function>\n",
    "    <weight0> \n",
    "    <bias0>\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "      depth = int(file.readline().strip())\n",
    "      for i in range (depth):\n",
    "        line = file.readline().strip().split()\n",
    "        unit = int(line[0])\n",
    "        activation = line[1]\n",
    "\n",
    "        # Weight Matrix\n",
    "        weight = []\n",
    "        for j in range(unit):\n",
    "          weight.append(list(map(float, file.readline().strip().split())))\n",
    "\n",
    "        # Bias Matrix\n",
    "        bias = list(map(float, file.readline().strip().split()))\n",
    "        \n",
    "        # Add layer\n",
    "        layer = Layer(weight, bias, activation)\n",
    "        self.add(layer)\n",
    "      \n",
    "      # End of file\n",
    "    # Close file\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_return, y_return = [], []\n",
    "\n",
    "    for i in arr_id:\n",
    "        X_return.append(list(X[i]))\n",
    "        y_return.append(list(y[i]))\n",
    "\n",
    "    return X_return, y_return\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "    # Shuffle data\n",
    "    data_X, data_y = self.shuffle(X, y)\n",
    "    \n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(data_X[head : tail])\n",
    "      batch_y.append(data_y[head : tail])\n",
    "      # batch_x.append(X[head : tail])\n",
    "      # batch_y.append(y[head : tail])\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr\n",
    "      net = layer.net(arr)\n",
    "      arr = activation_function[layer.activation](net)\n",
    "    return arr\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        # gradient *= layer.calculate_error(target = y, output = output, derivative = True)\n",
    "        dO = activation_function[layer.activation](x = layer.net(layer.data_in), derivative = True)\n",
    "        if (layer.activation != \"Softmax\"):\n",
    "          # gradient = (-1) * self.delta(target = y, output = output) * X\n",
    "          dE = - (y - output) # Derivative of MSE\n",
    "        else:\n",
    "          # Derivative of Cross Entropy\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "        \n",
    "        # prevl = self.layers[i-1]\n",
    "        # o = prevl.net(prevl.data_in)\n",
    "        \n",
    "        gdelta  = dE * dO.T\n",
    "        \n",
    "        #print(\"gdelta\", gdelta.shape)\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, gdelta)\n",
    "        gdelta = error * activation_function[layer.activation](x = layer.net(layer.data_in), derivative = True)\n",
    "      \n",
    "      # if i != 0:\n",
    "      #   prevl = self.layer[i-1]\n",
    "      #   input_layer = activation_function[prevl.activation](x = prevl.net())\n",
    "      # else:\n",
    "      #   input_layer = X\n",
    "      # print(\"dET\")\n",
    "      # print(dE.T.shape)\n",
    "      # print(\"dO\")\n",
    "      # print(dO.shape)\n",
    "\n",
    "      layer.delta_weight = np.dot(gdelta, layer.data_in) * self.learning_rate\n",
    "      layer.delta_bias = gdelta * self.learning_rate\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(X, y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        prediction = self.predict(X_train).T\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_train, prediction)\n",
    "      \n",
    "        # Backward propagation to count delta or gradient\n",
    "        self.backward_propagation(X_train, y_train, prediction)\n",
    "\n",
    "        # Update weight\n",
    "        for layer in self.layers:\n",
    "          print(layer.bias.shape)\n",
    "          print(layer.delta_bias.shape)\n",
    "          layer.weight += layer.delta_weight\n",
    "          layer.bias += layer.delta_bias\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1 / len(X)\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reach`ed maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(data['feature_names'], '\\n', y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "model_sklearn = MLPClassifier(max_iter = 1000)\n",
    "model_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sklearn = model_sklearn.predict(X_test)\n",
    "print(prediction_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(prediction_sklearn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scratch = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 1000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model_scratch.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model_scratch.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model_scratch.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model_scratch.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "# Layer Output\n",
    "# model_scratch.add(Layer(\"Softmax\", 3, 3))\n",
    "\n",
    "model_scratch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()\n",
    "model_scratch.mgd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((5,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
