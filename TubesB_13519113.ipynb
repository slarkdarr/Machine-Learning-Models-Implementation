{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Cross Entopy (Softmax)\n",
    "def cross_entropy(target, output):\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_upper_threshold = 5\n",
    "clip_lower_threshold = 0.5\n",
    "def clip(x):\n",
    "    ret = x\n",
    "    norm = np.sum(x * x)\n",
    "    if norm > clip_upper_threshold ** 2:\n",
    "        ret = ret * (clip_upper_threshold / np.sqrt(norm))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    self.weight = np.random.rand(output, input)\n",
    "    self.bias = np.random.rand(output, 1)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.delta = np.zeros(output)\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def net(self):\n",
    "    net = np.dot(self.weight, self.data_in) + self.bias\n",
    "    return net\n",
    "\n",
    "  def output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net)\n",
    "  \n",
    "  def derivative_output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net, derivative = True)\n",
    "\n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "        \n",
    "  def load_file(self, filename):\n",
    "    '''\n",
    "    ### DEPRECATED ###\n",
    "    File format\n",
    "    <depth>\n",
    "    <units> <activation function>\n",
    "    <weight0> \n",
    "    <bias0>\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "      depth = int(file.readline().strip())\n",
    "      for i in range (depth):\n",
    "        line = file.readline().strip().split()\n",
    "        unit = int(line[0])\n",
    "        activation = line[1]\n",
    "\n",
    "        # Weight Matrix\n",
    "        weight = []\n",
    "        for j in range(unit):\n",
    "          weight.append(list(map(float, file.readline().strip().split())))\n",
    "\n",
    "        # Bias Matrix\n",
    "        bias = list(map(float, file.readline().strip().split()))\n",
    "        \n",
    "        # Add layer\n",
    "        layer = Layer(weight, bias, activation)\n",
    "        self.add(layer)\n",
    "      \n",
    "      # End of file\n",
    "    # Close file\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr_in = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr_in\n",
    "      arr_in = layer.output()\n",
    "    return arr_in\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_result = [] \n",
    "    y_result = []\n",
    "\n",
    "#     try:\n",
    "    for i in arr_id:\n",
    "      X_result.append(list(X[i]))\n",
    "      y_result.append(list(y[i]))\n",
    "#     except:\n",
    "#         print(type(arr_id))\n",
    "#         print(arr_id)\n",
    "#         print(X)\n",
    "#         print(y)\n",
    "\n",
    "    return X_result, y_result\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "    \n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(np.array(X[head : tail]))\n",
    "      batch_y.append(np.array(y[head : tail]))\n",
    "        \n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        if (layer.activation == \"Softmax\"):\n",
    "          # Derivative of Cross Entropy times derivative output\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "          layer.delta = clip(dE * layer.derivative_output())\n",
    "        else:\n",
    "          # Derivative of MSE times derivative output\n",
    "          dE = (output - y)\n",
    "          layer.delta = clip(dE * layer.derivative_output())\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, nextl.delta)\n",
    "        layer.delta = error * layer.derivative_output()\n",
    "    \n",
    "      layer.delta_weight = clip(np.dot(layer.delta, layer.data_in.T) * self.learning_rate)\n",
    "      layer.delta_bias = clip(layer.delta * self.learning_rate)\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Shuffle data\n",
    "      data_X, data_y = self.shuffle(X, y)\n",
    "\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(data_X, data_y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        y_predict = self.forward_propagation(X_train)\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_predict, y_train.T)\n",
    "      \n",
    "        # Backward propagation to count delta\n",
    "        self.backward_propagation(X_train.T, y_train.T, y_predict)\n",
    "\n",
    "        # Update each layer\n",
    "        for layer in self.layers:\n",
    "          # Update weight\n",
    "          layer.weight += layer.delta_weight\n",
    "\n",
    "          # Update bias\n",
    "          delta_bias = layer.delta_bias\n",
    "          layer.bias += np.sum(delta_bias, axis=1).reshape(len(delta_bias), 1)\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1 / len(X)\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reached maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_sklearn = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "prediction_sklearn = model_sklearn.predict(X_test)\n",
    "print(prediction_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(prediction_sklearn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "model_scratch = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "\n",
    "# Layer 1\n",
    "model_scratch.add(Layer(\"ReLU\", 4, 10))\n",
    "# Layer 2\n",
    "model_scratch.add(Layer(\"ReLU\", 10, 10))\n",
    "# Layer 3\n",
    "model_scratch.add(Layer(\"Linear\", 10, 5))\n",
    "# Layer 4\n",
    "model_scratch.add(Layer(\"Sigmoid\", 5, 3))\n",
    "\n",
    "# Layer Output\n",
    "model_scratch.mgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah layer:  4\n",
      "============================================================\n",
      "Layer 1 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[0.39197195 0.80487849 0.08901206 0.97277294]\n",
      " [0.17378321 0.66068846 0.02117359 0.94674468]\n",
      " [0.8316787  0.48436075 0.27544551 0.76061833]\n",
      " [0.94311292 0.29420751 0.44675564 0.99289372]\n",
      " [0.99845535 0.11453258 0.76941347 0.90086565]\n",
      " [0.24263891 0.18805312 0.34384888 0.78591562]\n",
      " [0.03984703 0.56574264 0.38302135 0.74069951]\n",
      " [0.60705419 0.35689064 0.87011459 0.15487822]\n",
      " [0.83372127 0.58202184 0.13530059 0.1721255 ]\n",
      " [0.12889095 0.0629536  0.08731491 0.41460441]]\n",
      "Bias:\n",
      "[[0.75775004]\n",
      " [0.67971567]\n",
      " [0.86451071]\n",
      " [0.34272421]\n",
      " [0.7040273 ]\n",
      " [0.34097599]\n",
      " [0.60497078]\n",
      " [0.27630508]\n",
      " [0.72588838]\n",
      " [0.60829054]]\n",
      "============================================================\n",
      "Layer 2 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[9.29375257e-01 8.57373305e-01 7.37899346e-02 1.91151745e-01\n",
      "  2.41221580e-01 5.02277179e-01 2.39209631e-01 6.32224951e-01\n",
      "  3.71605159e-01 2.77212516e-01]\n",
      " [3.19190584e-01 9.93838471e-01 2.34748444e-01 6.48095698e-02\n",
      "  9.26156238e-01 5.96046603e-01 9.00956929e-01 3.96559939e-01\n",
      "  7.02432957e-01 5.87316167e-01]\n",
      " [6.96730274e-01 1.07452608e-01 4.83569097e-01 7.03495804e-01\n",
      "  9.31821874e-02 3.30215709e-01 8.54864412e-01 1.91338057e-01\n",
      "  9.08721118e-02 9.25884584e-02]\n",
      " [3.40126214e-01 8.54406482e-02 4.55312150e-01 1.39118750e-01\n",
      "  4.47873225e-01 3.26164248e-01 8.23919332e-01 3.43814352e-01\n",
      "  9.14554185e-02 9.28160610e-01]\n",
      " [1.84983213e-01 3.19624893e-01 7.45609959e-01 7.85040969e-02\n",
      "  7.97042232e-01 7.30112821e-01 2.57165655e-01 8.86628744e-01\n",
      "  3.25885566e-01 3.94871638e-01]\n",
      " [2.00182291e-01 7.02906378e-01 3.00002432e-01 4.39188492e-01\n",
      "  1.09995816e-01 1.45504735e-01 8.75655218e-02 8.69954955e-01\n",
      "  5.65332699e-01 3.82937291e-04]\n",
      " [6.95440634e-01 5.39968974e-02 8.27414941e-01 7.32224869e-01\n",
      "  1.03795981e-01 9.45731293e-02 6.19940962e-01 7.72533976e-02\n",
      "  9.26849579e-01 7.78889916e-01]\n",
      " [8.93894385e-01 9.72369982e-01 4.02258462e-01 8.24571399e-01\n",
      "  7.03357590e-01 9.14614011e-01 1.57661859e-01 8.60156427e-01\n",
      "  1.33205535e-02 7.92594162e-01]\n",
      " [8.86549719e-01 7.04795398e-01 4.25913194e-01 4.96553995e-01\n",
      "  8.52386529e-01 8.68092307e-01 3.36334891e-01 4.94592453e-01\n",
      "  6.12778334e-01 5.47209546e-01]\n",
      " [1.25308286e-02 4.88509645e-02 7.47132933e-01 3.52621403e-01\n",
      "  8.03446993e-01 2.84454486e-01 3.47846680e-01 8.17751848e-01\n",
      "  9.66415815e-01 8.78689441e-01]]\n",
      "Bias:\n",
      "[[0.71270556]\n",
      " [0.33862462]\n",
      " [0.27415087]\n",
      " [0.73002764]\n",
      " [0.22088763]\n",
      " [0.65049215]\n",
      " [0.41371248]\n",
      " [0.55469692]\n",
      " [0.33923664]\n",
      " [0.6530242 ]]\n",
      "============================================================\n",
      "Layer 3 (Activation: \"Linear\", Units: 5)\n",
      "Weight:\n",
      "[[0.92658796 0.19598451 0.48536957 0.19193485 0.97494185 0.38850877\n",
      "  0.47758145 0.25845843 0.24157237 0.99660633]\n",
      " [0.073226   0.34017796 0.27942688 0.63935219 0.94705144 0.29172026\n",
      "  0.66150947 0.24334602 0.61935232 0.06790804]\n",
      " [0.35683097 0.45679709 0.04637817 0.79133362 0.46910728 0.91725635\n",
      "  0.63696742 0.62729686 0.6571668  0.31873408]\n",
      " [0.62211111 0.24242991 0.99074866 0.86324756 0.6862922  0.03926395\n",
      "  0.38486323 0.80063051 0.9739904  0.52628885]\n",
      " [0.92140265 0.22826941 0.16468889 0.09534933 0.08142681 0.38316539\n",
      "  0.71063754 0.31430275 0.03909968 0.78256449]]\n",
      "Bias:\n",
      "[[0.07098777]\n",
      " [0.97310259]\n",
      " [0.67974776]\n",
      " [0.36776206]\n",
      " [0.69505003]]\n",
      "============================================================\n",
      "Layer 4 (Activation: \"Sigmoid\", Units: 3)\n",
      "Weight:\n",
      "[[0.03033655 0.28063786 0.84649892 0.77109133 0.83287933]\n",
      " [0.18550914 0.8773512  0.11711891 0.32763183 0.15192288]\n",
      " [0.6802598  0.02794055 0.68413776 0.4852394  0.01211088]]\n",
      "Bias:\n",
      "[[0.06898426]\n",
      " [0.6720785 ]\n",
      " [0.25542117]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model_scratch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "prediction_scratch = model_scratch.predict(X_test)\n",
    "print(prediction_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(prediction_scratch, y_test.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
