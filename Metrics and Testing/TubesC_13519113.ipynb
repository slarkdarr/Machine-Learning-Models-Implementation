{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi aktivasi\n",
    "def linear(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return x\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def relu(x, derivative = False):\n",
    "  if not (derivative):\n",
    "    return np.maximum(0, x)\n",
    "  return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def softmax(x, derivative = False):\n",
    "    if not (derivative):\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = x[:,i].reshape(-1,1)\n",
    "        resTemp = np.diagflat(temp) - np.dot(temp, temp.T)\n",
    "        result[:,i] = np.sum(resTemp, axis=1)\n",
    "    return result\n",
    "\n",
    "activation_function = {\n",
    "    \"Linear\": linear,\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"ReLU\": relu,\n",
    "    \"Softmax\": softmax,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi loss\n",
    "\n",
    "# Sum of squared errors (Linear, Sigmoid, ReLU)\n",
    "def sum_of_squared_errors(target, output):\n",
    "    return 0.5 * np.sum((target - output)**2)\n",
    "\n",
    "# Cross Entopy (Softmax)\n",
    "def cross_entropy(target, output):\n",
    "    result = (-1)*math.log(target)\n",
    "    return result\n",
    "\n",
    "cost_function = {\n",
    "    \"Linear\": sum_of_squared_errors,\n",
    "    \"Sigmoid\": sum_of_squared_errors,\n",
    "    \"ReLU\": sum_of_squared_errors,\n",
    "    \"Softmax\": cross_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, activation, input, output):\n",
    "    if activation not in ['Linear', 'Sigmoid', 'ReLU', 'Softmax']:\n",
    "          raise NotImplementedError(\"Layer activation `%s` is not implemented.\" \n",
    "                                      % activation)\n",
    "    np.random.seed(69)\n",
    "    self.weight = np.random.randn(output, input)\n",
    "    self.bias = np.random.randn(output, 1)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.delta = np.zeros(output)\n",
    "    self.delta_weight = np.zeros((output, input))\n",
    "    self.delta_bias = np.ones((output, 1))\n",
    "    self.data_in = np.zeros(output)\n",
    "\n",
    "  def set_weight(self, weight):\n",
    "    self.weight = weight\n",
    "\n",
    "  def set_bias(self, bias):\n",
    "    self.bias = bias\n",
    "    \n",
    "  def net(self):\n",
    "    net = np.dot(self.weight, self.data_in) + self.bias\n",
    "    return net\n",
    "\n",
    "  def output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net)\n",
    "  \n",
    "  def derivative_output(self):\n",
    "    net = self.net()\n",
    "    return activation_function[self.activation](x = net, derivative = True)\n",
    "\n",
    "  def calculate_error(self, target, output):\n",
    "    return cost_function[self.activation](target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate, error_threshold, max_iter, batch_size):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold \n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "  \n",
    "  def summary(self):\n",
    "    print(\"Learning rate: \", self.learning_rate)\n",
    "    print(\"Error threshold: \", self.error_threshold)\n",
    "    print(\"Max iteration: \", self.max_iter)\n",
    "    print(\"Batch size: \", self.batch_size)\n",
    "    print(\"Jumlah layer: \", len(self.layers))\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      print(\"============================================================\")\n",
    "      print('Layer {} (Activation: \"{}\", Units: {})'.format(i+1, layer.activation, len(layer.weight)))\n",
    "      print(\"Weight:\")\n",
    "      print(np.array(layer.weight))\n",
    "      print(\"Bias:\")\n",
    "      print(np.array(layer.bias))\n",
    "    print(\"============================================================\")\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def predict(self, input):\n",
    "    return self.forward_propagation(input)\n",
    "\n",
    "  def save_file(self, filename) :\n",
    "    f = open(filename,\"w\")\n",
    "\n",
    "    n_layer = len(self.layers)\n",
    "    layer_arr = []\n",
    "\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      weight_arr = layer.weight.tolist()\n",
    "      bias_arr = layer.bias.tolist()\n",
    "      activation = layer.activation\n",
    "      layer_content = {\n",
    "        \"weight\" : weight_arr,\n",
    "        \"bias\" : bias_arr,\n",
    "        \"activation\" : activation\n",
    "      }\n",
    "\n",
    "      layer_arr.append(layer_content)\n",
    "      \n",
    "    content = {\n",
    "      \"n_layer\" : n_layer,\n",
    "      \"layers\" : layer_arr,\n",
    "      \"learning_rate\" : self.learning_rate,\n",
    "      \"error_threshold\" : self.error_threshold,\n",
    "      \"max_iter\" : self.max_iter,\n",
    "      \"batch_size\" : self.batch_size\n",
    "    }\n",
    "\n",
    "    json.dump(content, f, indent = 4)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "  def load_file(self, filename) :\n",
    "    with open(filename) as json_model :\n",
    "      dataModel = json.load(json_model)\n",
    "      \n",
    "      for i in range (dataModel[\"n_layer\"]):\n",
    "        bias = dataModel[\"layers\"][i][\"bias\"]\n",
    "        weight = dataModel[\"layers\"][i][\"weight\"]\n",
    "        activation = dataModel[\"layers\"][i][\"activation\"]\n",
    "\n",
    "        inputx = len(dataModel[\"layers\"][i][\"weight\"][0])\n",
    "        outputy = len(dataModel[\"layers\"][i][\"weight\"])\n",
    "\n",
    "        tempLayer = Layer(activation,inputx,outputy)\n",
    "        tempLayer.set_weight(np.array(weight))\n",
    "        tempLayer.set_bias(np.array(bias))\n",
    "\n",
    "        self.add(tempLayer)\n",
    "\n",
    "      self.learning_rate = dataModel[\"learning_rate\"]\n",
    "      self.error_threshold = dataModel[\"error_threshold\"] \n",
    "      self.max_iter = dataModel[\"max_iter\"]\n",
    "      self.batch_size = dataModel[\"batch_size\"]\n",
    "    print('File loaded. Model detected')\n",
    "\n",
    "  def forward_propagation(self, inputs):\n",
    "    arr_in = np.array(inputs).T\n",
    "    for layer in self.layers:\n",
    "      layer.data_in = arr_in\n",
    "      arr_in = layer.output()\n",
    "    return arr_in\n",
    "\n",
    "  def shuffle(self, X, y):\n",
    "    arr_id = [i for i in range(len(y))]\n",
    "    np.random.shuffle(arr_id)\n",
    "    X_result = [] \n",
    "    y_result = []\n",
    "\n",
    "    for i in arr_id:\n",
    "      X_result.append(list(X[i]))\n",
    "      y_result.append(list(y[i]))\n",
    "\n",
    "    return X_result, y_result\n",
    "\n",
    "  def create_batch(self, X, y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    epoch = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "    for i in range(0, epoch):\n",
    "      head = i * self.batch_size\n",
    "      tail = (i + 1) * self.batch_size\n",
    "      batch_x.append(np.array(X[head : tail]))\n",
    "      batch_y.append(np.array(y[head : tail]))\n",
    "        \n",
    "    return batch_x, batch_y\n",
    "\n",
    "  def backward_propagation(self, X, y, output):\n",
    "    for i, layer in reversed(list(enumerate(self.layers))):\n",
    "      # Output Layer Chain Rule      \n",
    "      if (i == len(self.layers)-1):\n",
    "        if (layer.activation == \"Softmax\"):\n",
    "          # Derivative of Cross Entropy times derivative output\n",
    "          dE = y\n",
    "          for j in range(y.shape[1]):\n",
    "            k = np.argmax(y[:, i])\n",
    "            dE[k, j] = -(1 - dE[k, j])\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "        else:\n",
    "          # Derivative of MSE times derivative output\n",
    "          dE = -(output - y)\n",
    "          layer.delta = dE * layer.derivative_output()\n",
    "      \n",
    "      # Hidden Layer Chain Rule\n",
    "      else:\n",
    "        nextl = self.layers[i + 1]\n",
    "        error = np.dot(nextl.weight.T, nextl.delta)\n",
    "        layer.delta = error * layer.derivative_output()\n",
    "    \n",
    "      layer.delta_weight = np.dot(layer.delta, layer.data_in.T) * self.learning_rate\n",
    "      layer.delta_bias = layer.delta * self.learning_rate\n",
    "       \n",
    "  def mgd(self, X, y):\n",
    "    for iteration in range(0, self.max_iter):\n",
    "      # Shuffle data\n",
    "      data_X, data_y = self.shuffle(X, y)\n",
    "\n",
    "      # Divide into batch\n",
    "      batch_x, batch_y = self.create_batch(data_X, data_y)\n",
    "      batches = len(batch_x)\n",
    "      \n",
    "      error = 0  \n",
    "      for batch in range(0, batches):\n",
    "        X_train = batch_x[batch]\n",
    "        y_train = batch_y[batch]\n",
    "\n",
    "        # Forward propagation on input\n",
    "        y_predict = self.forward_propagation(X_train)\n",
    "\n",
    "        # Compute cost\n",
    "        error += self.layers[-1].calculate_error(y_predict, y_train.T)\n",
    "      \n",
    "        # Backward propagation to count delta\n",
    "        self.backward_propagation(X_train.T, y_train.T, y_predict)\n",
    "\n",
    "        # Update each layer\n",
    "        for layer in self.layers:\n",
    "          # Update weight\n",
    "          layer.weight += layer.delta_weight\n",
    "\n",
    "          # Update bias\n",
    "          delta_bias = layer.delta_bias\n",
    "          layer.bias += np.sum(delta_bias, axis=1).reshape(len(delta_bias), 1)\n",
    "          \n",
    "          # Reset delta value\n",
    "          layer.delta_weight = np.zeros(layer.weight.shape)\n",
    "          layer.delta_bias = np.zeros(layer.bias.shape)\n",
    "      \n",
    "      error *= 1/len(X)\n",
    "      if iteration % 50 == 0:\n",
    "        print(f\"Iteration {iteration}: \", error)\n",
    "\n",
    "      if error <= self.error_threshold:\n",
    "        print(\"Error is lower or equal than error threshold\")\n",
    "        print(\"Ended in {} iterations\".format(iteration))\n",
    "        return\n",
    "    \n",
    "    print(\"Reached maximum iterations\")\n",
    "    print(\"Ended in {} iterations\".format(self.max_iter))\n",
    "    return\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_confusion_matrix(self, y_true, y_pred):\n",
    "        K = len(np.unique(y_true)) # Number of classes \n",
    "        result = np.zeros((K, K))\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            result[y_true[i]][y_pred[i]] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def accuracy_score(self, y_true, y_pred, normalize=True):\n",
    "        if normalize:\n",
    "            acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "        else:\n",
    "            acc = np.sum(np.equal(y_true, y_pred))\n",
    "\n",
    "        return acc\n",
    "  \n",
    "    def precision_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        pred_unique, pred_counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "        if class_label is not None:\n",
    "            class_index = np.where(pred_unique == class_label)[0][0]\n",
    "\n",
    "            tp = np.sum(np.equal(y_true, class_label) & np.equal(y_pred, class_label))\n",
    "            total = pred_counts[class_index]\n",
    "\n",
    "            prec = tp/total\n",
    "        \n",
    "        else:\n",
    "            if binary:\n",
    "                tp = np.sum(np.equal(y_true, 1) & np.equal(y_pred, 1))\n",
    "                total = pred_counts[1]\n",
    "\n",
    "                prec = tp/total\n",
    "            else:\n",
    "                prec = []\n",
    "                for (i, val) in enumerate(pred_unique):\n",
    "                    class_index = np.where(pred_unique == val)[0][0]\n",
    "\n",
    "                    tp = np.sum(np.equal(y_true, val) & np.equal(y_pred, val))\n",
    "                    total = pred_counts[i]\n",
    "\n",
    "                    prec.append(tp/total)\n",
    "        \n",
    "        return prec\n",
    "\n",
    "    def recall_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        true_unique, true_counts = np.unique(y_true, return_counts=True)\n",
    "\n",
    "        if class_label is not None:\n",
    "            class_index = np.where(true_unique == class_label)[0][0]\n",
    "\n",
    "            tp = np.sum(np.equal(y_true, class_label) & np.equal(y_pred, class_label))\n",
    "            total = true_counts[class_index]\n",
    "\n",
    "            prec = tp/total\n",
    "        \n",
    "        else:\n",
    "            if binary:\n",
    "                tp = np.sum(np.equal(y_true, 1) & np.equal(y_pred, 1))\n",
    "                total = true_counts[1]\n",
    "\n",
    "                prec = tp/total\n",
    "            else:\n",
    "                prec = []\n",
    "                for (i, val) in enumerate(true_unique):\n",
    "                    class_index = np.where(true_unique == val)[0][0]\n",
    "\n",
    "                    tp = np.sum(np.equal(y_true, val) & np.equal(y_pred, val))\n",
    "                    total = true_counts[i]\n",
    "\n",
    "                    prec.append(tp/total)\n",
    "\n",
    "        return prec\n",
    "\n",
    "    def f1_score(self, y_true, y_pred, binary, class_label=None):\n",
    "        prec = self.precision_score(y_true, y_pred, binary, class_label)\n",
    "        rec = self.recall_score(y_true, y_pred, binary, class_label)\n",
    "\n",
    "        if class_label is not None:\n",
    "            f1 = 2 * ((prec * rec)/(prec + rec))\n",
    "        else:\n",
    "            f1 = []\n",
    "            class_length = len(prec) or len(rec)\n",
    "            for i in range(class_length):\n",
    "                f1_scr = (2 * ((prec[i] * rec[i])/(prec[i] + rec[i]))) if (prec[i]+rec[i] != 0) else 0.0\n",
    "\n",
    "                f1.append(f1_scr)\n",
    "\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(model, X, y, n_splits=10, scratch=True):\n",
    "  data_X, data_y = X, y\n",
    "  total_score = 0\n",
    "    \n",
    "  if len(X) != len(y):\n",
    "    raise Exception(\"Length X and y is not the same\")\n",
    "\n",
    "  data_size = len(X)\n",
    "  fold_size = data_size // n_splits\n",
    "  remainder = data_size % n_splits\n",
    "  last_idx = fold_size * data_size + remainder\n",
    "  for test_idx in range(n_splits):\n",
    "    # Split the datasets\n",
    "    head_test = fold_size * test_idx\n",
    "    # If last fold, same as last_idx\n",
    "    tail_test = last_idx if (test_idx == n_splits - 1) else fold_size * (test_idx + 1)\n",
    "      \n",
    "    X_left_train = data_X[0 : head_test]\n",
    "    X_right_train = data_X[tail_test : last_idx]\n",
    "    y_left_train = data_y[0 : head_test]\n",
    "    y_right_train = data_y[tail_test : last_idx]\n",
    "\n",
    "    X_train = np.concatenate((X_left_train, X_right_train))\n",
    "    y_train = np.concatenate((y_left_train, y_right_train))\n",
    "    X_test = data_X[head_test : tail_test]\n",
    "    y_test = data_y[head_test : tail_test]\n",
    "\n",
    "    # Train the dataset\n",
    "    if not scratch:\n",
    "      model.fit(X_train, y_train)\n",
    "    else:\n",
    "      model.mgd(X_train, y_train)\n",
    "\n",
    "    # Get the prediction\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    # Count the score\n",
    "    accuracy = score_prediction(prediction, y_test, scratch)\n",
    "    print(\"Fold {} score: {}\".format(test_idx+1, accuracy))\n",
    "    total_score += accuracy\n",
    "\n",
    "  average_score = total_score / n_splits\n",
    "  return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset iris\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.8344637342734189\n",
      "Iteration 50:  0.8332005585350447\n",
      "Iteration 100:  0.8329270120479051\n",
      "Iteration 150:  0.8408951777201468\n",
      "Iteration 200:  0.6666710273140173\n",
      "Iteration 250:  0.6666685874639616\n",
      "Iteration 300:  0.6666678931719247\n",
      "Iteration 350:  0.6666675661129715\n",
      "Iteration 400:  0.6666673761900029\n",
      "Iteration 450:  0.6666672522160623\n",
      "Iteration 500:  0.6666671649645266\n",
      "Iteration 550:  0.6666671002529414\n",
      "Iteration 600:  0.6666670503578325\n",
      "Iteration 650:  0.6666670107211013\n",
      "Iteration 700:  0.6666669784787386\n",
      "Iteration 750:  0.6666669517411594\n",
      "Iteration 800:  0.6666669292116243\n",
      "Iteration 850:  0.6666669099706497\n",
      "Iteration 900:  0.6666668933482317\n",
      "Iteration 950:  0.6666668788446424\n",
      "Iteration 1000:  0.6666668660796106\n",
      "Iteration 1050:  0.6666668547586463\n",
      "Iteration 1100:  0.6666668446501725\n",
      "Iteration 1150:  0.6666668355695461\n",
      "Iteration 1200:  0.6666668273678303\n",
      "Iteration 1250:  0.6666668199234266\n",
      "Iteration 1300:  0.6666668131362572\n",
      "Iteration 1350:  0.6666668069231957\n",
      "Iteration 1400:  0.6666668012140855\n",
      "Iteration 1450:  0.6666667959504193\n",
      "Iteration 1500:  0.6666667910819725\n",
      "Iteration 1550:  0.666666786565909\n",
      "Iteration 1600:  0.6666667823653499\n",
      "Iteration 1650:  0.6666667784483893\n",
      "Iteration 1700:  0.6666667747872782\n",
      "Iteration 1750:  0.6666667713577721\n",
      "Iteration 1800:  0.6666667681385928\n",
      "Iteration 1850:  0.6666667651109908\n",
      "Iteration 1900:  0.6666667622583775\n",
      "Iteration 1950:  0.6666667595660264\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "# Melakukan pembelajaran dalam batch\n",
    "model_sklearn_1 = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn_1.fit(X, y)\n",
    "\n",
    "model_scratch_1 = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "model_scratch_1.add(Layer(\"ReLU\", 4, 10)) # Layer 1\n",
    "model_scratch_1.add(Layer(\"ReLU\", 10, 10)) # Layer 2\n",
    "model_scratch_1.add(Layer(\"Linear\", 10, 5)) # Layer 3\n",
    "model_scratch_1.add(Layer(\"Sigmoid\", 5, 3)) # Layer Output\n",
    "model_scratch_1.mgd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(prediction, y, scratch=True):\n",
    "  if scratch:\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "      label_pred.append(np.argmax(prediction[:, i]))\n",
    "\n",
    "    label_y = []\n",
    "    for i in range(y.shape[0]):\n",
    "      label_y.append(np.argmax(y[i, :]))\n",
    "    \n",
    "    return accuracy_score(label_pred, label_y)\n",
    "\n",
    "  else:\n",
    "    return (accuracy_score(prediction, y))\n",
    "\n",
    "# Prediksi menggunakan fungsi buatan sendiri\n",
    "metrics = Metrics()\n",
    "def score_self_prediction(prediction, y, scratch=True):\n",
    "  label_y = []\n",
    "  for i in range(y.shape[0]):\n",
    "    label_y.append(np.argmax(y[i, :]))\n",
    "\n",
    "  if scratch:\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[1]):\n",
    "      label_pred.append(np.argmax(prediction[:, i]))\n",
    "  \n",
    "  else:\n",
    "    label_pred = []\n",
    "    for i in range(prediction.shape[0]):\n",
    "      label_pred.append(np.argmax(prediction[i, :]))\n",
    "\n",
    "  acc = metrics.accuracy_score(label_y, label_pred)\n",
    "  prec = metrics.precision_score(label_y, label_pred, binary=False)\n",
    "  rec = metrics.recall_score(label_y, label_pred, binary=False)\n",
    "  f1 = metrics.f1_score(label_y, label_pred, binary=False)\n",
    "  \n",
    "  return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Membandingkan kedua hasil prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil model sklearn: 0.98\n",
      "Hasil model scratch: 0.3333333333333333\n",
      "\n",
      "Hasil model sklearn prediksi sendiri:\n",
      "Accuracy score: \n",
      "0.98\n",
      "Precision score: \n",
      "[1.0, 0.9795918367346939, 0.9607843137254902]\n",
      "Recall score: \n",
      "[1.0, 0.96, 0.98]\n",
      "F1 Score: \n",
      "[1.0, 0.9696969696969697, 0.9702970297029702]\n",
      "\n",
      "Hasil model scratch prediksi sendiri:\n",
      "Accuracy score: \n",
      "0.3333333333333333\n",
      "Precision score: \n",
      "[0.3333333333333333]\n",
      "Recall score: \n",
      "[1.0, 0.0, 0.0]\n",
      "F1 Score: \n",
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "prediction_sklearn_1 = model_sklearn_1.predict(X)\n",
    "print(\"Hasil model sklearn: {}\".format(score_prediction(prediction_sklearn_1, y, scratch=False)))\n",
    "\n",
    "prediction_scratch_1 = model_scratch_1.predict(X)\n",
    "print(\"Hasil model scratch: {}\".format(score_prediction(prediction_scratch_1, y)))\n",
    "\n",
    "acc, prec, rec, f1 = score_self_prediction(prediction_sklearn_1, y, scratch=False)\n",
    "print(f\"\\nHasil model sklearn prediksi sendiri:\\nAccuracy score: \\n{acc}\\nPrecision score: \\n{prec}\\nRecall score: \\n{rec}\\nF1 Score: \\n{f1}\\n\")\n",
    "\n",
    "acc, prec, rec, f1 = score_self_prediction(prediction_scratch_1, y)\n",
    "print(f\"Hasil model scratch prediksi sendiri:\\nAccuracy score: \\n{acc}\\nPrecision score: \\n{prec}\\nRecall score: \\n{rec}\\nF1 Score: \\n{f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Melakukan pembelajaran dengan skema split train 90% dan test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan split data train 90% dan data test 10%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:  0.8301444158043019\n",
      "Iteration 50:  0.82591081197782\n",
      "Iteration 100:  0.8258389976544714\n",
      "Iteration 150:  0.8257702190919224\n",
      "Iteration 200:  0.8251163118877485\n",
      "Iteration 250:  0.8259257226866845\n",
      "Iteration 300:  0.8259257167984402\n",
      "Iteration 350:  0.8259257105574979\n",
      "Iteration 400:  0.8259257039297215\n",
      "Iteration 450:  0.8259256968785782\n",
      "Iteration 500:  0.825925689364013\n",
      "Iteration 550:  0.8259256813334239\n",
      "Iteration 600:  0.8259256727366858\n",
      "Iteration 650:  0.8259256635099537\n",
      "Iteration 700:  0.8259256535830756\n",
      "Iteration 750:  0.8259256428669772\n",
      "Iteration 800:  0.8259256312681275\n",
      "Iteration 850:  0.8259256186752185\n",
      "Iteration 900:  0.8259256049506337\n",
      "Iteration 950:  0.825925589930705\n",
      "Iteration 1000:  0.8259255734313599\n",
      "Iteration 1050:  0.8259255552161039\n",
      "Iteration 1100:  0.8259255350057972\n",
      "Iteration 1150:  0.8259255124402604\n",
      "Iteration 1200:  0.8259254871061538\n",
      "Iteration 1250:  0.8259254584336465\n",
      "Iteration 1300:  0.8259254257294232\n",
      "Iteration 1350:  0.8259253880785669\n",
      "Iteration 1400:  0.825925344254569\n",
      "Iteration 1450:  0.8259252926004716\n",
      "Iteration 1500:  0.8259252308053665\n",
      "Iteration 1550:  0.8259251555683002\n",
      "Iteration 1600:  0.825925061929076\n",
      "Iteration 1650:  0.8259249421980717\n",
      "Iteration 1700:  0.8259247836325577\n",
      "Iteration 1750:  0.825924563680757\n",
      "Iteration 1800:  0.8259242379067966\n",
      "Iteration 1850:  0.8259237054360504\n",
      "Iteration 1900:  0.8259226769841588\n",
      "Iteration 1950:  0.8259198451405849\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n"
     ]
    }
   ],
   "source": [
    "# Melakukan pembelajaran menggunakan data split\n",
    "model_sklearn_2 = MLPClassifier(max_iter = 2000)\n",
    "model_sklearn_2.fit(X_train, y_train)\n",
    "\n",
    "model_scratch_2 = NeuralNetwork(learning_rate = 0.001, error_threshold = 0.01, max_iter = 2000, batch_size = 5)\n",
    "model_scratch_2.add(Layer(\"ReLU\", 4, 10)) # Layer 1\n",
    "model_scratch_2.add(Layer(\"ReLU\", 10, 10)) # Layer 2\n",
    "model_scratch_2.add(Layer(\"Linear\", 10, 5)) # Layer 3\n",
    "model_scratch_2.add(Layer(\"Sigmoid\", 5, 3)) # Layer Output\n",
    "model_scratch_2.mgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil model sklearn: 0.9333333333333333\n",
      "Hasil model scratch: 0.4\n",
      "\n",
      "Hasil model sklearn prediksi sendiri:\n",
      "Accuracy score: \n",
      "1.0\n",
      "Precision score: \n",
      "[1.0, 1.0, 1.0]\n",
      "Recall score: \n",
      "[1.0, 1.0, 1.0]\n",
      "F1 Score: \n",
      "[1.0, 1.0, 1.0]\n",
      "\n",
      "Hasil model scratch prediksi sendiri:\n",
      "Accuracy score: \n",
      "0.4\n",
      "Precision score: \n",
      "[0.4]\n",
      "Recall score: \n",
      "[1.0, 0.0, 0.0]\n",
      "F1 Score: \n",
      "[0.5714285714285715]\n"
     ]
    }
   ],
   "source": [
    "# Membandingkan kedua hasil prediksi\n",
    "prediction_sklearn_2 = model_sklearn_2.predict(X_test)\n",
    "print(\"Hasil model sklearn: {}\".format(score_prediction(prediction_sklearn_2, y_test, scratch=False)))\n",
    "\n",
    "prediction_scratch_2 = model_scratch_2.predict(X_test)\n",
    "print(\"Hasil model scratch: {}\".format(score_prediction(prediction_scratch_2, y_test)))\n",
    "\n",
    "acc, prec, rec, f1 = score_self_prediction(prediction_sklearn_2, y_test, scratch=False)\n",
    "print(f\"\\nHasil model sklearn prediksi sendiri:\\nAccuracy score: \\n{acc}\\nPrecision score: \\n{prec}\\nRecall score: \\n{rec}\\nF1 Score: \\n{f1}\\n\")\n",
    "\n",
    "acc, prec, rec, f1 = score_self_prediction(prediction_scratch_2, y_test)\n",
    "print(f\"Hasil model scratch prediksi sendiri:\\nAccuracy score: \\n{acc}\\nPrecision score: \\n{prec}\\nRecall score: \\n{rec}\\nF1 Score: \\n{f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Melakukan pembelajaran dengan skema 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 score: 1.0\n",
      "Fold 2 score: 1.0\n",
      "Fold 3 score: 1.0\n",
      "Fold 4 score: 1.0\n",
      "Fold 5 score: 0.8\n",
      "Fold 6 score: 0.9333333333333333\n",
      "Fold 7 score: 1.0\n",
      "Fold 8 score: 1.0\n",
      "Fold 9 score: 0.8666666666666667\n",
      "Fold 10 score: 1.0\n",
      "Iteration 0:  0.8703118509550839\n",
      "Iteration 50:  0.8703590550465418\n",
      "Iteration 100:  0.7407586417846139\n",
      "Iteration 150:  0.7407439533361143\n",
      "Iteration 200:  0.7407424871062128\n",
      "Iteration 250:  0.740741936619904\n",
      "Iteration 300:  0.7407416489303442\n",
      "Iteration 350:  0.7407414723463874\n",
      "Iteration 400:  0.7407413530099225\n",
      "Iteration 450:  0.7407412670057694\n",
      "Iteration 500:  0.7407412021009188\n",
      "Iteration 550:  0.7407411513918069\n",
      "Iteration 600:  0.7407411106856907\n",
      "Iteration 650:  0.7407410772943687\n",
      "Iteration 700:  0.740741049411305\n",
      "Iteration 750:  0.740741025779417\n",
      "Iteration 800:  0.7407410054967896\n",
      "Iteration 850:  0.7407409878995699\n",
      "Iteration 900:  0.7407409724883186\n",
      "Iteration 950:  0.7407409588801004\n",
      "Iteration 1000:  0.7407409467764612\n",
      "Iteration 1050:  0.740740935941584\n",
      "Iteration 1100:  0.7407409261853578\n",
      "Iteration 1150:  0.7407409173553193\n",
      "Iteration 1200:  0.7407409093254929\n",
      "Iteration 1250:  0.7407409019919727\n",
      "Iteration 1300:  0.740740895268068\n",
      "Iteration 1350:  0.7407408890808966\n",
      "Iteration 1400:  0.740740883368741\n",
      "Iteration 1450:  0.7407408780789886\n",
      "Iteration 1500:  0.7407408731665154\n",
      "Iteration 1550:  0.7407408685924447\n",
      "Iteration 1600:  0.7407408643228871\n",
      "Iteration 1650:  0.7407408603285717\n",
      "Iteration 1700:  0.7407408565837073\n",
      "Iteration 1750:  0.7407408530656613\n",
      "Iteration 1800:  0.740740849754458\n",
      "Iteration 1850:  0.7407408466323967\n",
      "Iteration 1900:  0.7407408436837402\n",
      "Iteration 1950:  0.7407408408944484\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 1 score: 1.0\n",
      "Iteration 0:  0.7407407408590818\n",
      "Iteration 50:  0.7407407408590773\n",
      "Iteration 100:  0.7407407408590728\n",
      "Iteration 150:  0.7407407408590683\n",
      "Iteration 200:  0.7407407408590634\n",
      "Iteration 250:  0.740740740859059\n",
      "Iteration 300:  0.7407407408590542\n",
      "Iteration 350:  0.7407407408590497\n",
      "Iteration 400:  0.7407407408590452\n",
      "Iteration 450:  0.7407407408590405\n",
      "Iteration 500:  0.7407407408590361\n",
      "Iteration 550:  0.7407407408590313\n",
      "Iteration 600:  0.7407407408590267\n",
      "Iteration 650:  0.7407407408590222\n",
      "Iteration 700:  0.7407407408590179\n",
      "Iteration 750:  0.7407407408590131\n",
      "Iteration 800:  0.7407407408590085\n",
      "Iteration 850:  0.7407407408590039\n",
      "Iteration 900:  0.7407407408589993\n",
      "Iteration 950:  0.7407407408589948\n",
      "Iteration 1000:  0.7407407408589902\n",
      "Iteration 1050:  0.7407407408589854\n",
      "Iteration 1100:  0.7407407408589809\n",
      "Iteration 1150:  0.7407407408589766\n",
      "Iteration 1200:  0.7407407408589719\n",
      "Iteration 1250:  0.7407407408589674\n",
      "Iteration 1300:  0.7407407408589629\n",
      "Iteration 1350:  0.740740740858958\n",
      "Iteration 1400:  0.7407407408589537\n",
      "Iteration 1450:  0.7407407408589489\n",
      "Iteration 1500:  0.7407407408589445\n",
      "Iteration 1550:  0.7407407408589397\n",
      "Iteration 1600:  0.7407407408589352\n",
      "Iteration 1650:  0.7407407408589307\n",
      "Iteration 1700:  0.740740740858926\n",
      "Iteration 1750:  0.7407407408589213\n",
      "Iteration 1800:  0.740740740858917\n",
      "Iteration 1850:  0.7407407408589124\n",
      "Iteration 1900:  0.740740740858908\n",
      "Iteration 1950:  0.7407407408589032\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 2 score: 1.0\n",
      "Iteration 0:  0.7407408380824374\n",
      "Iteration 50:  0.7407408355846025\n",
      "Iteration 100:  0.7407408332114437\n",
      "Iteration 150:  0.740740830953864\n",
      "Iteration 200:  0.7407408288036312\n",
      "Iteration 250:  0.7407408267532708\n",
      "Iteration 300:  0.7407408247959957\n",
      "Iteration 350:  0.7407408229256094\n",
      "Iteration 400:  0.7407408211364611\n",
      "Iteration 450:  0.7407408194233751\n",
      "Iteration 500:  0.740740817781607\n",
      "Iteration 550:  0.740740816206799\n",
      "Iteration 600:  0.7407408146949404\n",
      "Iteration 650:  0.7407408132423345\n",
      "Iteration 700:  0.740740811845569\n",
      "Iteration 750:  0.7407408105015028\n",
      "Iteration 800:  0.7407408092071676\n",
      "Iteration 850:  0.7407408079598986\n",
      "Iteration 900:  0.7407408067571637\n",
      "Iteration 950:  0.7407408055966272\n",
      "Iteration 1000:  0.7407408044760926\n",
      "Iteration 1050:  0.7407408033935445\n",
      "Iteration 1100:  0.7407408023470801\n",
      "Iteration 1150:  0.7407408013349269\n",
      "Iteration 1200:  0.7407408003554264\n",
      "Iteration 1250:  0.7407407994070239\n",
      "Iteration 1300:  0.7407407984882644\n",
      "Iteration 1350:  0.7407407975977799\n",
      "Iteration 1400:  0.7407407967342865\n",
      "Iteration 1450:  0.7407407958965764\n",
      "Iteration 1500:  0.740740795083513\n",
      "Iteration 1550:  0.7407407942940292\n",
      "Iteration 1600:  0.7407407935271032\n",
      "Iteration 1650:  0.7407407927817942\n",
      "Iteration 1700:  0.7407407920572012\n",
      "Iteration 1750:  0.7407407913524632\n",
      "Iteration 1800:  0.7407407906668\n",
      "Iteration 1850:  0.7407407899994033\n",
      "Iteration 1900:  0.7407407893495972\n",
      "Iteration 1950:  0.7407407887166733\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 3 score: 1.0\n",
      "Iteration 0:  0.666666714058561\n",
      "Iteration 50:  0.6666667134565549\n",
      "Iteration 100:  0.6666667128696284\n",
      "Iteration 150:  0.6666667122972176\n",
      "Iteration 200:  0.6666667117387903\n",
      "Iteration 250:  0.6666667111938416\n",
      "Iteration 300:  0.6666667106618888\n",
      "Iteration 350:  0.6666667101424729\n",
      "Iteration 400:  0.6666667096351568\n",
      "Iteration 450:  0.6666667091395254\n",
      "Iteration 500:  0.6666667086551722\n",
      "Iteration 550:  0.6666667081817239\n",
      "Iteration 600:  0.666666707718815\n",
      "Iteration 650:  0.6666667072660971\n",
      "Iteration 700:  0.6666667068232401\n",
      "Iteration 750:  0.6666667063899191\n",
      "Iteration 800:  0.6666667059658359\n",
      "Iteration 850:  0.6666667055506997\n",
      "Iteration 900:  0.6666667051442207\n",
      "Iteration 950:  0.6666667047461408\n",
      "Iteration 1000:  0.6666667043561995\n",
      "Iteration 1050:  0.6666667039741508\n",
      "Iteration 1100:  0.6666667035997544\n",
      "Iteration 1150:  0.6666667032327862\n",
      "Iteration 1200:  0.6666667028730265\n",
      "Iteration 1250:  0.6666667025202648\n",
      "Iteration 1300:  0.6666667021743012\n",
      "Iteration 1350:  0.6666667018349359\n",
      "Iteration 1400:  0.6666667015019879\n",
      "Iteration 1450:  0.666666701175273\n",
      "Iteration 1500:  0.6666667008546264\n",
      "Iteration 1550:  0.6666667005398613\n",
      "Iteration 1600:  0.6666667002308364\n",
      "Iteration 1650:  0.6666666999273897\n",
      "Iteration 1700:  0.6666666996293715\n",
      "Iteration 1750:  0.666666699336638\n",
      "Iteration 1800:  0.6666666990490495\n",
      "Iteration 1850:  0.666666698766472\n",
      "Iteration 1900:  0.6666666984887779\n",
      "Iteration 1950:  0.666666698215835\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 4 score: 0.3333333333333333\n",
      "Iteration 0:  0.6296296609106586\n",
      "Iteration 50:  0.6296296606468674\n",
      "Iteration 100:  0.6296296603874835\n",
      "Iteration 150:  0.6296296601323903\n",
      "Iteration 200:  0.6296296598814893\n",
      "Iteration 250:  0.6296296596346761\n",
      "Iteration 300:  0.6296296593918506\n",
      "Iteration 350:  0.6296296591529181\n",
      "Iteration 400:  0.6296296589177852\n",
      "Iteration 450:  0.6296296586863646\n",
      "Iteration 500:  0.6296296584585643\n",
      "Iteration 550:  0.6296296582343007\n",
      "Iteration 600:  0.6296296580134961\n",
      "Iteration 650:  0.6296296577960687\n",
      "Iteration 700:  0.6296296575819427\n",
      "Iteration 750:  0.6296296573710422\n",
      "Iteration 800:  0.6296296571632954\n",
      "Iteration 850:  0.629629656958633\n",
      "Iteration 900:  0.6296296567569851\n",
      "Iteration 950:  0.6296296565582878\n",
      "Iteration 1000:  0.6296296563624751\n",
      "Iteration 1050:  0.6296296561694846\n",
      "Iteration 1100:  0.6296296559792577\n",
      "Iteration 1150:  0.6296296557917331\n",
      "Iteration 1200:  0.6296296556068552\n",
      "Iteration 1250:  0.6296296554245684\n",
      "Iteration 1300:  0.6296296552448178\n",
      "Iteration 1350:  0.6296296550675518\n",
      "Iteration 1400:  0.6296296548927185\n",
      "Iteration 1450:  0.6296296547202683\n",
      "Iteration 1500:  0.6296296545501532\n",
      "Iteration 1550:  0.6296296543823262\n",
      "Iteration 1600:  0.6296296542167409\n",
      "Iteration 1650:  0.6296296540533535\n",
      "Iteration 1700:  0.6296296538921196\n",
      "Iteration 1750:  0.6296296537329975\n",
      "Iteration 1800:  0.6296296535759472\n",
      "Iteration 1850:  0.6296296534209251\n",
      "Iteration 1900:  0.6296296532678954\n",
      "Iteration 1950:  0.6296296531168188\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 5 score: 0.0\n",
      "Iteration 0:  0.6296296529676586\n",
      "Iteration 50:  0.6296296528203781\n",
      "Iteration 100:  0.6296296526749424\n",
      "Iteration 150:  0.6296296525313163\n",
      "Iteration 200:  0.6296296523894672\n",
      "Iteration 250:  0.6296296522493621\n",
      "Iteration 300:  0.6296296521109687\n",
      "Iteration 350:  0.6296296519742585\n",
      "Iteration 400:  0.6296296518391936\n",
      "Iteration 450:  0.6296296517057524\n",
      "Iteration 500:  0.6296296515739035\n",
      "Iteration 550:  0.6296296514436158\n",
      "Iteration 600:  0.6296296513148651\n",
      "Iteration 650:  0.6296296511876233\n",
      "Iteration 700:  0.6296296510618669\n",
      "Iteration 750:  0.6296296509375608\n",
      "Iteration 800:  0.62962965081469\n",
      "Iteration 850:  0.6296296506932255\n",
      "Iteration 900:  0.6296296505731442\n",
      "Iteration 950:  0.6296296504544223\n",
      "Iteration 1000:  0.6296296503370372\n",
      "Iteration 1050:  0.629629650220966\n",
      "Iteration 1100:  0.6296296501061873\n",
      "Iteration 1150:  0.6296296499926791\n",
      "Iteration 1200:  0.6296296498804206\n",
      "Iteration 1250:  0.6296296497693913\n",
      "Iteration 1300:  0.6296296496595721\n",
      "Iteration 1350:  0.6296296495509409\n",
      "Iteration 1400:  0.629629649443481\n",
      "Iteration 1450:  0.6296296493371725\n",
      "Iteration 1500:  0.6296296492319972\n",
      "Iteration 1550:  0.6296296491279368\n",
      "Iteration 1600:  0.629629649024974\n",
      "Iteration 1650:  0.6296296489230915\n",
      "Iteration 1700:  0.6296296488222725\n",
      "Iteration 1750:  0.6296296487224999\n",
      "Iteration 1800:  0.6296296486237581\n",
      "Iteration 1850:  0.6296296485260313\n",
      "Iteration 1900:  0.629629648429303\n",
      "Iteration 1950:  0.6296296483335602\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 6 score: 0.0\n",
      "Iteration 0:  0.6296296482387845\n",
      "Iteration 50:  0.629629648144964\n",
      "Iteration 100:  0.6296296480520832\n",
      "Iteration 150:  0.6296296479601285\n",
      "Iteration 200:  0.6296296478690859\n",
      "Iteration 250:  0.6296296477789429\n",
      "Iteration 300:  0.6296296476896842\n",
      "Iteration 350:  0.6296296476012982\n",
      "Iteration 400:  0.6296296475137724\n",
      "Iteration 450:  0.6296296474270937\n",
      "Iteration 500:  0.62962964734125\n",
      "Iteration 550:  0.62962964725623\n",
      "Iteration 600:  0.6296296471720201\n",
      "Iteration 650:  0.6296296470886106\n",
      "Iteration 700:  0.6296296470059896\n",
      "Iteration 750:  0.6296296469241455\n",
      "Iteration 800:  0.6296296468430687\n",
      "Iteration 850:  0.6296296467627462\n",
      "Iteration 900:  0.6296296466831698\n",
      "Iteration 950:  0.6296296466043285\n",
      "Iteration 1000:  0.6296296465262107\n",
      "Iteration 1050:  0.6296296464488084\n",
      "Iteration 1100:  0.6296296463721109\n",
      "Iteration 1150:  0.6296296462961093\n",
      "Iteration 1200:  0.6296296462207933\n",
      "Iteration 1250:  0.6296296461461542\n",
      "Iteration 1300:  0.6296296460721836\n",
      "Iteration 1350:  0.6296296459988701\n",
      "Iteration 1400:  0.6296296459262075\n",
      "Iteration 1450:  0.6296296458541869\n",
      "Iteration 1500:  0.6296296457827989\n",
      "Iteration 1550:  0.6296296457120347\n",
      "Iteration 1600:  0.629629645641888\n",
      "Iteration 1650:  0.6296296455723496\n",
      "Iteration 1700:  0.6296296455034116\n",
      "Iteration 1750:  0.6296296454350666\n",
      "Iteration 1800:  0.6296296453673069\n",
      "Iteration 1850:  0.6296296453001251\n",
      "Iteration 1900:  0.6296296452335135\n",
      "Iteration 1950:  0.6296296451674652\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 7 score: 0.0\n",
      "Iteration 0:  0.6296296451020454\n",
      "Iteration 50:  0.6296296450371022\n",
      "Iteration 100:  0.6296296449727008\n",
      "Iteration 150:  0.629629644908835\n",
      "Iteration 200:  0.6296296448454981\n",
      "Iteration 250:  0.6296296447826836\n",
      "Iteration 300:  0.6296296447203849\n",
      "Iteration 350:  0.6296296446585956\n",
      "Iteration 400:  0.6296296445973095\n",
      "Iteration 450:  0.6296296445365208\n",
      "Iteration 500:  0.6296296444762234\n",
      "Iteration 550:  0.6296296444164114\n",
      "Iteration 600:  0.629629644357078\n",
      "Iteration 650:  0.6296296442982189\n",
      "Iteration 700:  0.6296296442398275\n",
      "Iteration 750:  0.6296296441818989\n",
      "Iteration 800:  0.629629644124427\n",
      "Iteration 850:  0.6296296440674075\n",
      "Iteration 900:  0.6296296440108331\n",
      "Iteration 950:  0.6296296439547001\n",
      "Iteration 1000:  0.6296296438990037\n",
      "Iteration 1050:  0.6296296438437377\n",
      "Iteration 1100:  0.6296296437888979\n",
      "Iteration 1150:  0.6296296437344789\n",
      "Iteration 1200:  0.629629643680476\n",
      "Iteration 1250:  0.629629643626885\n",
      "Iteration 1300:  0.6296296435737004\n",
      "Iteration 1350:  0.6296296435209179\n",
      "Iteration 1400:  0.6296296434685335\n",
      "Iteration 1450:  0.6296296434165415\n",
      "Iteration 1500:  0.6296296433649387\n",
      "Iteration 1550:  0.6296296433137203\n",
      "Iteration 1600:  0.6296296432628828\n",
      "Iteration 1650:  0.6296296432124193\n",
      "Iteration 1700:  0.6296296431623287\n",
      "Iteration 1750:  0.6296296431126055\n",
      "Iteration 1800:  0.6296296430632462\n",
      "Iteration 1850:  0.6296296430142462\n",
      "Iteration 1900:  0.6296296429656022\n",
      "Iteration 1950:  0.62962964291731\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 8 score: 0.0\n",
      "Iteration 0:  0.6296296428693008\n",
      "Iteration 50:  0.6296296428217012\n",
      "Iteration 100:  0.6296296427744431\n",
      "Iteration 150:  0.6296296427275202\n",
      "Iteration 200:  0.6296296426809321\n",
      "Iteration 250:  0.6296296426346735\n",
      "Iteration 300:  0.6296296425887408\n",
      "Iteration 350:  0.6296296425431316\n",
      "Iteration 400:  0.6296296424978418\n",
      "Iteration 450:  0.6296296424528682\n",
      "Iteration 500:  0.6296296424082076\n",
      "Iteration 550:  0.6296296423638565\n",
      "Iteration 600:  0.6296296423198117\n",
      "Iteration 650:  0.6296296422760705\n",
      "Iteration 700:  0.6296296422326295\n",
      "Iteration 750:  0.6296296421894853\n",
      "Iteration 800:  0.6296296421466353\n",
      "Iteration 850:  0.6296296421040761\n",
      "Iteration 900:  0.6296296420618055\n",
      "Iteration 950:  0.6296296420198195\n",
      "Iteration 1000:  0.6296296419781165\n",
      "Iteration 1050:  0.6296296419366925\n",
      "Iteration 1100:  0.6296296418955449\n",
      "Iteration 1150:  0.6296296418546715\n",
      "Iteration 1200:  0.6296296418140694\n",
      "Iteration 1250:  0.6296296417737358\n",
      "Iteration 1300:  0.6296296417336681\n",
      "Iteration 1350:  0.6296296416938635\n",
      "Iteration 1400:  0.6296296416543192\n",
      "Iteration 1450:  0.6296296416150334\n",
      "Iteration 1500:  0.6296296415760031\n",
      "Iteration 1550:  0.6296296415372261\n",
      "Iteration 1600:  0.6296296414986994\n",
      "Iteration 1650:  0.6296296414604209\n",
      "Iteration 1700:  0.6296296414223883\n",
      "Iteration 1750:  0.6296296413845992\n",
      "Iteration 1800:  0.6296296413470515\n",
      "Iteration 1850:  0.6296296413097422\n",
      "Iteration 1900:  0.6296296412726697\n",
      "Iteration 1950:  0.6296296412358315\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 9 score: 0.0\n",
      "Iteration 0:  0.6296296411992248\n",
      "Iteration 50:  0.6296296411628486\n",
      "Iteration 100:  0.6296296411267003\n",
      "Iteration 150:  0.6296296410907775\n",
      "Iteration 200:  0.6296296410550781\n",
      "Iteration 250:  0.6296296410196008\n",
      "Iteration 300:  0.6296296409843422\n",
      "Iteration 350:  0.6296296409493012\n",
      "Iteration 400:  0.6296296409144754\n",
      "Iteration 450:  0.6296296408798633\n",
      "Iteration 500:  0.6296296408454626\n",
      "Iteration 550:  0.629629640811271\n",
      "Iteration 600:  0.6296296407772872\n",
      "Iteration 650:  0.6296296407435094\n",
      "Iteration 700:  0.6296296407099354\n",
      "Iteration 750:  0.629629640676563\n",
      "Iteration 800:  0.629629640643391\n",
      "Iteration 850:  0.6296296406104178\n",
      "Iteration 900:  0.6296296405776406\n",
      "Iteration 950:  0.6296296405450588\n",
      "Iteration 1000:  0.62962964051267\n",
      "Iteration 1050:  0.6296296404804724\n",
      "Iteration 1100:  0.6296296404484646\n",
      "Iteration 1150:  0.6296296404166452\n",
      "Iteration 1200:  0.6296296403850118\n",
      "Iteration 1250:  0.6296296403535635\n",
      "Iteration 1300:  0.6296296403222982\n",
      "Iteration 1350:  0.6296296402912145\n",
      "Iteration 1400:  0.6296296402603112\n",
      "Iteration 1450:  0.6296296402295855\n",
      "Iteration 1500:  0.6296296401990371\n",
      "Iteration 1550:  0.6296296401686641\n",
      "Iteration 1600:  0.6296296401384649\n",
      "Iteration 1650:  0.6296296401084385\n",
      "Iteration 1700:  0.6296296400785825\n",
      "Iteration 1750:  0.6296296400488962\n",
      "Iteration 1800:  0.6296296400193779\n",
      "Iteration 1850:  0.6296296399900261\n",
      "Iteration 1900:  0.6296296399608398\n",
      "Iteration 1950:  0.6296296399318169\n",
      "Reached maximum iterations\n",
      "Ended in 2000 iterations\n",
      "Fold 10 score: 0.0\n",
      "Rata-rata score dari model sklearn: 0.96\n",
      "Rata-rata score dari model scratch: 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "# 10 fold cross validation\n",
    "avg_sklearn = kFold(model_sklearn_2, X, y, scratch=False)\n",
    "avg_scratch = kFold(model_scratch_2, X, y, scratch=True)\n",
    "\n",
    "print(\"Rata-rata score dari model sklearn: {}\".format(avg_sklearn))\n",
    "print(\"Rata-rata score dari model scratch: {}\".format(avg_scratch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Menyimpan model hipotesis hasil pembelajaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Error threshold:  0.01\n",
      "Max iteration:  2000\n",
      "Batch size:  5\n",
      "Jumlah layer:  4\n",
      "============================================================\n",
      "Layer 1 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.97210836 -0.55770941  1.10749596 -0.65360332]\n",
      " [-1.59748566  0.39772612  1.20586542  1.05629911]\n",
      " [ 0.80750376  0.6556063  -0.18642273 -0.48265512]\n",
      " [-0.84007163  1.18058312 -1.32884084 -0.12439675]\n",
      " [-0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.93023756 -1.17389187  0.77769324 -1.07920728]\n",
      " [-1.98192696 -1.19387174  0.16862644 -2.09861293]\n",
      " [ 0.01915532  0.30218571 -0.96038242  1.59176309]\n",
      " [ 0.35233315 -0.6519459  -1.16200164 -2.28303757]\n",
      " [-0.66973331 -0.28121802  1.40664345 -0.16441541]]\n",
      "Bias:\n",
      "[[-0.13732465]\n",
      " [-1.38093545]\n",
      " [-0.20028595]\n",
      " [-0.10417045]\n",
      " [ 0.08445267]\n",
      " [ 0.76792804]\n",
      " [-1.57490495]\n",
      " [-1.41536889]\n",
      " [ 0.93064584]\n",
      " [ 0.39035644]]\n",
      "============================================================\n",
      "Layer 2 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.91242863 -0.60354197  1.20135905 -0.60140014 -1.59748566  0.40904977\n",
      "   1.20586542  1.05629911  0.85254391  0.65760923]\n",
      " [-0.22473706 -0.52218217 -0.84007163  1.18058312 -1.32884084 -0.12439675\n",
      "  -0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.93466082 -1.16996432  0.73427474 -1.11215464 -1.98192696 -1.1959042\n",
      "   0.16862644 -2.09861293  0.01915532  0.3066514 ]\n",
      " [-0.96038242  1.59176309  0.35233315 -0.6519459  -1.16200164 -2.28303757\n",
      "  -0.66273522 -0.27412048  1.37576624 -0.19023313]\n",
      " [-0.14997874 -1.38093545 -0.19942784 -0.10417045  0.08445267  0.76916041\n",
      "  -1.57490495 -1.41536889  0.93064584  0.41561036]\n",
      " [ 1.41882484 -2.46875639  0.26559777 -1.21841107 -0.74092644  1.0818419\n",
      "  -0.71417021  1.85473142  1.52459913 -1.84577425]\n",
      " [-0.63327976  0.1300763   1.71598337 -0.7012101   0.13190869 -1.70533737\n",
      "  -2.10575458  0.73512193 -0.67059795 -0.15096597]\n",
      " [-0.19108452 -0.15787655  1.43168398 -0.42211077  0.79197497  1.44881232\n",
      "   0.8749181  -1.17416614  0.60359526 -0.76815991]\n",
      " [ 1.43703916  1.59487438  0.37561825  1.89835026  1.78385212  1.43521435\n",
      "   0.79380501 -0.42655674 -1.48182836  0.37965147]\n",
      " [ 0.18583889  0.93476078 -0.15284656  0.89987826  2.12052387  0.9183451\n",
      "   0.05341478  0.15107901 -1.49249972 -0.31083559]]\n",
      "Bias:\n",
      "[[-0.32217744]\n",
      " [ 0.53368414]\n",
      " [ 0.26613425]\n",
      " [-0.62918173]\n",
      " [-0.11183727]\n",
      " [-0.28397971]\n",
      " [ 0.96028073]\n",
      " [-0.17635211]\n",
      " [ 1.65729079]\n",
      " [-0.50784907]]\n",
      "============================================================\n",
      "Layer 3 (Activation: \"Linear\", Units: 5)\n",
      "Weight:\n",
      "[[ 0.91934998 -0.60354197  1.16338783 -0.60140014 -1.59898014  0.40777096\n",
      "   1.2087353   1.07022162  0.85674757  0.69053552]\n",
      " [-0.2186042  -0.52218217 -0.83944536  1.18058312 -1.33164675 -0.10412079\n",
      "  -0.49376081 -1.91947433 -1.84106602  0.26743979]\n",
      " [ 0.9437448  -1.16996432  0.74065585 -1.11215464 -1.9864839  -1.1628496\n",
      "   0.17508286 -2.05578082  0.03004391  0.30620827]\n",
      " [-0.97750856  1.59176309  0.3489078  -0.6519459  -1.15488647 -2.33304611\n",
      "  -0.67417781 -0.3427064   1.35579047 -0.19657918]\n",
      " [-0.14086541 -1.38093545 -0.18197747 -0.10417045  0.07678767  0.8263366\n",
      "  -1.56331379 -1.34293083  0.95020331  0.40263099]]\n",
      "Bias:\n",
      "[[ 1.42301731]\n",
      " [-2.46683551]\n",
      " [ 0.20236989]\n",
      " [-1.22331393]\n",
      " [-0.73580115]]\n",
      "============================================================\n",
      "Layer 4 (Activation: \"Sigmoid\", Units: 3)\n",
      "Weight:\n",
      "[[ 0.9155071  -0.60354197  1.16229517 -0.60140014 -1.59748566]\n",
      " [ 0.39772612  1.20586542  1.05629911  0.85254391  0.68939141]\n",
      " [-0.35785138 -0.37025546 -0.70548949  1.28601074 -1.27967725]]\n",
      "Bias:\n",
      "[[-0.12439675]\n",
      " [-0.4974814 ]\n",
      " [-1.95108742]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_scratch_1.summary()\n",
    "model_scratch_1.save_file(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Membaca model hipotesis dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded. Model detected\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_load = NeuralNetwork(learning_rate = 1, error_threshold = 1, max_iter = 1, batch_size = 1)\n",
    "model_load.load_file(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Error threshold:  0.01\n",
      "Max iteration:  2000\n",
      "Batch size:  5\n",
      "Jumlah layer:  4\n",
      "============================================================\n",
      "Layer 1 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.97210836 -0.55770941  1.10749596 -0.65360332]\n",
      " [-1.59748566  0.39772612  1.20586542  1.05629911]\n",
      " [ 0.80750376  0.6556063  -0.18642273 -0.48265512]\n",
      " [-0.84007163  1.18058312 -1.32884084 -0.12439675]\n",
      " [-0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.93023756 -1.17389187  0.77769324 -1.07920728]\n",
      " [-1.98192696 -1.19387174  0.16862644 -2.09861293]\n",
      " [ 0.01915532  0.30218571 -0.96038242  1.59176309]\n",
      " [ 0.35233315 -0.6519459  -1.16200164 -2.28303757]\n",
      " [-0.66973331 -0.28121802  1.40664345 -0.16441541]]\n",
      "Bias:\n",
      "[[-0.13732465]\n",
      " [-1.38093545]\n",
      " [-0.20028595]\n",
      " [-0.10417045]\n",
      " [ 0.08445267]\n",
      " [ 0.76792804]\n",
      " [-1.57490495]\n",
      " [-1.41536889]\n",
      " [ 0.93064584]\n",
      " [ 0.39035644]]\n",
      "============================================================\n",
      "Layer 2 (Activation: \"ReLU\", Units: 10)\n",
      "Weight:\n",
      "[[ 0.91242863 -0.60354197  1.20135905 -0.60140014 -1.59748566  0.40904977\n",
      "   1.20586542  1.05629911  0.85254391  0.65760923]\n",
      " [-0.22473706 -0.52218217 -0.84007163  1.18058312 -1.32884084 -0.12439675\n",
      "  -0.4974814  -1.94712335 -1.84865498  0.26466031]\n",
      " [ 0.93466082 -1.16996432  0.73427474 -1.11215464 -1.98192696 -1.1959042\n",
      "   0.16862644 -2.09861293  0.01915532  0.3066514 ]\n",
      " [-0.96038242  1.59176309  0.35233315 -0.6519459  -1.16200164 -2.28303757\n",
      "  -0.66273522 -0.27412048  1.37576624 -0.19023313]\n",
      " [-0.14997874 -1.38093545 -0.19942784 -0.10417045  0.08445267  0.76916041\n",
      "  -1.57490495 -1.41536889  0.93064584  0.41561036]\n",
      " [ 1.41882484 -2.46875639  0.26559777 -1.21841107 -0.74092644  1.0818419\n",
      "  -0.71417021  1.85473142  1.52459913 -1.84577425]\n",
      " [-0.63327976  0.1300763   1.71598337 -0.7012101   0.13190869 -1.70533737\n",
      "  -2.10575458  0.73512193 -0.67059795 -0.15096597]\n",
      " [-0.19108452 -0.15787655  1.43168398 -0.42211077  0.79197497  1.44881232\n",
      "   0.8749181  -1.17416614  0.60359526 -0.76815991]\n",
      " [ 1.43703916  1.59487438  0.37561825  1.89835026  1.78385212  1.43521435\n",
      "   0.79380501 -0.42655674 -1.48182836  0.37965147]\n",
      " [ 0.18583889  0.93476078 -0.15284656  0.89987826  2.12052387  0.9183451\n",
      "   0.05341478  0.15107901 -1.49249972 -0.31083559]]\n",
      "Bias:\n",
      "[[-0.32217744]\n",
      " [ 0.53368414]\n",
      " [ 0.26613425]\n",
      " [-0.62918173]\n",
      " [-0.11183727]\n",
      " [-0.28397971]\n",
      " [ 0.96028073]\n",
      " [-0.17635211]\n",
      " [ 1.65729079]\n",
      " [-0.50784907]]\n",
      "============================================================\n",
      "Layer 3 (Activation: \"Linear\", Units: 5)\n",
      "Weight:\n",
      "[[ 0.91934998 -0.60354197  1.16338783 -0.60140014 -1.59898014  0.40777096\n",
      "   1.2087353   1.07022162  0.85674757  0.69053552]\n",
      " [-0.2186042  -0.52218217 -0.83944536  1.18058312 -1.33164675 -0.10412079\n",
      "  -0.49376081 -1.91947433 -1.84106602  0.26743979]\n",
      " [ 0.9437448  -1.16996432  0.74065585 -1.11215464 -1.9864839  -1.1628496\n",
      "   0.17508286 -2.05578082  0.03004391  0.30620827]\n",
      " [-0.97750856  1.59176309  0.3489078  -0.6519459  -1.15488647 -2.33304611\n",
      "  -0.67417781 -0.3427064   1.35579047 -0.19657918]\n",
      " [-0.14086541 -1.38093545 -0.18197747 -0.10417045  0.07678767  0.8263366\n",
      "  -1.56331379 -1.34293083  0.95020331  0.40263099]]\n",
      "Bias:\n",
      "[[ 1.42301731]\n",
      " [-2.46683551]\n",
      " [ 0.20236989]\n",
      " [-1.22331393]\n",
      " [-0.73580115]]\n",
      "============================================================\n",
      "Layer 4 (Activation: \"Sigmoid\", Units: 3)\n",
      "Weight:\n",
      "[[ 0.9155071  -0.60354197  1.16229517 -0.60140014 -1.59748566]\n",
      " [ 0.39772612  1.20586542  1.05629911  0.85254391  0.68939141]\n",
      " [-0.35785138 -0.37025546 -0.70548949  1.28601074 -1.27967725]]\n",
      "Bias:\n",
      "[[-0.12439675]\n",
      " [-0.4974814 ]\n",
      " [-1.95108742]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model_load.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Membuat instance baru lalu memprediksi hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create random n instances\n",
    "def create_random_instance(n):\n",
    "    '''\n",
    "    Creating n random instances\n",
    "    '''\n",
    "    rand_array = []\n",
    "    n_attr = 4\n",
    "    for i in range (n):\n",
    "        rand_row = []\n",
    "        for j in range (n_attr):\n",
    "            rand_row.append(round(np.random.uniform(0, 7), 2))\n",
    "        rand_array.append(rand_row)\n",
    "    return(rand_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: \n",
      "[[3.71 2.05 1.94 4.75]\n",
      " [1.63 4.74 3.03 2.71]\n",
      " [2.35 3.59 6.91 3.72]\n",
      " [4.76 2.74 0.82 4.95]]\n",
      "result: \n",
      "[[1.00000000e+00 1.00000000e+00 1.00480019e-01 1.00000000e+00]\n",
      " [2.27904417e-16 5.02038222e-07 1.23520749e-01 2.65879575e-24]\n",
      " [1.21883696e-18 6.67632838e-01 7.59862213e-01 4.07243891e-27]]\n",
      "[0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Create new instances & predict them using scratch model\n",
    "new_instances = create_random_instance(4)\n",
    "result = model_load.predict(new_instances)\n",
    "\n",
    "# Print instance & result of predict\n",
    "print(\"instance: \")\n",
    "print(np.array(new_instances))\n",
    "print(\"result: \")\n",
    "print(result)\n",
    "print(list(map(np.argmax, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Analisis Hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis hasil nomor 2.\n",
    "\n",
    "Berdasarkan hasil perbandingan confusion matrix dan perhitungan kinerja dari sklearn yang telah dieksekusi, didapatkan bahwa kinerja dari model yang dibuat oleh sklearn lebih baik dibandingkan dengan model yang dibuat secara sendiri. Pada model scratch perubahan error menuju titik konvergen tidak terlalu besar, hal tersebut karena masih terdapat kesalahan pada implementasi optimizer bobot dan bias yang digunakan pada model buatan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis hasil nomor 3.\n",
    "\n",
    "Berdasarkan hasil pembelajaran FFNN untuk dataset iris dengan skema split train 90% dan test 10% yang telah dieksekusi, didapatkan bahwa kinerja dari model yang dibuat oleh sklearn lebih baik dibandingkan dengan model yang dibuat secara sendiri. Pada model scratch perubahan error menuju titik konvergen tidak terlalu besar, hal tersebut karena masih terdapat kesalahan pada implementasi optimizer bobot dan bias yang digunakan pada model buatan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan hasil nomor 2 dan 3\n",
    "Berdasarkan hasil pembelajaran nomor 2 dan nomor 3 dapat dilihat bahwa ketika menggunakan skema 90% data training dan 10% data testing maka didapatkan hasil yang lebih baik. Hal tersebut karena metode splitting data akan menghasilkan performa yang lebih baik ketika data yang digunakan sebagai data latihan cukup banyak, dalam kasus ini dataset iris memiliki data yang cukup sehingga menghasilkan performa yang lebih baik pada dataset yang telah dibagi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
